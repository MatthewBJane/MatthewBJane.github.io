{
  "hash": "56e4d96f67abcdf5fd36696c17aa1a08",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multilevel Item Response Theory: Incorporating Random Effects in IRT models\"\n\nauthor: \"Matthew B. Jané\"\nimage: \"blog-image.png\"\nimage-height: \"5%\"\ntoc: true\ncode-fold: false\ndate: 2024-10-31\ndescription: \"This is a short blog post on performing multilevel item response theory models with the `mirt` package [@mirt]. This blog will discuss incorporating random effects into \"\n\ncategories:\n  - Individual Participant Data\n  - Repeated Measures\n  - Meta-Analysis\n\nimage-meta: \"blog-image.png\"\nbibliography: references.bib\n---\n\n```{=html}\n<a href=\"https://www.buymeacoffee.com/matthewbjane\"><img src=\"https://img.buymeacoffee.com/button-api/?text=Support my work&emoji=☕&slug=matthewbjane&button_colour=ed5c9b&font_colour=ffffff&font_family=Arial&outline_colour=ffffff&coffee_colour=ffffff\" /></a>\n```\n\n\n\n<br>\n\n\n## A basic multilevel model\n\n\nLet's think of a traditional multilevel model that contains two sources of error, $u_i$ (between-group error) and $e_{ij}$ (within-group error), where $i$ denotes the group and $j$ denotes an individual,\n\n$$\ny_{ij} = \\mu + u_i + e_{ij}\n$$ {#eq-1}\n\nwhere $\\mu$ is the (fixed) population mean and $y_{ij}$ is the outcome variable for school $i$ and individual $j$. Let's assume that the error variables $u_i$ and $e_{ij}$ are normally distributed such that,\n\n$$\nu_i \\sim \\mathcal{N}(0, \\tau),\n$$ {#eq-2}\n$$\ne_{ij} \\sim \\mathcal{N}(0, \\sigma).\n$$ {#eq-3}\n\n\nUsing this basic multilevel structure we can model the `Penicillin` dataset in `lme4` [@lme4]. For the outcome of interest we can use the diameter of the zone of inhibition of the growth of the organism (*B. subtilis*; diameter is related to the concentration of Penicillin in the solution). The grouping variable will be the 6 samples that are labeled A,B,C,D,E,F where each sample has 24 observations corresponding to 24 plates (we will ignore plates as a grouping variable for this example). We can visualize the structure of the dataset with a flowchart,\n\n\n\n```{mermaid}\n\nflowchart TD\n    A(sample A) --> o1A(obs 1)\n    A(sample A) --> o2A(obs 2)\n    A(sample A) --> SPACEA(...)\n    A(sample A) --> o24A(obs 24)\n    spacing(...)\n    F(Sample F) --> o1F(obs 1)\n    F(Sample F) --> o2F(obs 2)\n    F(Sample F) --> SPACEF(...)\n    F(Sample F) --> o24F(obs 24)\n    \n\n```\n\n\n\n\nNow let's view the actual dataset in R,\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nlibrary(tidyverse)\n\n# note: we will be ignoring the Plate variable\nhead(Penicillin %>% select(-plate),12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   diameter sample\n1        27      A\n2        23      B\n3        26      C\n4        23      D\n5        23      E\n6        21      F\n7        27      A\n8        23      B\n9        26      C\n10       23      D\n11       23      E\n12       21      F\n```\n\n\n:::\n:::\n\n\n\nRelating back to equation 1, $y_{ij}$ will represent the diameter for sample $i = \\{A,B,C,D,E,F\\}$ and observation $j = \\{1_i...24_i\\} = \\{1_A...24_A...1_F...24_F\\}$. Then $\\mu$ is the global diameter mean, $u_i$ is the between-sample error (this also means that $\\mu+u_i$ is the sample mean) and $e_{ij}$ is the within-sample error (this of course mean that $\\mu+u_i + e_{ij}$ recovers the values of the outcome, $y_{ij}$). We also have the parameters $\\tau^2$ and $\\sigma^2$ which denote the between-sample variance and within-sample variance respectively (we will assume that all groups have the same within-group variance, $\\sigma=\\sigma_A=...=\\sigma_F$). Let's visualize all the data before we start fitting a model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdist)\n\nggplot(data = Penicillin, aes(x = diameter, y = sample)) +\n  stat_histinterval(slab_fill = \"skyblue3\", color = \"skyblue4\", \n                    point_interval = mean_qi, breaks = seq(18,28,by=.5), \n                    align = .25, scale = .7)  +\n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = 1.2) +\n  geom_hline(yintercept = unique(Penicillin$sample), linewidth = .2, alpha = .2) + \n  scale_x_continuous(breaks=18:28) +\n  labs(x = \"Diameter (mm)\", y = \"Sample\")\n```\n\n::: {.cell-output-display}\n![](blog-post-11_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can see that the means are quite different between each group, yet it is probably reasonable to fix the within-group variance as the same value for each group. Calculating the means and standard deviations for each group demonstrates supports this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPenicillin %>%\n  summarize(mean_diameter = round(mean(diameter),2),\n            sd_diameter = round(sd(diameter),2),\n            .by = sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sample mean_diameter sd_diameter\n1      A         25.17        0.96\n2      B         21.96        1.00\n3      C         24.92        1.02\n4      D         22.88        0.95\n5      E         22.96        1.04\n6      F         19.96        1.08\n```\n\n\n:::\n:::\n\n\n\n\nUsing `lme4` we can model the data and obtain estimates of the parameters of interest: $\\mu, \\tau, \\sigma$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit model\nmlm <- lmer(diameter ~ 1 + (1 | sample), data = Penicillin)\n\n# display results\nsummary(mlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: diameter ~ 1 + (1 | sample)\n   Data: Penicillin\n\nREML criterion at convergence: 435.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.97355 -0.90190  0.02988  1.00350  2.02207 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n sample   (Intercept) 3.701    1.924   \n Residual             1.019    1.010   \nNumber of obs: 144, groups:  sample, 6\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  22.9722     0.7899   29.08\n```\n\n\n:::\n:::\n\n\n\nBased on the model, we get an estimate of the global mean from the `(Intercept)` under the `Fixed effects` header which gives $\\hat{\\mu} = 22.97$. Under the `Random effects` header, we see the `sample` and `Residual` terms under `Groups` which gives us the between-sample variance/standard deviation ($\\tau^2$/$\\tau$) and the within-sample variance/standard deviation ($\\sigma^2$/$\\sigma$), respectively. The model fit gives us $\\hat{\\tau} = 1.92$ and $\\hat{\\sigma} = 1.02$. We can also extract estimates of the error components of the model $\\hat{u}_i$ and $\\hat{e}_{ij}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain u_i\nranef(mlm)$sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)\nA  2.16954755\nB -1.00238589\nC  1.92238391\nD -0.09611920\nE -0.01373131\nF -2.97969506\n```\n\n\n:::\n\n```{.r .cell-code}\n# obtain e_ij (first 20)\nresiduals(mlm)[1:20]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           1            2            3            4            5            6 \n 1.858230225  1.030163672  1.105393871  0.123896973  0.041509091  1.007472834 \n           7            8            9           10           11           12 \n 1.858230225  1.030163672  1.105393871  0.123896973  0.041509091  1.007472834 \n          13           14           15           16           17           18 \n-0.141769775 -0.969836328  0.105393871  1.123896973  1.041509091  0.007472834 \n          19           20 \n 0.858230225  1.030163672 \n```\n\n\n:::\n:::\n\n\n\n\n## Item response theory\n\nItem response theory deals with trying to estimate an underlying characteristic of a person based on categorical/ordinal measurements of that characteristic. For example, let's say we have a math test with 10 problem items. For an item $i$ and person $p$, they can answer the item correctly ($Y_{ip}=1$) or incorrectly ($Y_{ip}=0$). We can suppose that the probability that a person correctly answers a given item depends on the person's mathematical ability, we will call this $\\theta_p$. We can write this probability as $\\Pr (Y_{ip} = 1 \\,|\\, \\theta_p)$ where the probability is conditional on the person's ability. However, the probability will likely change based on item-specific characteristics such as the difficulty of an item (harder items have a lower probability of getting them correct) and sensitivity of the item (how related is the item to the construct of mathematical ability). If we denote difficulty as $d$ and sensitivity as $a$ then $\\Pr (Y_{ip} = 1 \\,|\\, \\theta_p,a_i,d_i)$ can be modeled as a sigmoidal function,\n\n$$\n\\Pr (Y_{ip} = 1 \\,|\\, \\theta_p,a_i,d_i) = \\phi(\\theta_{p}; a_i,d_i)\n$$\n\nWhere $\\phi$ is typically a logistic function, but it can be modeled as other sigmoids such as a cumulative normal. For a logistic function the probability of answering a question correctly is\n\n$$\n\\Pr (Y_{ip} = 1 \\,|\\, \\theta_p,a_i,d_i) = \\frac{\\exp[a_i(\\theta_p - d_i)]}{1 + \\exp[a_i(\\theta_p - d_i)]}\n$$\n\nThis function can be visualized by an item characteristic curve:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(latex2exp)\n\na <- c(1,1,1.5,.5)\nd <- c(-1,1,0,0)\ntheta <- seq(-3,3,.1)\n\nM1 <- exp(a[1]*(theta - d[1]))\np1 <- M1 / (1 + M1)\nM2 <- exp(a[2]*(theta - d[2]))\np2 <- M2 / (1 + M2)\nM3 <- exp(a[3]*(theta - d[3]))\np3 <- M3 / (1 + M3)\nM4 <- exp(a[4]*(theta - d[4]))\np4 <- M4 / (1 + M4)\n\nh1 <- ggplot(data = NULL) +\n  geom_line(aes(x = theta, y = p1),linewidth = 1.5,color = \"steelblue2\") + \n  geom_line(aes(x = theta, y = p2),linewidth = 1.5,color = \"hotpink3\") + \n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = 1) +\n  annotate(geom=\"text\", x = -1, y = .78, label = \"easy item\\n(d=-1)\", color = \"steelblue2\", fontface = \"bold\",size = 4) +\n  annotate(geom=\"text\", x = 2, y = .40, label = \"hard item\\n(d=1)\", color = \"hotpink3\", fontface = \"bold\",size = 4) +\n  xlim(-3,3) +\n  labs(x = TeX(\"Ability (\\\\theta)\"), y = \"Probability of Correct Response\")+ \n  scale_x_continuous(breaks = -3:3) +\n  ggtitle(\"Effect of difficulty\")\n\n\nh2 <- ggplot(data = NULL) +\n  geom_line(aes(x = theta, y = p3),linewidth = 1.5,color = \"steelblue2\") + \n  geom_line(aes(x = theta, y = p4),linewidth = 1.5,color = \"hotpink3\") + \n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = 1) +\n  annotate(geom=\"text\", x = -.7, y = .83, label = \"more sensitive\\n(a=1.5)\", color = \"steelblue2\", fontface = \"bold\",size = 4) +\n  annotate(geom=\"text\", x = 1.8, y = .45, label = \"less sensitive\\n(a=.5)\", color = \"hotpink3\", fontface = \"bold\",size = 4) +\n  labs(x = TeX(\"Ability (\\\\theta)\"), y = \"\") + \n  scale_x_continuous(breaks = -3:3) +\n  ggtitle(\"Effect of sensitivity\")\n\nh1 + h2\n```\n\n::: {.cell-output-display}\n![](blog-post-11_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nWith real data, we do not have access to a person's ability and therefore it has to be estimated based on the test item responses. We can estimate $\\theta$ as well as the difficulty $d$ and sensitivity $a$ parameters as long as we can assume that $\\theta$ has a mean of zero and a variance of 1. In R, we can fit use the `mirt` package [@mirt] to obtain item parameter estimates and $\\theta$ estimates. We will use the `ability` dataset from the `psychTools` package [@psychTools]. This data set contains 16 cognitive items with dummy-coded correct/incorrect responses. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mirt)\nlibrary(psychTools)\n\n# preview data\nhead(ability)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   reason.4 reason.16 reason.17 reason.19 letter.7 letter.33 letter.34\n5         0         0         0         0        0         1         0\n6         0         0         1         0        1         0         1\n7         0         1         1         0        1         0         0\n8         1        NA         0         0        0         0         1\n9        NA         1         1         0        0         1         0\n10        1         1         1         1        1         1         1\n   letter.58 matrix.45 matrix.46 matrix.47 matrix.55 rotate.3 rotate.4 rotate.6\n5          0         0         0         0         1        0        0        0\n6          0         0         0         0         0        0        0        1\n7          0         1         1         0         0        0        0        0\n8          0         0        NA         0         0        0        0        0\n9         NA         1         1         0         0        0        0        0\n10         1         1         1         1         0        1        1        1\n   rotate.8\n5         0\n6         0\n7         0\n8         0\n9         0\n10        0\n```\n\n\n:::\n:::\n\n\n\nFor the moment we are going to assume their is a single ability underlying these responses (since they are three different tasks it is more likely that you will see additional covariance between items from the same task). Let's fit the 2-parameter (difficulty and sensitivity) item response theory model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nirt <- mirt(data = ability, itemtype = \"2PL\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: data contains response patterns with only NAs\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIteration: 1, Log-Lik: -12883.040, Max-Change: 0.65326\nIteration: 2, Log-Lik: -12676.169, Max-Change: 0.23694\nIteration: 3, Log-Lik: -12632.976, Max-Change: 0.14393\nIteration: 4, Log-Lik: -12620.655, Max-Change: 0.08933\nIteration: 5, Log-Lik: -12616.222, Max-Change: 0.04868\nIteration: 6, Log-Lik: -12614.571, Max-Change: 0.03334\nIteration: 7, Log-Lik: -12613.393, Max-Change: 0.01612\nIteration: 8, Log-Lik: -12613.099, Max-Change: 0.01166\nIteration: 9, Log-Lik: -12612.940, Max-Change: 0.00697\nIteration: 10, Log-Lik: -12612.778, Max-Change: 0.00381\nIteration: 11, Log-Lik: -12612.753, Max-Change: 0.00309\nIteration: 12, Log-Lik: -12612.737, Max-Change: 0.00233\nIteration: 13, Log-Lik: -12612.711, Max-Change: 0.00169\nIteration: 14, Log-Lik: -12612.708, Max-Change: 0.00101\nIteration: 15, Log-Lik: -12612.706, Max-Change: 0.00091\nIteration: 16, Log-Lik: -12612.701, Max-Change: 0.00070\nIteration: 17, Log-Lik: -12612.701, Max-Change: 0.00049\nIteration: 18, Log-Lik: -12612.701, Max-Change: 0.00022\nIteration: 19, Log-Lik: -12612.701, Max-Change: 0.00020\nIteration: 20, Log-Lik: -12612.701, Max-Change: 0.00054\nIteration: 21, Log-Lik: -12612.701, Max-Change: 0.00059\nIteration: 22, Log-Lik: -12612.701, Max-Change: 0.00015\nIteration: 23, Log-Lik: -12612.701, Max-Change: 0.00039\nIteration: 24, Log-Lik: -12612.701, Max-Change: 0.00048\nIteration: 25, Log-Lik: -12612.701, Max-Change: 0.00010\n```\n\n\n:::\n:::\n\n\n\nNow let us see the item parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(irt, IRTpars = T, simplify = T)$items\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  a          b g u\nreason.4  1.7320882 -0.6525175 0 1\nreason.16 1.3301105 -0.9772691 0 1\nreason.17 1.8985045 -0.8652319 0 1\nreason.19 1.2934980 -0.6134082 0 1\nletter.7  1.4998348 -0.5209984 0 1\nletter.33 1.2657257 -0.4432499 0 1\nletter.34 1.5992977 -0.5338006 0 1\nletter.58 1.4298053  0.1021676 0 1\nmatrix.45 0.9623397 -0.2526989 0 1\nmatrix.46 1.0283731 -0.3426231 0 1\nmatrix.47 1.2559059 -0.5962972 0 1\nmatrix.55 0.7861337  0.6347721 0 1\nrotate.3  1.8296161  1.1472412 0 1\nrotate.4  2.0869537  0.9916273 0 1\nrotate.6  1.6060674  0.7060261 0 1\nrotate.8  1.5752760  1.2798825 0 1\n```\n\n\n:::\n:::\n\n\nNote that the `b` parameter here is the difficulty parameter. We can also get the $\\theta$ estimates ($\\hat{\\theta}$) with the `fscores()` function from `mirt`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first 20 people\nggplot(data = NULL, aes(x=as.numeric(fscores(irt)))) +\n  stat_histinterval(breaks = seq(-3,3,by=.1), slab_fill = \"steelblue1\",color = \"black\") +\n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = .5) +\n  scale_y_continuous(breaks = c()) +\n  scale_x_continuous(breaks = -3:3) +\n  labs(x = TeX(\"Estimated ability (\\\\hat{\\\\theta})\"),y = \"\")\n```\n\n::: {.cell-output-display}\n![](blog-post-11_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nLet's plot out an easy item (`reason.16`) and a difficult item (`rotate.8`). We can include empirical probability estimates overlaid on top by binning $\\hat{\\theta}$ \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntheta <- seq(-3,3,.1)\nparams <- coef(irt, IRTpars = T, simplify = T)\na1 <- params$items[\"reason.16\",\"a\"]\nd1 <- params$items[\"reason.16\",\"b\"]\n\na2 <- params$items[\"rotate.8\",\"a\"]\nd2 <- params$items[\"rotate.8\",\"b\"]\n\np1 <- exp(a1*(theta - d1)) / (1 + exp(a1*(theta - d1)))\np2 <- exp(a2*(theta - d2)) / (1 + exp(a2*(theta - d2)))\n\nggplot(data = NULL) +\n  geom_line(aes(x = theta, y = p1),linewidth = 1.5,color = \"steelblue2\") + \n  geom_line(aes(x = theta, y = p2),linewidth = 1.5,color = \"hotpink3\") + \n  stat_summary_bin(aes(x = as.numeric(fscores(irt)), y = as_tibble(ability)$reason.16), \n                   fun.data = mean_cl_normal, bins = 8, color = \"steelblue3\") +\n  stat_summary_bin(aes(x = as.numeric(fscores(irt)), y = as_tibble(ability)$rotate.8), \n                   fun.data = mean_cl_normal, bins = 8, color = \"hotpink4\") +\n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = 1, plot.title  = element_markdown()) +\n  xlim(-3,3) +\n  labs(x = TeX(\"Ability (\\\\theta)\"), y = \"Probability of Correct Response\", \n       title = \"Items <span style = 'color:steelblue2;'>**reason.16**</span> and <span style = 'color:hotpink3;'>**reason.16**</span>\") + \n  scale_x_continuous(breaks = -3:3) \n```\n\n::: {.cell-output-display}\n![](blog-post-11_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Multilevel IRT\n\nNow that we have a basic idea for a multilevel model and a an item response theory model, we can start combining them. Let's recall the first multilevel model equation of $y_{ij} = \\mu + u_i + e_{ij}$, however now let's replace $y_{ij}$ with the ability parameter from the item response theory model $\\theta_{gp}$ for group $g$ and person $p$ such that,\n\n$$\n\\theta_{gp} = \\mu + u_g + e_{gp}\n$$\nwhere now $u_g$ is the between-group residual term and $e_{gp}$ is the within-group residual term. In the original multilevel model we constructed we knew the value of the outcome $y_{ij}$ however now we have an unknown parameter as the outcome $\\theta_{gp}$ and therefore we have to estimate it. The item response theory model must be appended now to incorporate the grouping variable:\n\n$$\n\\Pr (Y_{igp} = 1 \\,|\\, \\theta_{gp},a_i,d_i) = \\frac{\\exp[a_i(\\theta_{gp} - d_i)]}{1 + \\exp[a_i(\\theta_{gp} - d_i)]}\n$$\n\nNote that item parameters should be independent of group or individuals, if they weren't then we would have differential item functioning and may warrant exclusion of the item from the model. Using a data set from the General Social Survey (GSS; 1972-2022) from the `gssr` package [@gssr], we can use responses to 10 vocabulary items and treat the region they live in as a random effect (i.e., New England). Therefore $\\mu+u_g$ will denote mean ability for region $g$. Since the GSS is a massive data set, we can sample 10,000 individuals so that the model fitting runs a bit quicker.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remotes::install_github(\"kjhealy/gssr\")\nlibrary(gssr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(gss_all)\n\ngss <- gss_all %>%\n  # select only necessary data\n  select(worda, wordb, wordc, wordd, worde, wordf, wordg, wordh, wordi, wordj, reg16) %>%\n  mutate_at(vars(worda, wordb, wordc, wordd, worde, wordf, wordg, wordh, wordi, wordj), as.numeric) %>%\n  # filter out missing data\n  filter(complete.cases(.)) %>%\n  slice_sample(n = 10000, replace = FALSE)\n\n# view data set\nhead(gss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n  worda wordb wordc wordd worde wordf wordg wordh wordi wordj reg16             \n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl+lbl>         \n1     0     1     0     1     0     0     0     0     0     0 8 [mountain]      \n2     1     1     0     1     1     1     0     1     1     1 3 [east north cen…\n3     1     1     1     1     1     1     1     1     1     0 3 [east north cen…\n4     1     1     0     1     1     1     1     0     0     1 2 [middle atlanti…\n5     1     1     1     1     1     1     1     1     1     0 5 [south atlantic]\n6     1     1     1     1     1     1     1     1     1     1 0 [foreign]       \n```\n\n\n:::\n\n```{.r .cell-code}\n# remove from original data from environment (very large file)\nremove(gss_all)\n```\n:::\n\n\n\nNow let's fit the model with the `mixedmirt()` function \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlm_irt <- mixedmirt(data = gss %>% select(-reg16), \n                     covdata = gss %>% select(reg16),\n                     model = \"Theta = 1-10\",\n                     fixed = ~ 1, \n                     lr.random = ~ 1|reg16, \n                     itemtype = '2PL')\n```\n:::\n\n\n\nNow that we have fit the model we can obtain the $\\theta$ estimates. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = NULL, aes(x = as.numeric(randef(mlm_irt)$Theta), y = factor(gss$reg16))) +\n  stat_slabinterval(slab_fill = \"hotpink3\", point_interval = \"mean_qi\") +\n  theme_ggdist(base_size = 15) +\n  theme(aspect.ratio = 1.4) +\n  labs(x = TeX(\"Ability (\\\\theta)\"),\n       y = \"Region\")\n```\n\n::: {.cell-output-display}\n![](blog-post-11_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nWe can also obtain the item parameters via the `coef()` function,\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mlm_irt, IRTpars = T, simplify = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$worda\n        (Intercept)    a1  d  g  u\npar          -1.538 3.654  0  0  1\nCI_2.5       -1.599 3.537 NA NA NA\nCI_97.5      -1.478 3.770 NA NA NA\n\n$wordb\n        (Intercept)    a1  d  g  u\npar          -1.538 7.416  0  0  1\nCI_2.5       -1.599 7.071 NA NA NA\nCI_97.5      -1.478 7.761 NA NA NA\n\n$wordc\n        (Intercept)    a1  d  g  u\npar          -1.538 0.544  0  0  1\nCI_2.5       -1.599 0.503 NA NA NA\nCI_97.5      -1.478 0.585 NA NA NA\n\n$wordd\n        (Intercept)    a1  d  g  u\npar          -1.538 9.198  0  0  1\nCI_2.5       -1.599 8.667 NA NA NA\nCI_97.5      -1.478 9.728 NA NA NA\n\n$worde\n        (Intercept)    a1  d  g  u\npar          -1.538 3.364  0  0  1\nCI_2.5       -1.599 3.263 NA NA NA\nCI_97.5      -1.478 3.465 NA NA NA\n\n$wordf\n        (Intercept)    a1  d  g  u\npar          -1.538 3.716  0  0  1\nCI_2.5       -1.599 3.605 NA NA NA\nCI_97.5      -1.478 3.828 NA NA NA\n\n$wordg\n        (Intercept)    a1  d  g  u\npar          -1.538 0.846  0  0  1\nCI_2.5       -1.599 0.802 NA NA NA\nCI_97.5      -1.478 0.890 NA NA NA\n\n$wordh\n        (Intercept)    a1  d  g  u\npar          -1.538 0.934  0  0  1\nCI_2.5       -1.599 0.888 NA NA NA\nCI_97.5      -1.478 0.981 NA NA NA\n\n$wordi\n        (Intercept)    a1  d  g  u\npar          -1.538 2.882  0  0  1\nCI_2.5       -1.599 2.791 NA NA NA\nCI_97.5      -1.478 2.973 NA NA NA\n\n$wordj\n        (Intercept)    a1  d  g  u\npar          -1.538 0.694  0  0  1\nCI_2.5       -1.599 0.651 NA NA NA\nCI_97.5      -1.478 0.737 NA NA NA\n\n$GroupPars\n        MEAN_1 COV_11\npar          0      1\nCI_2.5      NA     NA\nCI_97.5     NA     NA\n\n$reg16\n        COV_reg16_reg16\npar               0.996\nCI_2.5            0.638\nCI_97.5           1.353\n```\n\n\n:::\n:::\n\n\n\n\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] gssr_0.6         psychTools_2.4.3 mirt_1.44.0      lattice_0.22-6  \n [5] latex2exp_0.9.6  ggtext_0.1.2     patchwork_1.2.0  ggdist_3.3.2    \n [9] lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n[13] purrr_1.0.2      readr_2.1.5      tidyr_1.3.1      tibble_3.2.1    \n[17] ggplot2_3.5.1    tidyverse_2.0.0  lme4_1.1-36      Matrix_1.7-1    \n\nloaded via a namespace (and not attached):\n [1] Rdpack_2.6.2         pbapply_1.7-2        gridExtra_2.3       \n [4] permute_0.9-7        testthat_3.2.1.1     rlang_1.1.4         \n [7] magrittr_2.0.3       compiler_4.4.2       mgcv_1.9-1          \n[10] vctrs_0.6.5          crayon_1.5.3         pkgconfig_2.0.3     \n[13] fastmap_1.2.0        backports_1.5.0      labeling_0.4.3      \n[16] utf8_1.2.4           rmarkdown_2.27       markdown_1.13       \n[19] sessioninfo_1.2.2    tzdb_0.4.0           haven_2.5.4         \n[22] nloptr_2.1.1         xfun_0.45            jsonlite_1.8.8      \n[25] Deriv_4.1.6          parallel_4.4.2       cluster_2.1.6       \n[28] R6_2.5.1             stringi_1.8.4        parallelly_1.38.0   \n[31] boot_1.3-31          rpart_4.1.23         brio_1.1.5          \n[34] Rcpp_1.0.12          knitr_1.47           future.apply_1.11.2 \n[37] snow_0.4-4           audio_0.1-11         base64enc_0.1-3     \n[40] R.utils_2.12.3       splines_4.4.2        nnet_7.3-19         \n[43] timechange_0.3.0     tidyselect_1.2.1     rstudioapi_0.16.0   \n[46] yaml_2.3.8           vegan_2.6-10         codetools_0.2-20    \n[49] dcurver_0.9.2        curl_5.2.2           listenv_0.9.1       \n[52] withr_3.0.0          evaluate_0.24.0      foreign_0.8-87      \n[55] future_1.34.0        xml2_1.3.6           pillar_1.9.0        \n[58] checkmate_2.3.1      renv_0.17.3          rtf_0.4-14.1        \n[61] reformulas_0.4.0     distributional_0.4.0 generics_0.1.3      \n[64] hms_1.1.3            commonmark_1.9.2     munsell_0.5.1       \n[67] scales_1.3.0         minqa_1.2.8          globals_0.16.3      \n[70] glue_1.7.0           RPushbullet_0.3.4    Hmisc_5.2-2         \n[73] tools_4.4.2          data.table_1.15.4    beepr_2.0           \n[76] SimDesign_2.18       fs_1.6.4             grid_4.4.2          \n[79] rbibutils_2.3        colorspace_2.1-0     nlme_3.1-166        \n[82] htmlTable_2.4.3      Formula_1.2-5        cli_3.6.3           \n[85] fansi_1.0.6          gtable_0.3.5         R.methodsS3_1.8.2   \n[88] digest_0.6.36        progressr_0.15.1     GPArotation_2024.3-1\n[91] htmlwidgets_1.6.4    farver_2.1.2         htmltools_0.5.8.1   \n[94] R.oo_1.27.0          lifecycle_1.0.4      gridtext_0.1.5      \n[97] MASS_7.3-61         \n```\n\n\n:::\n:::\n\n\n\n## Citing R packages\n\nThe following packages were used in this post:\n\n-   `brms` [@brms]\n-   `tidyverse` [@tidyverse]\n-   `ggdist` [@ggdist]\n-   `modelr` [@modelr]\n-   `tidybayes` [@tidybayes]\n\n<iframe src=\"https://embeds.beehiiv.com/f3ffd81a-4723-4419-bb1d-afc8af3bac36\" data-test-id=\"beehiiv-embed\" width=\"100%\" height=\"320\" frameborder=\"0\" scrolling=\"no\" style=\"border-radius: 4px; border: 2px solid #33333322; margin: 0; background-color: transparent;\">\n\n</iframe>\n",
    "supporting": [
      "blog-post-11_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}