[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Meta-Analysis Magic",
    "section": "",
    "text": "Welcome to my blog, Meta-Analysis Magic! Here, you’ll uncover mind-bending tricks that will help you unveil those elusive, unreported statistics tucked away in primary studies. But that’s not all – we’ll also dive into the thrilling realm of lesser-known meta-analysis modeling techniques that tend to slip under the radar.\nTo make your journey even more enchanting, each post will come complete with R code, making it a breeze for you to work your own meta-analysis wizardry. Let’s embark on this adventure together!\nIf you like the blog, consider subscribing below to be notified whenever a new post comes out (it’s free!).\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from the Standard Deviation of Change Scores\n\n\n\nrepeated measures\n\n\npre/post correlations\n\n\n\nWelcome to the first blog post of Meta-Analysis Magic! In this post we will go over a couple of ways to directly calculate pre/post correlations from alternative…\n\n\n\nMatthew B. Jané\n\n\nSep 8, 2023\n\n\n\n\n\n\n\n\n\n\n\nApproximating Standard Deviation from Inter-Quantile Range\n\n\n\nstandard deviations\n\n\napproximations\n\n\n\nOccasionally researchers report inter-quantile ranges (e.g., inter-quartile range) to measure the spread of the data. Since meta-analysts need standard deviations to…\n\n\n\nMatthew B. Jané\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from a Paired T-Test\n\n\n\nrepeated measures\n\n\npre/post correlations\n\n\ntest statistics\n\n\n\nIn this post we will go over how to convert a t-statistic from a paired t-test into a pre/post correlation. This is useful when change score standard deviations are unknown.…\n\n\n\nMatthew B. Jané\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\nSampling Distributions of Squared Effect Sizes and their Implication for Meta-Analysis\n\n\n\nsampling error\n\n\nartifacts\n\n\n\nIn this post we will discuss how squared effect sizes such as \\(\\eta^2\\) and \\(R^2\\) are distributed and the implications of this when conducting a meta-analysis\n\n\n\nMatthew B. Jané\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nRespectful Operationalism, Realism, and Models of Measurement Error in Psychometrics\n\n\n\npsychological measurement\n\n\nmeasurement error\n\n\n\nFull disclaimer: I am not well-versed in these issues, and I am not a philosopher. The purpose of this post is for me to articulate my own thought process, rather than have…\n\n\n\nMatthew B. Jané\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\nA Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt\n\n\n\nmeta-analysis\n\n\nsocial media\n\n\n\nI don’t know anything about the literature on social media and mental health so my focus on this post is to interrogate the statistical approach taken by the article written…\n\n\n\nMatthew B. Jané\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Website  Email  Twitter  Github\n\n\nCurrent PhD student interested in developing statistical methods and software for meta-analyses and primary studies. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. I am affiliated with the Systematic Health Action Research Program (SHARP) at the University of Connecticut where I am advised by Dr. Blair T. Johnson. I am also on the editorial board for Psychological Bulletin as a methodological reviewer.\n\n\n\n\n2022–Present | Ph.D. Quantitative Psychology, University of Connecticut, Storrs, Connecticut\n\nResearch involves developing statistical methods and software for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut, Storrs, Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut, Storrs, Connecticut\n\n\n\n\n\nJané, M. B., Xiao, Q., Yeung, S. K., Caldwell, A. R., Dunleavy, D. J., … (2023) Guide to Effect Sizes and Confidence Intervals. A collaborative guide to calculating, interpreting, and reporting effect sizes and confidence intervals. https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/\nJané, M B. (2023) Artifact Corrections for Effect Sizes. An open-source book for correcting bias in effect size estimates contaminated by statistical artifacts. In prepation\n\n\n\n\n\n R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/ThemePark\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. matthewbjane.com/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. matthewbjane.com/ArtifactCorrections\n Project Data & Code Primary Study Data and Code, Repository of data and code for all primary study projects. https://github.com/MatthewBJane/primary-project-data-code.\n Project Data & Code Meta-Analysis Data and Code, Repository of data and code for all meta-analytic projects. https://github.com/MatthewBJane/meta-analysis-project-data-code.\n\n\n\n\n\nPsychological Bulletin, 2 reviews\nAdvances in Methods and Practices in Psychological Science, 1 review\n\n\n\n\nJohnson, B. T., Jané, M. B., Curley, C. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nJané, M. B., Johnson, B. T. (2023). Correcting for Measurement Error in Repeated Measures Standardized Mean Differences. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Stress Reduction in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Martinez-Berman, L., Johnson, B. T. (2023). Cognitive Ability Moderates the Accuracy of Self-Reported Perfect Pitch. Manuscript in Preparation.\n\nData Code PDF OSF\n\nJané, M. B., Uanhoro, J., Boika, N., Steele, J., Batinovic, L., Johnson, B. T. (2024). Predictive Validity of the Unified Tertiary Matriculation Examination on University Grades: A Living Meta-Analysis. https://matthewbjane.com/utme-validity-meta-analysis/ Manuscript in Preparation.\n\nData Code PDF OSF"
  },
  {
    "objectID": "CV.html#bio",
    "href": "CV.html#bio",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Current PhD student interested in developing statistical methods and software for meta-analyses and primary studies. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. I am affiliated with the Systematic Health Action Research Program (SHARP) at the University of Connecticut where I am advised by Dr. Blair T. Johnson. I am also on the editorial board for Psychological Bulletin as a methodological reviewer."
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "2022–Present | Ph.D. Quantitative Psychology, University of Connecticut, Storrs, Connecticut\n\nResearch involves developing statistical methods and software for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut, Storrs, Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut, Storrs, Connecticut"
  },
  {
    "objectID": "CV.html#textbooks",
    "href": "CV.html#textbooks",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Jané, M. B., Xiao, Q., Yeung, S. K., Caldwell, A. R., Dunleavy, D. J., … (2023) Guide to Effect Sizes and Confidence Intervals. A collaborative guide to calculating, interpreting, and reporting effect sizes and confidence intervals. https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/\nJané, M B. (2023) Artifact Corrections for Effect Sizes. An open-source book for correcting bias in effect size estimates contaminated by statistical artifacts. In prepation"
  },
  {
    "objectID": "CV.html#software",
    "href": "CV.html#software",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/ThemePark\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. matthewbjane.com/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. matthewbjane.com/ArtifactCorrections\n Project Data & Code Primary Study Data and Code, Repository of data and code for all primary study projects. https://github.com/MatthewBJane/primary-project-data-code.\n Project Data & Code Meta-Analysis Data and Code, Repository of data and code for all meta-analytic projects. https://github.com/MatthewBJane/meta-analysis-project-data-code."
  },
  {
    "objectID": "CV.html#peer-reviews",
    "href": "CV.html#peer-reviews",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Psychological Bulletin, 2 reviews\nAdvances in Methods and Practices in Psychological Science, 1 review"
  },
  {
    "objectID": "CV.html#publications",
    "href": "CV.html#publications",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Johnson, B. T., Jané, M. B., Curley, C. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nJané, M. B., Johnson, B. T. (2023). Correcting for Measurement Error in Repeated Measures Standardized Mean Differences. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Stress Reduction in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Martinez-Berman, L., Johnson, B. T. (2023). Cognitive Ability Moderates the Accuracy of Self-Reported Perfect Pitch. Manuscript in Preparation.\n\nData Code PDF OSF\n\nJané, M. B., Uanhoro, J., Boika, N., Steele, J., Batinovic, L., Johnson, B. T. (2024). Predictive Validity of the Unified Tertiary Matriculation Examination on University Grades: A Living Meta-Analysis. https://matthewbjane.com/utme-validity-meta-analysis/ Manuscript in Preparation.\n\nData Code PDF OSF"
  },
  {
    "objectID": "blog-posts/blog-post-6.html#introduction",
    "href": "blog-posts/blog-post-6.html#introduction",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Introduction",
    "text": "Introduction\nThe article by Rausch and Haidt (R&H) conducts a re-analysis of a meta-analysis by Ferguson (2024) on RCTs assessing the causal effect (for the sake of this post, I am avoiding any causal inference issues) of social media usage on outcomes related to well-being (e.g., life satisfaction, anxiety). However, the re-analysis conducted by R&H has severe flaws. Generally speaking, if you conduct a re-analysis of any study you should be certain that the re-analysis is of higher methodological quality than the original study. That is not the case here. This blog will get a bit technical, but it is important because meta-analysis isn’t just crunching numbers across studies. There is a formal statistical model that we should adhere to to actually produce coherent and interpretable results.\nThe original meta-analysis by Ferguson (2024) is not without a number of flaws, but I think it accomplished what it set out to do. Notably, the meta-analysis was pre-registered, transparent, and had publicly available data. The only problem with the open data is that the statistics used to calculate the effect sizes (reported t-statistics, means, SDs, etc.) are completely missing from the database which makes reproducing it a lot harder.\nFerguson (2024)’s meta-analysis has a couple things that stood out to me as I was skimmed through it. It seems that all measures related to mental health and well-being are combined even if they are conceptually quite different (e.g., self-esteem and anxiety). When a study reported multiple different outcome measures, Ferguson decided to pool them together presumably to handle the dependency in effects. This is not an ideal strategy for two reasons, it assumes there is no heterogeneity between outcomes and it also loses specificity in the outcome. It would have been nice to see narrower bins of constructs meta-analyzed separately such as clinical anxiety/depression measures or life satisfaction/subjective well-being measures.\nAlso, the results convey a common misinterpretation of the estimate in a random effects meta-analysis. The meta-analytic estimate was communicated as follows:\n\nAs can be seen the overall estimate for d across studies was 0.088, which was nonsignificant and well below the SESOI (r = .10, d = 0.21).\n\nWhat does an “overall estimate for d” actually mean (note: d is the effect size, we will talk about this in the next section)? Is there a true d that we are estimating by averaging the d values from all these studies together? No. A random-effects meta-analysis is estimating the mean of true effect sizes. That is, there exists a distribution of true effects and the meta-analytic estimate conveys the central tendency. Therefore, the meta-analytic estimate can not tell you whether the the intervention produces an effect of d = .088 (lets ignore the variability in this estimate for the moment), it only tells you it has a an effect of d = .088, on average. Heterogeneity in effect sizes describes how wide the distribution of true effects is. You could have genuine true effects that are very positive (i.e., social media use is detrimental) and very negative (i.e., social media use is beneficial) even if the meta-analytic estimate is zero. There also could be study-level characteristics that could account for this heterogeneity, for instance, studies using a particular type of experimental design or a certain outcome shows a positive effect while others don’t.\nA side note: I also came across this sentence from Ferguson (2024)’s meta-analysis on page 3:\n\nGiven the high power of meta-analysis, almost all meta-analyses are “statistically significant.”\n\nI have heard this before but it is just not true, in general. In a random effects meta-analysis where you have both within and between study variance can give pretty wide confidence intervals and large p-values. This would be a reasonable statement if you are referring to a fixed effect meta-analysis with a large total sample size or a random effect meta-analysis with a ton of studies. Neither of which are true in this case."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#lets-get-into-it",
    "href": "blog-posts/blog-post-6.html#lets-get-into-it",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Let’s get into it",
    "text": "Let’s get into it\nThe first part of this blog sets up a statistical meta-analysis model called a random-effects model. It is important to keep in mind that a meta-analysis is not throwing a bunch of standardized effects into a calculator and computing a mean. We need a coherent statistical model that allows us to draw meaningful inferences from the data."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#effect-size-of-interest",
    "href": "blog-posts/blog-post-6.html#effect-size-of-interest",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Effect Size of Interest",
    "text": "Effect Size of Interest\nFirst thing we should do is to define the effect size we are using to quantify the effect of social media on well-being related outcomes. The article by Ferguson (2024) says they used a standardized mean difference known as a Cohen’s d to quantify the difference between the post-test scores in the social media using group and the non-social media using group (or some variation of that). You may think “we should be controlling for baseline differences between treatment and control groups”, well in an RCT subject’s are randomly allocated to each group so their baseline scores should be, on average, the same. We can define a (true) effect size \\(\\delta\\) as,\n\\[\n\\delta = \\frac{\\mu_\\textrm{social} - \\mu_\\textrm{no social}}{\\sigma}\n\\tag{1}\\]\nwhere \\(\\mu_\\textrm{social}\\) and \\(\\mu_\\textrm{no social}\\) is the mean of post-test scores for the social media using group and the non-social media using group (i.e., control group), respectively. The standardizer \\(\\sigma\\) is the within-group standard deviation and it assumes that both the social media and non-social media group have the same standard deviation (probably a fine assumption 🤷). A sample estimator of the effect size will be denoted as \\(d\\) and it represents the effect size we actually observe from the results of a study,\n\\[\nd = \\frac{m_\\textrm{social} - m_\\textrm{no social}}{s_\\mathrm{pooled}},\n\\tag{2}\\]\nwhere English letters denote the sample estimators of the corresponding parameters in Equation 1. The standardizer \\(s_\\mathrm{pooled}\\) here is the pooled standard deviation. Since we only have access to the standard deviation of each group we have to pool them together to get a more precise estimate of \\(\\sigma\\) (again, we are assuming the standard deviations within both groups are equal in the population),\n\\[\ns_\\mathrm{pooled} = \\sqrt{\\frac{(n_\\textrm{social}-1)s^2_\\textrm{social} + (n_\\textrm{no social}-1)s^2_\\textrm{no social}}{n_\\textrm{social} + n_\\textrm{no social} - 2}}\n\\]\nI will note here, that there was no correction for small sample bias in the original meta-analysis as far as I could tell so I will not apply any correction here."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#statistical-model",
    "href": "blog-posts/blog-post-6.html#statistical-model",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Statistical Model",
    "text": "Statistical Model\nUltimately, the observed effect size \\(d\\) is a sample estimate of the true effect size \\(\\delta\\) and therefore will contain error. This error is referred to as sampling error and is defined as the difference between the observed effect size and the true effect size. For a given study \\(i\\), we can relate the observed effect size \\(d_i\\) and the true effect size \\(\\delta_i\\) by including a sampling error term (\\(\\varepsilon_i\\)),\n\\[\nd_i = \\delta_i + \\varepsilon_i\n\\] where \\(\\varepsilon_i\\) is literally defined as \\(\\varepsilon_i = d_i - \\delta_i\\). The extent to which sampling errors are obscuring our effect size can be captured by the sampling variance. We can assume that sampling errors are normally distributed and evenly distributed around zero such that, \\(\\varepsilon_i \\sim \\mathcal{N}(0,v_i)\\) where \\(v_i\\) is the sampling variance for study \\(i\\). The sampling variance for a given study is dependent on the sample size with larger samples having more precision and thus less variance. Assuming normally distributed data, the sampling variance of the standardized mean difference (#eq-sample-effect) is calculated with the following formula,\n\\[\nv = \\left(\\frac{n-1}{n-3}\\right) \\left(\\frac{n}{n_\\textrm{social}n_\\textrm{no social}}\\right)\\left( \\frac{d^2}{2n}\\right)\n\\]\nwhere \\(n=n_\\textrm{social}+n_\\textrm{no social}\\). However unfortunately, the available dataset from Ferguson (2024) did not provide the sample sizes within each group so instead we will have to assume they are equal (\\(n_\\textrm{social}=n_\\textrm{no social}\\)) and use the simplified formula,\n\\[\nv = \\left(\\frac{n-1}{n-3}\\right) \\left(\\frac{4}{n}\\right)\\left(1 + \\frac{d^2}{8}\\right)\n\\]\nNow we might expect that any variation in effect sizes across studies is attributable to variance in random sampling errors, however this is unlikely to be the case. In reality, effect sizes tend to vary across studies above and beyond what we would expect from sampling error alone. This extra variation (known as heterogeneity) in effect sizes could be due to sampling from different populations, utilizing different methodologies, using different measures, etc.\nRecall that we modeled an observed effect size as the sum of the true effect size for that study and sampling error \\(d_i = \\delta_i + \\varepsilon_i\\). We defined the distribution of sampling errors so now we can also define the distribution of true effect sizes as \\(\\delta_i \\sim \\mathcal{N}(\\mu_\\delta,\\tau^2)\\) where \\(\\mu_\\delta\\) is the mean of true effect sizes across studies and \\(\\tau^2\\) is the variance of true effect sizes (i.e., heterogeneity). Ultimately our goal in a meta-analysis is to estimate both \\(\\mu_\\delta\\) and \\(\\tau\\). The common misinterpretation in (random-effect) meta-analyses that I described in the introduction is that a meta-analysis is estimating a single true effect size. If there is heterogeneity (\\(\\tau^2 &gt; 0\\)) then there is a distribution of true effects that we must consider."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#parameter-estimation",
    "href": "blog-posts/blog-post-6.html#parameter-estimation",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nIn meta-analysis we are trying to estimate particular parameters (i.e., the mean \\(\\mu_\\delta\\) and variance \\(\\tau\\) of true effect sizes) just like any other statistical model. Simply averaging study effect sizes to obtain some “overall” effect size is not a principled statistical approach. This is why we went through the trouble of describing a random effects model in the previous section. We want to estimate the mean of true effect sizes (not the “overall effect size” or “the true effect size”). To estimate the mean of true effects \\(\\mu_\\delta\\) we can not just calculate the arithmetic mean of observed effect sizes. Instead we must use appropriate weights that take advantage of differential precision across studies. Particularly, we want to weight each study by the inverse variance,\n\\[\nw_i = \\frac{1}{\\tau^2 + v_i}\n\\] This will preferentially weight studies with less sampling variance (i.e., larger sample sizes). Also, since \\(\\tau^2\\) is the same for all studies, the larger \\(\\tau^2\\) is relative to \\(v_i\\) then the weights will become more equal across studies. We can then use these weights to estimate the mean true effect size by taking a weighted average of observed effect sizes across \\(k\\) studies,\n\\[\n\\hat{\\mu}_\\delta = \\frac{\\sum^k_{i=1} w_i d_i}{\\sum^k_{i=1} w_i}\n\\] where the little carrot \\(\\hat{\\cdot}\\) denotes an estimate. We can also compute the 95% confidence interval of \\(\\hat{\\mu}_\\delta\\) (the set of plausible values of the actual mean that are compatible with the data) using the Hartung, Knapp, Sidik and Jonkman method (this adjusts for poor coverage rates in small \\(k\\) meta-analyses, which is the case here),\n\\[\nCI =  \\hat{\\mu}_\\delta \\pm 1.96 \\cdot\\sqrt{\\frac{\\sum^k_{i=1}w_i(d_i-\\mu_\\delta)^2}{(k-1)\\sum^k_{i=1}w_i}}\n\\]\nThe last two things we want to estimate are the variance in true effects \\(\\tau^2\\) and what is known as a prediction interval. The variance in true effects is estimated a variety of different ways and we won’t go through all of them here. Instead we will exclusively use a restricted maximum likelihood estimator (REML) which is an iterative estimation procedure. Ferguson (2024) uses a maximum likelihood estimator (ML), however ML tends to under-estimate the heterogeneity. The 95% prediction interval uses the information from \\(\\hat{\\tau}\\) to construct an interval that can be interpreted as the interval where 95% of true effect sizes fall.\n\\[\nPI = \\hat{\\mu}_\\delta \\pm 1.96 \\cdot\\hat\\tau\n\\]\nSo just keep in mind that the \\(CI\\) describes the variability in the estimate of the mean and \\(PI\\) describes the variability in the true effects."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#summarizing-and-reproducing-ferguson2024",
    "href": "blog-posts/blog-post-6.html#summarizing-and-reproducing-ferguson2024",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Summarizing and reproducing Ferguson (2024)",
    "text": "Summarizing and reproducing Ferguson (2024)\nOkay now that we have set up our model, we can now actually look at the data from the meta-analysis. Let’s load in some R packages and the dataset from the OSF repository referenced in Ferguson (2024).\n\n# load in packages\nlibrary(osfr)\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(readxl)\nlibrary(psychmeta)\n\n# download data from OSF repo\ninvisible(\n  osf_retrieve_file(\"jcha2\") %&gt;%\n    osf_download(conflicts = \"overwrite\")\n)\n\n# load in dataset and filter out studies without data\ndat &lt;- read_excel(\"Best Practices Coding Experiments.xlsx\",\n                  sheet = 1) %&gt;%\n  filter(!is.na(d))\n\n# display effect size data for first 6 studies\nhead(dat[c(\"Citation\",\"n\",\"d\")])\n\n# A tibble: 6 × 3\n  Citation                    n      d\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcott 2020              1661  0.09 \n2 Brailovskaia 2020         286  0.154\n3 Brailovskaia 2022         322  0    \n4 Collins and Edgers 2022   121 -0.138\n5 Deters & Mehl 2013         86 -0.207\n6 Faulhaber et al., 2023    230  0.484\n\n\nThe data only gives us the effect size d and the sample size n for each study so we need to calculate the sampling variance. Using the psychmeta package we can calculate the sampling variance for each study as follows,\n\n# compute sampling variance\ndat &lt;- dat %&gt;% \n  mutate(v = var_error_d(d = d, n1 = n))\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\")])\n\n# A tibble: 6 × 4\n  Citation                    n      d       v\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241\n2 Brailovskaia 2020         286  0.154 0.0141 \n3 Brailovskaia 2022         322  0     0.0125 \n4 Collins and Edgers 2022   121 -0.138 0.0337 \n5 Deters & Mehl 2013         86 -0.207 0.0479 \n6 Faulhaber et al., 2023    230  0.484 0.0181 \n\n\nNow we are set up to fit a random effects model. Using the metafor package we can use the REML method for estimating the heterogeneity and the Hartung-Knapp-Sidik-Jonkman method for obtaining the confidence intervals and p-values.\n\nmdl &lt;- rma(yi = d, \n           vi = v, \n           data = dat,\n           method = \"REML\",\n           test=\"knha\")\nmdl\n\n\nRandom-Effects Model (k = 27; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0564 (SE = 0.0230)\ntau (square root of estimated tau^2 value):      0.2374\nI^2 (total heterogeneity / total variability):   76.62%\nH^2 (total variability / sampling variability):  4.28\n\nTest for Heterogeneity:\nQ(df = 26) = 91.9083, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval    ci.lb   ci.ub    \n  0.0879  0.0576  1.5249  26  0.1394  -0.0306  0.2063    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe are pretty much able to reproduce the results seen in Ferguson (2024) with slight differences probably due to having to assume equal sample sizes between groups, using a different heterogeneity estimator and using the Hartung-Knapp-Sidik-Jonkman method for the confidence intervals. Let’s get the 95% prediction interval as well.\n\npredict.rma(mdl)\n\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.0879 0.0576 -0.0306 0.2063 -0.4143 0.5901 \n\n\nSo, 95% of true effects fall between -.41 and .59 according to our model."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#rhs-analysis",
    "href": "blog-posts/blog-post-6.html#rhs-analysis",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "R&H’s analysis",
    "text": "R&H’s analysis\nR&H conduct a sub-group analysis by chopping up the studies into three bins. First issue here is that this analysis is completely post-hoc and who knows if they created these bins after trying various combinations to reach the conclusion that they desired. R&H separate the 27 studies into three bins (this is verbatim from the article):\n\nMulti-week reduction experiments. These ten studies examined the impact of reducing social media use for at least two weeks, allowing withdrawal symptoms to dissipate.\nShort (one week or less) reduction experiments. These ten studies examined brief periods of abstinence from social media use, which are likely to pick up withdrawal symptoms from heavy users.\nExposure experiments. In these seven studies, the ‘treatment’ is typically brief exposure to some kind of social media, such as requiring high school students to look at their Facebook or Instagram page for 10 minutes.\n\nThe second problem we see is that the first two groups are separated by dichotomizing a naturally continuous variable (length of intervention). Sure it makes the data more digestible, but you simply lose information. This is generally considered bad statistical practice and we will see why a little later.\nTo reproduce R&H’s analysis, we will add a column to the data set that denotes the group membership (1 = Multi-week reduction experiments, 2 = One week reduction experiments, 3 = Less than one week reduction experiments, 4 = Exposure experiments) of each study.\n\ndat &lt;- dat %&gt;%\n  mutate(subgroup = c(1, # Alcott 2020\n                      1, # Brailovskaia 2020\n                      1, # Brailovskaia 2022\n                      1, # Collins and Edgers 2022\n                      4, # Deters & Mehl 2013\n                      1, # Faulhaber et al., 2023\n                      3, # Gajdics 2022\n                      1, # Hall et al. 2021\n                      1, # Hunt 2018\n                      1, # Hunt 2021\n                      2, # Kleefeld dissertation\n                      2, # Lambert 2022\n                      4, # Lepp 2022\n                      2, # Mahalingham 2023\n                      3, # Mitev 2021\n                      4, # Ozimek 2020\n                      3, # Przybylski 2021\n                      4, # Sagioglou 2014 study 2\"\n                      4, # Tartaglia\n                      1, # Thai 2021\n                      1, # Thai 2023\n                      2, # Tromholt 2016\n                      2, # Valley2019\n                      2, # van wezel 2021\n                      3, # Vanman 2018\n                      4, # Ward 2017\n                      4  # Yuen et al., 2019\n                      ))\n\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\",\"subgroup\")])\n\n# A tibble: 6 × 5\n  Citation                    n      d       v subgroup\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241        1\n2 Brailovskaia 2020         286  0.154 0.0141         1\n3 Brailovskaia 2022         322  0     0.0125         1\n4 Collins and Edgers 2022   121 -0.138 0.0337         1\n5 Deters & Mehl 2013         86 -0.207 0.0479         4\n6 Faulhaber et al., 2023    230  0.484 0.0181         1\n\n\nNow that we have the subgroup column, we can conduct the sub-group analysis. Here comes the next problem, R&H decide to simply calculates the arithmetic (unweighted) mean of the observed effect sizes within each group. The problem is that they aren’t capitalizing on the fact that different studies have different levels of precision. The only way it would make sense to compute an unweighted average with respect to the statistical model we are employing is if sampling error didn’t exist (it does) or heterogeneity is infinite (its not), so either way its just the wrong approach.\nLet’s first reproduce R&H’s results by taking the unweighted average of observed effect sizes.\n\nresults &lt;- summarize(.data = dat,\n                     mean_d = mean(d),\n                     k_studies = n(),\n                     .by = subgroup)\n\n# reorder the subgroup display\nresults[c(1,4,3,2),]\n\n# A tibble: 4 × 3\n  subgroup  mean_d k_studies\n     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;\n1        1  0.204         10\n2        2  0.0868         6\n3        3 -0.172          4\n4        4  0.0566         7\n\n\nAll the values are accurate. We can compare it to the table shown in R&H’s article:\n\nOkay we see a positive average effect for the multi-week reduction experiments, but lets now use proper inverse-variance weights to calculate the average. This time we will model the groups as dummy predictors and specify the proper contrasts for each group:\n\n# construct a meta-analytic model with subgroup\n# as a dummy-coded categorical predictor\nmdl &lt;- rma(d ~ factor(subgroup,levels = c(1,2,3,4)), \n           vi = v, \n           data = dat,\n           test=\"knha\")\n\n# acquire subgroup-level mean estimates\nestimates &lt;- as.data.frame(\n  predict(mdl, \n          rbind(g1 = c(0,0,0),\n                g2 = c(1,0,0),\n                g3 = c(0,1,0),\n                g4 = c(0,0,1))))\n\n# print results\nround(data.frame(group = 1:4, mean_delta = estimates$pred),3)\n\n  group mean_delta\n1     1      0.177\n2     2      0.154\n3     3     -0.172\n4     4      0.082\n\n\nAs we can see the estimates are a bit different. Another thing that is completely missing from the R&H analysis is any indication of variability in their mean estimates and their is no indication of heterogeneity. It is conveyed as a fixed quantity, however just because we combine many studies together does not mean that uncertainty ceases to exist.\n\nround(data.frame(group = 1:4, \n                 mean_delta = estimates$pred,\n                 CI_low = estimates$ci.lb,\n                 CI_high = estimates$ci.ub,\n                 PI_low = estimates$pi.lb,\n                 PI_high = estimates$pi.ub),3)\n\n  group mean_delta CI_low CI_high PI_low PI_high\n1     1      0.177 -0.005   0.360 -0.296   0.651\n2     2      0.154 -0.098   0.407 -0.350   0.659\n3     3     -0.172 -0.443   0.099 -0.686   0.342\n4     4      0.082 -0.151   0.315 -0.414   0.577\n\n\nTrue effects for group 1 (multi-week reduction studies, i.e., the focal group) are quite variable and can range from -0.296 to 0.651. An interesting point to bring up is that Ferguson (2024) set the smallest effect size of interest (SESOI) to be d = .21 in his pre-registration (equivalent to a Pearson correlation of r = .10). However, that does not help us much here since the upper bound of the confidence interval is well above d = .21 so you can’t really make the claim that that there is no clinically meaningful effect, on average. And you definitely can’t make the claim that all/most true effects are not clinically meaningful since the upper bound of the PI is 0.651. Truthfully, the SESOI established in Ferguson (2024) does not seem to have any good theoretical or practical justification outside of “seems reasonable” so it is unclear what the utility of this arbitrary boundary is in this case?"
  },
  {
    "objectID": "blog-posts/blog-post-6.html#properly-modeling-the-length-of-intervention-in-reduction-studies",
    "href": "blog-posts/blog-post-6.html#properly-modeling-the-length-of-intervention-in-reduction-studies",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Properly modeling the length of intervention in reduction studies",
    "text": "Properly modeling the length of intervention in reduction studies\nInstead of binning the length of the intervention into groups I will use the weeks reported in the R&H article to create a continuous moderator for reduction studies only.\n\ndat &lt;- dat %&gt;%\n  mutate(length_weeks = c(4, # Alcott 2020\n                          2, # Brailovskaia 2020\n                          2, # Brailovskaia 2022\n                          10, # Collins and Edgers 2022\n                          NA, # Deters & Mehl 2013\n                          2, # Faulhaber et al., 2023\n                          1/7, # Gajdics 2022\n                          2.5, # Hall et al. 2021\n                          1, # Hunt 2018\n                          1, # Hunt 2021\n                          1, # Kleefeld dissertation\n                          1, # Lambert 2022\n                          NA, # Lepp 2022\n                          1, # Mahalingham 2023\n                          1/7, # Mitev 2021\n                          NA, # Ozimek 2020\n                          1/7, # Przybylski 2021\n                          NA, # Sagioglou 2014 study 2\"\n                          NA, # Tartaglia\n                          3, # Thai 2021\n                          3, # Thai 2023\n                          1, # Tromholt 2016\n                          1, # Valley2019\n                          1, # van wezel 2021\n                          5/7, # Vanman 2018\n                          NA, # Ward 2017\n                          NA  # Yuen et al., 2019\n  ))\n\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\",\"length_weeks\")])\n\n# A tibble: 6 × 5\n  Citation                    n      d       v length_weeks\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241            4\n2 Brailovskaia 2020         286  0.154 0.0141             2\n3 Brailovskaia 2022         322  0     0.0125             2\n4 Collins and Edgers 2022   121 -0.138 0.0337            10\n5 Deters & Mehl 2013         86 -0.207 0.0479            NA\n6 Faulhaber et al., 2023    230  0.484 0.0181             2\n\n\nOkay so now we have the moderator we can construct a linear meta-regression model of the form,\n\\[\nd_i = b_0 + b_1 X_\\mathrm{length} + u_i + \\varepsilon_i\n\\] Notice now that true effects how is distributed around the regression line such that the distribution of true effects is now\n\\[\n\\delta_i \\sim \\mathcal{N}(b_0 + b_1 X_\\mathrm{length},\\tau^2)\n\\]\n\nmdl_weeks &lt;- rma(d ~ length_weeks,\n                 vi = v,\n                 data = dat,\n                 method = \"REML\",\n                 test = \"knha\")\n\nWarning: 7 studies with NAs omitted from model fitting.\n\nmdl_weeks\n\n\nMixed-Effects Model (k = 20; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0647 (SE = 0.0299)\ntau (square root of estimated tau^2 value):             0.2544\nI^2 (residual heterogeneity / unaccounted variability): 79.40%\nH^2 (unaccounted variability / sampling variability):   4.85\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 18) = 75.7503, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 18) = 0.0018, p-val = 0.9667\n\nModel Results:\n\n              estimate      se    tval  df    pval    ci.lb   ci.ub    \nintrcpt         0.0884  0.0922  0.9581  18  0.3507  -0.1054  0.2821    \nlength_weeks    0.0014  0.0330  0.0423  18  0.9667  -0.0678  0.0706    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOkay so as it turns out there appears to be little/no effect of the length of the intervention on the effect size. Let us plot out the data to see what the relationship actually looks like:\n\n\n\n\n\n\n\n\n\nThe regression line looks almost parallel to the d = 0 line (the grey line). You may notice the outlier at 10 weeks and you may want to remove it from the analysis. I would ask why? The data point does not appear to be a coding error or a mistake? It is a valid data point and piece of evidence that should not be taken out of the analysis post-hoc. Hell, it could be that the the positive effect of not being on social media only lasts a few weeks and then fades out (not an uncommon trend in psychology interventions). Anyways, any reason to remove that “outlier” would seem to me like a post-hoc rationalization."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#discussion",
    "href": "blog-posts/blog-post-6.html#discussion",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Discussion",
    "text": "Discussion\nThe re-analysis of Ferguson (2024)’s meta-analysis by R&H does not have adequate statistical rigor to build a “case for causality”. Post-hoc subgroup analyses conducted by R&H did not use a principled statistical model, they did not report any variability in their estimates, the estimates themselves were sub-optimal (unweighted averaging), and they misinterpreted the point-estimates that they calculated. They did not do a proper comparison of their point-estimates and instead they treated them as fixed quantities and simply claimed that these average effect sizes are different without consideration of variability in their estimation procedure. Their conclusion claimed that:\n\nwe still found that his data supports (rather than undermines) the contention by some scholars that “reductions in social media time would improve adolescent mental health,” at least as long as the reductions continue for two weeks or longer.\n\nBased on my re-analysis of R&H’s re-analysis I find that the intervention length when modeled properly as a continuous variable (since it literally is) does not support this claim.\nIf I did anything incorrectly or if I did not use best practices, I take full responsibility and you can let me know by sending me a DM on twitter or emailing me at matthewbjane@gmail.com."
  },
  {
    "objectID": "blog-posts/blog-post-4.html#defining-a-squared-effect-size",
    "href": "blog-posts/blog-post-4.html#defining-a-squared-effect-size",
    "title": "Sampling Distributions of Squared Effect Sizes and their Implication for Meta-Analysis",
    "section": "Defining a Squared Effect Size",
    "text": "Defining a Squared Effect Size\nSometimes squared effect sizes are used in order to interpret results (e.g., \\(R^2\\), \\(\\eta^2\\)). However these squared indices are biased in small sample sizes. To see why this is the case we can describe the mathematically. Generally, lets say we have some observed effect size \\(h_i\\) for a sample \\(i\\). This can be modeled as a function of a population correlation \\(\\theta\\) and sampling error \\(e_i\\),\n\\[\nh_i = \\theta + e_i\n\\]\nWhere the expectation of \\(h_i\\) (i.e., the average) upon repeated sampling is equal to \\(\\theta\\). More importantly for us, the expectation of sampling error \\(e_i\\) across repeated samples would be zero. For the expectation of \\(e_i\\) to be zero, \\(e\\) would need to be randomly distributed around zero with both positive and negative values. Let us see what happens to the model once we square both sides to obtain \\(h^2\\):\n\\[\nh^2_i = (\\theta + e_i)^2\n\\]\nand thus,\n\\[\nh^2_i = \\theta^2 + 2\\theta e_i + e_i^2\n\\]\nThe squared effect sizes is now no longer an unbiased estimate of theta. The bias in \\(h_i^2\\) can be seen by taking the expected value of both sides (note that the \\(\\text{Var}(e_i)\\) is the sampling error variance and \\(\\mathbb{E}[\\cdot]\\) is the expected value or the mean across repeated samples):\n\\[\\begin{align}\n\\mathbb{E}[h^2_i] & = \\mathbb{E}[\\theta^2 + 2\\theta e_i + e_i^2] \\\\[.5em]\n                & = \\theta^2 + 2\\theta \\times \\mathbb{E}[e_i] + \\mathbb{E}[e^2_i]\\\\[.5em]\n                & = \\theta^2 + \\mathbb{E}[e_i^2] \\\\[.5em]\n                & = \\theta^2 + \\left(\\text{Var}[e_i] + \\mathbb{E}[e_i]^2\\right) \\\\[.5em]\n                & = \\theta^2 + \\text{Var}[e_i] \\\\\n\\end{align}\\]\nSince the sampling error variance is always positive (unless the sample size was infinite…), there will be a positive bias in expected value of \\(h_i^2\\). It is important to note that the variance in sampling error (i.e., \\(\\text{Var}(e_i)\\)) will be larger with smaller sample sizes."
  },
  {
    "objectID": "blog-posts/blog-post-4.html#simulated-example",
    "href": "blog-posts/blog-post-4.html#simulated-example",
    "title": "Sampling Distributions of Squared Effect Sizes and their Implication for Meta-Analysis",
    "section": "Simulated Example",
    "text": "Simulated Example\nLets assume that in the population, the squared correlation between predicted and observed outcome (\\(R^2\\)) is zero, that is, the independent variables (\\(X_1\\),\\(X_2\\), and \\(X_3\\)) do not explain any variance in the dependent variable (\\(Y\\)). Lets simulate two studies, where one study has a sample size of 100 and the other has a sample size of 25. Then lets replicate these studies 5,000 times. In the first figure we will see that the mean value of \\(R^2\\) is not zero even though the population value is. And with the smaller sample sizes within each study (\\(N = 25\\)), the mean \\(R^2\\) is larger. The plots below show the distribution of \\(R^2\\) using the ggdist package (Kay 2023).\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(ThemePark)\nlibrary(patchwork)\nlibrary(latex2exp)\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR21 &lt;- R22 &lt;- R23 &lt;- c()\nfor(i in 1:5000){\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N,0,1)\n  x2 &lt;- rnorm(N,0,1)\n  x3 &lt;- rnorm(N,0,1)\n  y &lt;- rnorm(N,0,1)\n  \n  # linear regression model\n  mdl &lt;- lm(y ~ x1 + x2 + x3)\n  R21[i] &lt;- summary(mdl)$r.squared\n\n  # linear regression model\n  mdl &lt;- lm(y ~ x1 + x2 + x3,subset = sample(1:N,25))\n  R22[i] &lt;- summary(mdl)$r.squared\n}\n\n\nh1 &lt;- ggplot(data=NULL, aes(R21)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_slabinterval(color = barbie_theme_colors['dark'], \n                    fill = barbie_theme_colors['light']) +\n  scale_x_continuous(limits = c(0,.5)) +\n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = \"\",x=TeX(\"$R^2$\"), \n       subtitle =  TeX(paste0('N=', 100, ', Ave $R^2$=', round(mean(R21),2))))\n\nh2 &lt;- ggplot(data=NULL, aes(R22)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_slabinterval(color = barbie_theme_colors['dark'], \n                    fill = barbie_theme_colors['light']) +\n  scale_x_continuous(limits = c(0,.5)) +\n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = \"\",x=TeX(\"$R^2$\"), \n       subtitle = TeX(paste0('N=', 25, ', Ave $R^2$=', round(mean(R22),2))))\n  \nh1 + h2 & plot_annotation(theme = theme(plot.background = element_rect(color=\"#f1f9fa\",fill =\"#f1f9fa\")))\n\n\n\n\n\n\n\n\n\nIf we were to run a meta-analysis on all 5,000 studies and found a meta-analytic average \\(R^2\\) of .13, we may conclude that there is a meaningful prediction/effect, however, this could simply be an artifact of sampling error. The plot below shows how the mean \\(R^2\\) changes as you vary the sample size.\n\n\nCode\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR2 &lt;- N &lt;- c()\nfor(i in 1:5000){\n  N[i] &lt;- runif(1,10,70)\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N[i] ,0,1)\n  x2 &lt;- rnorm(N[i] ,0,1)\n  x3 &lt;- rnorm(N[i] ,0,1)\n  y &lt;- rnorm(N[i] ,0,1)\n  # linear regression model\n  mdl &lt;- lm(y ~ x1)\n  R2[i] &lt;- summary(mdl)$r.squared\n}\n\n\nggplot(data=NULL, aes(N,R2)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_smooth( color = barbie_theme_colors['dark']) + \n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = TeX(\"Mean $R^2$\"),x=\"Sample Size\", title = \"\")+ \n  expand_limits(y = 0)\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "blog-posts/blog-post-4.html#conclusion",
    "href": "blog-posts/blog-post-4.html#conclusion",
    "title": "Sampling Distributions of Squared Effect Sizes and their Implication for Meta-Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn a meta-analysis, it is always best to use directional effect sizes that can be both positive and negative such as Cohen’s \\(d\\)’s or Pearson correlations. There are other strategies for meta-analyzing \\(R^2\\) values such as using individual person data meta-analysis (using the raw data for each study) or meta-analyzing the correlation matrix between predictors and outcomes first and then running the regression model on the meta-analytic correlation matrix (this would not allow for non-linear terms though). If it is necessary to meta-analyze \\(R^2\\) you will want to use the adjusted \\(R^2\\) value. See the plot below that shows the adjusted \\(R^2\\) across different sample sizes\n\n\nCode\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR2 &lt;- N &lt;- c()\nfor(i in 1:5000){\n  N[i] &lt;- runif(1,10,70)\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N[i] ,0,1)\n  x2 &lt;- rnorm(N[i] ,0,1)\n  x3 &lt;- rnorm(N[i] ,0,1)\n  y &lt;- rnorm(N[i] ,0,1)\n  # linear regression model\n  mdl &lt;- lm(y ~ x1)\n  R2[i] &lt;- summary(mdl)$adj.r.squared\n}\n\n\nggplot(data=NULL, aes(N,R2)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_smooth( color = barbie_theme_colors['dark']) + \n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = TeX(\"Mean adj. $R^2$\"),x=\"Sample Size\", title = \"\")+ \n  ylim(c(-.4,.4)) +\n  expand_limits(y = 0)\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "href": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Step 1: Obtain the necessary statistics",
    "text": "Step 1: Obtain the necessary statistics\nIn order to calculate the pre/post correlation (\\(r\\)), we need the standard deviation (SD) of pre-test scores (\\(SD_{pre}\\)), the SD of post-test scores (\\(SD_{post}\\)), the mean change (\\(M_{change}\\)), the paired t-statistic (\\(t\\)), the sample size (\\(N\\)). In this blog post, we are assuming that the change score standard deviation (\\(SD_{change}\\)) is unavailable to us. If the pre-test SD is available, but the post-test SD is unavailable, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis (\\(\\overline{SD}_{ratio}\\); see the blog post on 9/8/2023), then we can approximate the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nSD_{post}\\approx \\bar{SD}_{ratio}\\times SD_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal. If the study reports an F-statistic from a one-way repeated measures ANOVA, the F-statistic is equal to the square of the t-statistic,\n\\[\nt = \\sqrt{F}\n\\]\nEnsure that you apply the correct sign (negative or positive) to the t-statistic, since the F-statistic is always positive.\n\n# Obtain values\nM_change &lt;- 3\nSD_post &lt;- 11\nSD_pre &lt;- 9\nt &lt;- 2.50\nN &lt;- 50"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nLets start by figuring out how to find the change score SD. The paired t-statistic is defined as the mean change score divided by the standard error of change scores, such that, \\[\nt = \\frac{M_{change}}{SE_{change}}\n\\] Since we need the change score SD, we can use the definition of the standard error of the mean to put the t-statistic in terms of \\(SD_{change}\\): \\[\nSE_{change}=\\frac{SD_{change}}{\\sqrt{N}}\n\\] and therefore \\(t\\) can be expressed as,\n\\[\nt=\\frac{M_{change}}{\\left(\\frac{SD_{change}}{\\sqrt{N}}\\right)}\n\\]\nthen we just need to solve for \\(SD_{change}\\):\n\\[\nSD_{change}=\\frac{M_{change}\\times\\sqrt{N}}{t}\n\\]\nOkay so now let us recall the definition of change score SDs from the blog post on 9/8/2023. In that blog we discussed how to obtain the pre/post correlation from the change score SD, now that we have converted \\(t\\) to \\(SD_{change}\\), we can solve for the correlation in a similar way. First things first, the change score SD can be defined as, \\[\nSD_{change} = \\sqrt{SD^2_{pre} + SD^2_{post} - 2\\times r\\times SD_{pre}SD_{post}}\n\\]\nWe can re-arrange this to isolate the pre/post correlation (\\(r\\)),\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - SD^2_{change}}{2 \\times SD_{pre}\\times SD_{post}}\n\\]\nIn our case, the study did not report the change score SD, therefore we can replace it with our derived \\(SD_{change}\\) from a paired t-test:\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - \\left(\\frac{M_{change}\\times\\sqrt{N}}{t}\\right) ^2}{2 \\times SD_{pre}\\times SD_{post}}\n\\] Lets neaten this formulation up a tad:\n\\[\nr = \\frac{t^2\\left(SD^2_{pre} + SD^2_{post}\\right) - N\\times M^2_{change} }{2 \\times t^2 \\times SD_{pre}\\times SD_{post} }\n\\]\nIsn’t that just a beautiful thing?? So there you have it! the full equation for the pre/post correlation from a paired t-test! Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.657\""
  },
  {
    "objectID": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. The calculated correlation is exactly correct!\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nSD_pre &lt;- 9\nSD_post &lt;- 11\nr_true &lt;- .70\nM_pre &lt;- 20\nM_post &lt;- 25\nN &lt;- 100\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=N,\n               mu=c(M_pre,M_post),\n               Sigma = data.frame(x=c(SD_pre^2,r_true*SD_pre*SD_post),\n                                  y=c(r_true*SD_pre*SD_post,SD_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations, t-stats, and mean change\nSD_pre &lt;- sd(x_pre)\nSD_post &lt;- sd(x_post)\nt &lt;- mean(x_change) / (sd(x_change)/sqrt(N))\nM_change &lt;- mean(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\n# print results\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Email Twitter Github Scholar\n\n\nHello there! I’m Matthew B. Jané - I’m a PhD student in Quantitative Psychology at the University of Connecticut. My research involves developing statistical methods and software to aide in meta-analysis and evidence synthesis. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. My advisor is Dr. Blair T. Johnson and I am a member of the Systematic Health Action Research Program (SHARP). I am also on the editorial board for Psychological Bulletin as a methodological reviewer."
  },
  {
    "objectID": "Books.html",
    "href": "Books.html",
    "title": "Books",
    "section": "",
    "text": "Guide to Effect Sizes and Confidence Intervals      2024\n    Go to book\n  \n\n\n\n  \n    \n    \n  \n  \n    Artifact Corrections for Effect Sizes\n     Coming 2024\n    Coming Soon!"
  },
  {
    "objectID": "Consulting.html",
    "href": "Consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "consulting@matthewbjane.com\nI provide three main types of consulting services: statistical consulting, data visualization, and academic website development.\nIf you’re interested in learning more about how I can assist with your projects or if you have questions or require advice, please don’t hesitate to reach out. I offer a free 20-minute consultation to discuss your specific needs and determine how I can best support you. To schedule this consultation, feel free to contact me at consulting@matthewbjane.com. I will be pleased to support you every step of the way.\n\n\nI offer a wide array of services designed to cater to a broad spectrum of statistical needs, whether you’re an academic or part of the industry. Here’s a detailed list of the services I provide:\n\nMeta-analysis\nData-analysis\nData cleaning and management\nStatistical Modeling\nStatistical Software\nPsychometrics and Structural Equation Modeling\nR Programming\nComputationally Reproducible and Self-Contained Manuscripts\n\nI’ve structured my pricing to accommodate different client types, ensuring a transparent and cost-effective approach. The current hourly rates are as follows:\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/hr\n\n\nAcademic\n$130/hr\n\n\nStudent\n$90/hr\n\n\n\nProjects with a substantial scope (i.e., more than 20 hours worth of consulting work) can be structured so that their pricing will be done on a per-project rather than a per-hour basis.\n\n\n\nI can help you prepare high-quality data visualizations for academic publications, websites, slideshows, etc. There are two types of data visualizations that I provide: stand-alone diagrams and computationally reproducible figures.\nTo ensure that your specific needs are met, you can either send me a detailed description of the desired diagram via email (consulting@matthewbjane.com) with the following information:\n\nWhat kind of figure do you need?\nWhen do you realistically need to have it ready by?\nDo you have all the information needed to produce the figure?\nCould you provide an example of a figure you are trying to emulate?\n\nThen we can set up a free 20-minute virtual meeting to discuss the details further.\n\n\nStand-alone diagrams are used as visual representations of a scientific concept. Each diagram I produce will be made available as a high-resolution PNG, JPEG, and SVG file. For your convenience I included an example of my work below .\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/diagram\n\n\nAcademic\n$120/diagram\n\n\nStudent\n$80/diagram\n\n\n\n\n\n\n\n\nI can also help you generate figure panels that contain real or simulated data. For each panel, my service ensures that you will receive the final visual representation (as a high-resolution PNG or JPG), but also an R script containing the code for reproducing the figure. For your convenience, I’ve included examples of my work, showcasing both a two-panel figure (top) and a single-panel figure (bottom).\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/panel\n\n\nAcademic\n$130/panel\n\n\nStudent\n$100/panel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI can help you create your own academic/personal website powered through Quarto. Websites can be published through Quarto Pub or Github pages. For an example, my website matthewbjane.com is entirely created through Quarto. Below are some base packages that you can start with, however prices may vary if features are added or removed from these base packages:\n\n\n\nFeatures\nRate\n\n\n\n\nHome Page + CV + Blog + Favicon/Banner/Logo\n$1,100\n\n\nHome Page + CV + Blog\n$800\n\n\nHome Page + CV\n$550\n\n\n\n\n\n\nVelu Immonen, Graduate Student at Solent University:\n\nWith Matthew’s expert guidance, I navigated through my first meta-analysis with success. His support in effect size selection, model specification, and data visualisation proved indispensable. It felt like having ChatGPT on your ear, with the distinction that Matthew actually offered up-to-date best practices without making up citations – not to mention the human conversation. This service provided me with clarity and considerable time savings. I wholeheartedly endorse it for anyone delving into the world of meta-analysis.\n\nAlysson Enes, Graduate Student at Federal University of Paraná:\n\nMatthew helped me with a data visualization service. I spent weeks trying to solve a code in a figure, but I couldn’t find the solution. When I contacted Matthew, we had an initial meeting where he was extremely helpful in solving my problem. Matthew quickly sent me a guide with a full explanation of the problem, including how I could use the data visualization on other datasets. I recommend Matthew’s work to anyone who is having problems with data visualization."
  },
  {
    "objectID": "Consulting.html#statistical-consulting-services",
    "href": "Consulting.html#statistical-consulting-services",
    "title": "Consulting",
    "section": "",
    "text": "I offer a wide array of services designed to cater to a broad spectrum of statistical needs, whether you’re an academic or part of the industry. Here’s a detailed list of the services I provide:\n\nMeta-analysis\nData-analysis\nData cleaning and management\nStatistical Modeling\nStatistical Software\nPsychometrics and Structural Equation Modeling\nR Programming\nComputationally Reproducible and Self-Contained Manuscripts\n\nI’ve structured my pricing to accommodate different client types, ensuring a transparent and cost-effective approach. The current hourly rates are as follows:\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/hr\n\n\nAcademic\n$130/hr\n\n\nStudent\n$90/hr\n\n\n\nProjects with a substantial scope (i.e., more than 20 hours worth of consulting work) can be structured so that their pricing will be done on a per-project rather than a per-hour basis."
  },
  {
    "objectID": "Consulting.html#data-visualization-services",
    "href": "Consulting.html#data-visualization-services",
    "title": "Consulting",
    "section": "",
    "text": "I can help you prepare high-quality data visualizations for academic publications, websites, slideshows, etc. There are two types of data visualizations that I provide: stand-alone diagrams and computationally reproducible figures.\nTo ensure that your specific needs are met, you can either send me a detailed description of the desired diagram via email (consulting@matthewbjane.com) with the following information:\n\nWhat kind of figure do you need?\nWhen do you realistically need to have it ready by?\nDo you have all the information needed to produce the figure?\nCould you provide an example of a figure you are trying to emulate?\n\nThen we can set up a free 20-minute virtual meeting to discuss the details further.\n\n\nStand-alone diagrams are used as visual representations of a scientific concept. Each diagram I produce will be made available as a high-resolution PNG, JPEG, and SVG file. For your convenience I included an example of my work below .\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/diagram\n\n\nAcademic\n$120/diagram\n\n\nStudent\n$80/diagram\n\n\n\n\n\n\n\n\nI can also help you generate figure panels that contain real or simulated data. For each panel, my service ensures that you will receive the final visual representation (as a high-resolution PNG or JPG), but also an R script containing the code for reproducing the figure. For your convenience, I’ve included examples of my work, showcasing both a two-panel figure (top) and a single-panel figure (bottom).\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/panel\n\n\nAcademic\n$130/panel\n\n\nStudent\n$100/panel"
  },
  {
    "objectID": "Consulting.html#academic-websites-powered-by-quarto",
    "href": "Consulting.html#academic-websites-powered-by-quarto",
    "title": "Consulting",
    "section": "",
    "text": "I can help you create your own academic/personal website powered through Quarto. Websites can be published through Quarto Pub or Github pages. For an example, my website matthewbjane.com is entirely created through Quarto. Below are some base packages that you can start with, however prices may vary if features are added or removed from these base packages:\n\n\n\nFeatures\nRate\n\n\n\n\nHome Page + CV + Blog + Favicon/Banner/Logo\n$1,100\n\n\nHome Page + CV + Blog\n$800\n\n\nHome Page + CV\n$550"
  },
  {
    "objectID": "Consulting.html#testimonials",
    "href": "Consulting.html#testimonials",
    "title": "Consulting",
    "section": "",
    "text": "Velu Immonen, Graduate Student at Solent University:\n\nWith Matthew’s expert guidance, I navigated through my first meta-analysis with success. His support in effect size selection, model specification, and data visualisation proved indispensable. It felt like having ChatGPT on your ear, with the distinction that Matthew actually offered up-to-date best practices without making up citations – not to mention the human conversation. This service provided me with clarity and considerable time savings. I wholeheartedly endorse it for anyone delving into the world of meta-analysis.\n\nAlysson Enes, Graduate Student at Federal University of Paraná:\n\nMatthew helped me with a data visualization service. I spent weeks trying to solve a code in a figure, but I couldn’t find the solution. When I contacted Matthew, we had an initial meeting where he was extremely helpful in solving my problem. Matthew quickly sent me a guide with a full explanation of the problem, including how I could use the data visualization on other datasets. I recommend Matthew’s work to anyone who is having problems with data visualization."
  },
  {
    "objectID": "blog-posts/blog-post-2.html#note-the-assumptions",
    "href": "blog-posts/blog-post-2.html#note-the-assumptions",
    "title": "Approximating Standard Deviation from Inter-Quantile Range",
    "section": "Note the Assumptions!",
    "text": "Note the Assumptions!\nSince this method provides an approximation of a normal distribution it is important to point out that the method may be biased under different distributions. For example, if we try the same method on a Student’s t distribution with heavy tails (3.5 degrees of freedom), \\(S_{(75-25)}\\) would be equal to 1.50187 which is slightly larger than 1.34898 that we computed from the normal distribution. If we were to assume a normal distribution when it was truly a student’s t, the method would over-estimate the standard deviation by ~11%.\n\n\n\n\n\n\n\n\n\n\nApplying it to simulated data\nLets see how it performs in simulated sample of normally distributed data… Close enough!!\n\n# Set seed\nset.seed(343)\n\n# Simulate normal data (Mean = 10, SD = 5)\nX = rnorm(100,10,5)\n\n# Define IQR\nq25 &lt;- as.numeric(quantile(X,.25))\nq75 &lt;- as.numeric(quantile(X,.75))\nX_75_25 &lt;- q75 - q25\n\n# Compute IQR in SD units\nS_75_25 &lt;- qnorm(.75) - qnorm(.25)\n\n# Estimate standard deviation\nS_X &lt;-  X_75_25 / S_75_25\n\n# Print results\nprint(S_X)\n\n[1] 4.706901"
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "href": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Step 1: Obtain Pre-test, Post-test, and Change score standard deviations",
    "text": "Step 1: Obtain Pre-test, Post-test, and Change score standard deviations\nIn order to calculate the pre/post correlation (\\(r\\)), we need an estimate of the standard deviation (SD) of pre-test scores (\\(S_{pre}\\)), the SD of post-test scores (\\(S_{post}\\)), and the SD of change scores (\\(S_{change}\\), where \\(x_{change}=x_{post}-x_{pre}\\)). If the post-test SD is unavailable, but the pre-test SD is available, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis,\n\\[\n\\bar{S}_{ratio}=\\frac{1}{k}\\sum_{i=1}^k \\frac{S_{post,i}}{S_{pre,i}}\n\\] Then we can make an approximation of the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nS_{post}\\approx \\bar{S}_{ratio}\\times S_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal.\n\n# Define standard deviations\nS_pre &lt;- 9\nS_post &lt;- 11\nS_change &lt;- 8"
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nThe correlation between pre-test and post-test scores (\\(r\\)) can be calculated by re-arranging the equation for change score SD:\n\\[\nS_{change} = \\sqrt{S^2_{pre} + S^2_{post} - 2rS_{pre}S_{post}}\n\\] Then we can solve for \\(r\\),\n\\[\nr = \\frac{S^2_{pre} + S^2_{post} - S^2_{change}}{2S_{pre}S_{post}}\n\\] Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.697\""
  },
  {
    "objectID": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. It can be seen that the correlation calculated from the formulas above is perfectly precise.\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nS_pre &lt;- 9\nS_post &lt;- 11\nr_true &lt;- .70\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=200,\n               mu=c(0,0),\n               Sigma = data.frame(x=c(S_pre^2,r_true*S_pre*S_post),\n                                  y=c(r_true*S_pre*S_post,S_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations\nS_pre &lt;- sd(x_pre)\nS_post &lt;- sd(x_post)\nS_change &lt;- sd(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "blog-posts/blog-post-5.html#some-useful-definitions",
    "href": "blog-posts/blog-post-5.html#some-useful-definitions",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Some useful definitions",
    "text": "Some useful definitions\nLet us first note that when I refer to a measurand I simply mean the (psychological) attribute that we intend to measure.\nA realist defines the measurand outside of the measurement procedure, that is, the measurand is some test-independent attribute that is estimated by scores produced by the test.\nAn operationalist would instead suppose that the measurand is defined by the test itself and can not be defined outside of the test (i.e., the measurand is test-dependent). Note that a strict operationalist in the form that I discuss in this blog post is a sort of extreme operationalist. In the measurement error modeling section, I also describe a laxed operationalist, but it is just a useful descriptor for contrasting two types of operationalist measurement models.\nA respectful operationalist is an operationalist that defines the measurand in terms of the test, but the test is validated for extra-operational meanings and usability (i.e., meanings and usability beyond the test, Vessonen 2021). So the respectful operationalist is kind of like “sure, yeah we can define the measurand in terms of the test, but the test needs to incorporate common connotations of the target concept and produce usable scores”."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#what-might-validity-look-like-in-each-of-these-frameworks",
    "href": "blog-posts/blog-post-5.html#what-might-validity-look-like-in-each-of-these-frameworks",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "What might validity look like in each of these frameworks",
    "text": "What might validity look like in each of these frameworks\nIn realist framework, validity may fall under the principles set by Borsboom, Mellenbergh, and Van Heerden (2004), where the psychological attribute (or construct) has to both 1) exist and 2) cause variation in the measurement procedure. This view of validity is centered around construct validity.\nA (strict) operationalist need not validate a measure, even if the measure is nonsensical, the measurand is still defined by the test.\nA respectful operationalist can validate a measure using a similar approach to Michael T. Kane’s argument-based approach to validity in which he guides researchers to specify and justify the intended use of the measure. Specifically, Vessonen (2021) proposes that a test is valid if the test incorporates common extra-operational connotations of the target concept. This will be more closely related to test-related types of validity such as content and criterion validity."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#operationalist-and-realist-measurement-error-models",
    "href": "blog-posts/blog-post-5.html#operationalist-and-realist-measurement-error-models",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Operationalist and Realist Measurement Error Models",
    "text": "Operationalist and Realist Measurement Error Models\nOkay let’s start at the foundation: we observe a test score \\(X_p\\) for a person, \\(p\\).\nA strict operationalist may simply define a person’s true score \\(T_p\\) as the observed score such that, \\(T_p = X_p\\). Trivially, the observed score can be modeled as the true score,\n\\[\nX_p = T_p\n\\tag{1}\\]\nThere is something attractive and so practical about this model, however there are immediate problems of usability. What if we test that person again and they obtain a different score? What if the scoring is done by a rater and a different rater assigns a different score to that person? To answer this, let us use an example where a person’s true score is defined as the observed score \\(X_{par}\\), for person \\(p\\), test administration \\(a\\), and rater \\(r\\) such that,\n\\[\nT_p = X_{par}\n\\]\nHowever the problem now is that if the observed score varies across test-administrations and/or raters then the true score will not be generalizable across test-administrations or raters,\n\\[\nT_p = X_{par} \\neq X_{pa(r+1)} \\neq X_{p(a+1)r}\n\\]\nThis will force us to define a true score for not only each person, but also each test administration and rater such that,\n\\[\nT_{par} = X_{par}.\n\\]\nHowever differences in observed scores between test-administration and raters may not be of scientific interest if we only really care about person-level differences. This specific operationalist approach is simply unusable in practice. Although, it is interesting to note, that the reliability of observed scores is necessarily perfect and can be shown by the reliability index, \\(\\rho_{XT}=1\\) (i.e., the correlation between true and observed scores).\nA more laxed operationalist definition of true scores would be to define it based on the expectation of observed scores conditional on a person. Therefore a definition of a true score for a given person can be expressed by,\n\\[\nT_p = \\mathbb{E}[X_{par}\\mid p]\n\\]\nConditional expectation (\\(\\mathbb{E}[\\cdot]\\)) is the average observed score over all possible observed scores for a given person. This also provides the beautiful property that \\(X_{p}\\) for a given person, is calibrated to \\(T_p\\), since,\n\\[\nT_p -  \\mathbb{E}[X_p\\mid p] = T_p - T_p =  0.\n\\]\nSo now, even if observed scores vary across raters and/or test administrations, the true score will remain constant. The question at this point is how can we model a single instance of an observed score. Here, we will need to introduce measurement errors. Measurement errors can be defined as differences between observed scores and true scores. Remember that in our strict operationalist model (Equation 1) we did not have any measurement errors since the true score was equivalent to the observed score (i.e., if \\(T_p = X_p\\), then \\(T_p - X_p = 0\\)). Now that the true score is the conditional expectation of the observed score, a single instance of an observed score can differ from the true score. We can thus add a new term to the model described in Equation 1,\n\\[\nX_{p} = T_p + E_{p},\n\\tag{2}\\]\nwhere \\(E_{p}\\) indicates the error in the observed score for a given person and measurement. Since we started with the definition of true scores being \\(T_p = \\mathbb{E}[X \\mid  p]\\), an extremely useful consequence of this is that the conditional expectation of measurement errors within a person is zero. We can demonstrate why this is the case with a short derivation (first re-arranging Equation 2 so that \\(E_{p}\\) is on the left hand side),\n\\[\\begin{align}\n\\mathbb{E}[E_{p}\\mid p] &= \\mathbb{E}[T_p -  X_p\\mid p] \\\\[.3em]\n&= \\mathbb{E}[T_p\\mid p] -  \\mathbb{E}[X_p\\mid p] \\\\[.3em]\n&= T_p -  T_p \\\\[.3em]\n&= 0 \\\\\n\\end{align}\\]\nErrors that balance out over all possible observed scores for a given person is what distinguishes the classical test theory model from other measurement models (Kroc and Zumbo 2020). It is important to note that the algebraic formulation of \\(X=T+E\\) is not what defines the classical test theory model, in fact, many measurement error models come in an analogous form. It is the conditional expectations between the components of the model which distinguish classical errors from other measurement error models (e.g., Berkson errors, see Kroc and Zumbo 2020).\nA realist model may suppose that the true score is a construct score that is an objective value that is defined outside of the measurement operations (the actual value of the construct/attribute being measured; Borsboom and Mellenbergh (2002)). Therefore we can not define true scores in terms of observed scores without knowing how observed scores are calibrated true scores. The realist model for an observed score is superficially identical to Equation 2,\n\\[\nX_{p} = T_p + E_{p},\n\\tag{3}\\]\nwith the major difference being that \\(\\mathbb{E}[X|p] \\neq T_p\\). Note that Equation 3 is not the classical test theory model because it does not, by default, meet the assumptions of classical test theory that the laxed operationalist model does (Equation 2). The big difference between this realist model the laxed operationalist model is that the conditional expectation of errors is no longer zero in the realist approach, and thus systematic errors (i.e., biased estimates of true scores) can exist. In order to recover proper calibration of the observed scores to true scores, we would have to just assume that \\(\\mathbb{E}[E_p\\mid p]=0\\) (see flaws below).\nWhen it comes to psychological constructs the realist approach has two major flaws to me.\n\nNecessitates the existence of a construct score that is independent of the measure. Without sufficient evidence of a construct existing, we are adding additional complexity to our theory. Therefore it adds increased complexity into our theory.\nEven if the construct is real and has a true construct score, we have no idea how the observed scores are calibrated to those true scores. The operationalist model produces a calibrated measure by definition whereas the realist model needs to add an additional assumption (i.e., conditioned on a given person, the expectation of measurement errors is zero) in order for the observed scores to be calibrated.\n\nThe operationalist model may have the biggest flaw so far:\n\nSince the measurand is defined by the measure itself, you can not really draw any meaningful inferences about anything outside of the measure.\n\nThere is a third option though!"
  },
  {
    "objectID": "blog-posts/blog-post-5.html#respectful-operationalist-measurement-error-model",
    "href": "blog-posts/blog-post-5.html#respectful-operationalist-measurement-error-model",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Respectful Operationalist Measurement Error Model",
    "text": "Respectful Operationalist Measurement Error Model\nAs put by the originator of Respectful Operationalism Elina Vessonen (2021) states,\n\n[Respectful Operationalism] is the view that we may define a target concept in terms of a test, as long as that test is validated to incorporate common connotations of the target concept and the usability of the measure\n\nTherefore we need to identify which observed scores are produced by tests that have been validated for extra-operational connotations for the concept of interest. We generally have inter-subjective agreement about the meanings of concepts like intelligence, anxiety, and depression mean outside of the measurement instrument. We can imagine that items contain content that are more or less related to those inter-subjective meanings. The extent to which the item content is relevant to the concept of interest reflects the item’s content validity. The true score can be defined as the conditional expectation of observed scores obtained from tests containing items that are sufficiently valid (this could be done from inter-rater assessment of the conceptual relatedness of items and concepts) can define the true scores. that contain sufficient validity. Let’s define an observed score produced by a sufficiently valid test as \\(X_p^\\star\\). As an example, a measure of depression that produces observed scores from responses to the question, “what is your age?”, would not encompass common connotations of depression and therefore those observed scores would not in the set of valid observed scores. This new model can be defined as,\n\\[\nX^\\star_p = T_p + E_p\n\\]\nwhere\n\\[\nT_p = \\mathbb{E}[X^\\star_p\\mid p]\n\\]\nIn this way, inferences about concepts can be made from observed scores since they hold inherent relevance by virtue of prior validation of the test’s extra-operational meaning."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#my-current-stance",
    "href": "blog-posts/blog-post-5.html#my-current-stance",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "My current stance",
    "text": "My current stance\nRespectful operationalism appears to offer the practical advantages of operationalism such as calibrated measurement outcomes, without the limitation of being unable to draw meaningful inferences beyond the test. To consider myself a realist for a particular attribute/construct, then I would have to be sufficiently convinced of the tenants of construct validity described by Borsboom, Mellenbergh, and Van Heerden (2004): (1) the attribute exists (2) the construct causes variation in outcomes of the measurement procedure. Currently, I am not sufficiently convinced this is the case this is the case for most (if not all) psychological constructs; therefore, I am not comfortable making the commitments necessary to take on a realist position. Therefore, to me, a know-nothin grad student, Respectful Operationalism seems to best align with my current worldview.\nFurther reading, see these papers by Elina Vessonen (2019, 2021)."
  },
  {
    "objectID": "Software.html",
    "href": "Software.html",
    "title": "Software",
    "section": "",
    "text": "ThemePark\nGenerating popular culture styled ggplot themes. Featured on rweekly.org and flowingdata.com\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOSC\nAn R Package for generating Probability of Outcome Superiority Curves (POSCs).\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenSynthesis\nWebsite cataloging publicly available meta-analytic databases.\n Github Repository  Webpage\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Simulator\nA Shiny App for Visualizing Statistical Artifacts.\n Shiny App\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Corrections for Effect Sizes\nA webpage documenting equations and code for effect size artifact corrections. MatthewBJane.com/ArtifactCorrections\n Webpage\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-Analysis Data & Code\nRepository of data and code for all meta-analytic projects.\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary Study Data & Code\nRepository of data and code for all primary study projects.\n Github Repository"
  },
  {
    "objectID": "ArtifactCorrections.html",
    "href": "ArtifactCorrections.html",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "",
    "text": "Effect sizes are indices that help researchers, stakeholders, and policymakers understand the relationship between variables and draw meaningful conclusions from data. However, the usefulness of an effect size is only as good as its estimation. To ensure the accuracy of effect size estimates, it is important to mitigate the attenuation induced by various statistical artifacts, such as measurement error and range restriction. This page provides documentation on these artifacts and provides corrections that can be applied to attenuated effect sizes to obtain unbiased estimates of the true population effect size. Equations and R code are provided for each correction. For additional information on these corrections and their application in meta-analysis, consult the book by Hunter and Schmidt (1990) and the paper by Wiernik and Dahlke (2020) . The {psychmeta} package (Dahlke and Wiernik 2019) contains many of these corrections with convenient implementation in R."
  },
  {
    "objectID": "ArtifactCorrections.html#small-samples-1",
    "href": "ArtifactCorrections.html#small-samples-1",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Small Samples",
    "text": "Small Samples\nBelow are the correction factors that may be applied to the correlation coefficient (r) and the standardized mean difference (d). Note that the correction for r is not conventionally used as it tends to be a very small adjustment.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = r \\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = se_{r}\\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right)}\\]\n\n# Parameters needed \nr =  0.50 # observed correlation between x and y \nn =  20 # sample size\nSEr = (1 - r^2) / sqrt(n - 1)  # standard error of observed correlation between x and y \n\n#Point Estimate \nrho = r * (1 + (1 - r^2) / (2 * (n - 4))) \n\n#Standard Error\nSErho = SEr *  (1 + (1 - r^2) / (2 * (n - 4)))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.51, SE = 0.18\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate \\[\\displaystyle{ \\hat{\\delta} = d \\left( 1-\\frac{3}{4n-9}\\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = se_{d}\\left( 1-\\frac{3}{4n-9}\\right) }\\]\n\n# Parameters needed \nd = 0.50 # observed standardized mean difference \nn = 20 # total sample size\nSEd = 0.10 # standard error of observed standardized mean difference \n\n# Point Estimate\ndelta = d * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Standard Error\nSEdelta = SEd * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.47, SE = 0.09\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate measurement error in continuous variables",
    "text": "Univariate measurement error in continuous variables\nMeasurement error will likely exist in both variables under invistigation (i.e., \\(x\\) and \\(y\\)), however applying a correction may depend on the research question. For example, if you would like to know how related two psychological constructs are, correcting both variables for measurement error is appropriate, however it may not be appropriate if you would like to use one variable to predict the other (e.g., exam scores to predict college grades). Since observed scores are all that is available to us, therefore correcting the predictor variable for measurement error will not capture its real world predictive utility.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'}}} }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'}}} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x within sample\n\n# Point Estimate\nrho = r / sqrt(rxx)\n\n# Standard Error\nSErho = SEr / sqrt(rxx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{d}{\\sqrt{r_{yy'}}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_{d}}{\\sqrt{r_{yy'}}} }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y within sample\n\n# Point Estimate\ndelta = d / sqrt(ryy)\n\n# Standard Error\nSEdelta = SEd / sqrt(ryy)\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.56, SE = 0.56\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate measurement error for continuous variables",
    "text": "Bivariate measurement error for continuous variables\nUnreliability of \\(x\\) and \\(y\\) will bias a pearson correlation coefficient by adding random, uncorrelated noise into the bivariate relationship. If the goal is to understand the relationship between the true, uncontaminated scores between two things, then measurement error on both variables should be corrected for.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\n\n# Point Estimate\nrho = r / sqrt(rxx * ryy)\n\n# Standard Error\nSErho = SEr / sqrt(rxx * ryy)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.67, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#misclassification",
    "href": "ArtifactCorrections.html#misclassification",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Misclassification",
    "text": "Misclassification\nMisclassification encompasses measurement errors for categorical variables. For example, our ability to identify individuals with major depressive disorder is only as accurate as our measurement of depression, therefore if the measure for depression contains measurement error then so will the assignment of individuals in the major depressive group and the control group (i.e., some people with major depressive disorder will be mis-labeled as a control and vice versa). To correct for group misclassification, you must first have an estimate of the phi coefficient (\\(\\phi_{gG}\\)) from the 2x2 contingency table comparing actual vs observed group membership. Phi can also be approximated directly from the misclassification rate (\\(p_{mis}\\)), however this may cause undercorrections when misclassification rates differ between groups.\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi\\) coefficient from the contingency table between actual group membership (G) and observed group membership (g):\n\\[\\displaystyle{ \\phi_{gG} = \\sqrt{\\frac{\\chi_{gG}^{2}}{n}} }\\] or \\[\\displaystyle{\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} }\\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 3. Dissatenuate correlation for misclassification:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\phi_{gG}} }\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nn = 50 # sample size\nchi2 = 32 # chi squared statistic for actual vs observed group contingency table\n# p_mis = .20 # misclassification rate (only if alternative step 1 is used)\n\n# Point Estimate\nphi = sqrt(chi2 / n)# step 1\n# phi = 1 - 2 * p_mis  # step 1 (assume equal misclassification)\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 2\nrho = r / phi # step 3\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.31, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate dichotomization of naturally continuous variables",
    "text": "Univariate dichotomization of naturally continuous variables\nIn some cases, researchers may categorize naturally continuous variables into two groups in order to facilitate interpretation or to perform specific statistical analyses. For example, congenital amusia, also known as tone deafness, is often diagnosed by scoring below a predetermined cutoff in a pitch or melodic discrimination task. This cutoff is arbitrary (although potentially useful) since pitch discrimination ability typically follows a normal distribution. Before someone can adjust the effect sizes for this artificial dichotomization, it is necessary to determine the proportions of participants above or below the cutoff (\\(p_x\\)) as well as the cutoff value (\\(c_{x}\\)), which can be estimated using the quantile function of a standard normal distribution at \\(p_{x}\\).\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\n\n\n# Point Estimate\ncx = qnorm(px) # Find cut point \nrho = r * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Standard Error\nSErho = SEr * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.63, SE = 0.13\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate - Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)):\n\\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Disattenuate correlation for artificial dichotomization:\n\\[\\displaystyle{ \\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} } }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\npx = 0.50 # proportion of individuals in upper or lower group of X\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\ncx = qnorm(px)# Find cut point\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r * sqrt(px * (1 - px)) / dnorm(cx)# step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 1.39, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate dichotomization of naturally continuous variables",
    "text": "Bivariate dichotomization of naturally continuous variables\nI somewhat rarer circumstances, researchers may dichotomize both x and y variables. This will attenuate correlation coefficients more than usual. If a tetrachoric correlation is available, this is more analogous to a pearson correlation coefficient than the correction below.\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x p_y (1-p_x) (1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x p_y(1-p_x)(1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\npy = 0.50 # proportion of individuals in upper or lower group of Y\n\n# Point Estimate\ncx = qnorm(px)# Find cut point on Y variable \ncy = qnorm(py)# Find cut point on Y variable\nrho = r * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Standard Error\nSErho = SEr * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.79, SE = 0.16\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction",
    "text": "Univariate Direct Range Restriction\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Standard Error\nSErho = SEr / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Correct r for direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed correlation\nSEd = 0.50 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1)) # step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )"
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction",
    "text": "Univariate Indirect Range Restriction\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\nStandard Error \\[\\displaystyle{se_{\\rho} = \\frac{se_{r}}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Standard Error\nSErho = SEr / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} \\]\n-Step 2. Correct r for direct range restriction:\n\\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} \\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 1\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))  # step 2\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.59, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction",
    "text": "Bivariate Direct Range Restriction\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define gamma:\n\\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n-Step 2. Correct r for bivariate direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = -\\Gamma + \\mathrm{sign} (r) \\sqrt{\\Gamma^2 + 1} }\\]\nStandard Error\n\\[\\displaystyle{se_{\\rho} = se_{r} \\left( \\frac{\\hat{\\rho}}{r} \\right) }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy  # step 1\nrho = -Gamma + sign(r) * sqrt(Gamma^2 + 1)# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.61, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction",
    "text": "Bivariate Indirect Range Restriction\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }\\]\nStandard Error\n-Step 1. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[\\displaystyle{ \\beta_1 = u_y r - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{\\sqrt{\\left|1 - u_y^2 \\right|^3}} }\\]\n-Step 2. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\displaystyle{ \\beta_2 = u_x r - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{\\sqrt{\\left|1 - u_x^2 \\right|^3}} }\\]\n-Step 3. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\displaystyle{ \\beta_3 = u_x u_y }\\] -Step 4. Calculate standard error for \\(u_x\\)\n\\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 5. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 6. Calculate standard error for \\(r\\)\n\\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\] -Step 7. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{b_1^2 se_{u_x}^2 + b_2^2 se_{u_y}^2 + b_3^2 se_{r}^2 } }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error for observed correlation between x and y\nrxx = 0.70  # reliability of x within study sample\nryy = 0.80 # reliability of y within study sample\nux  = 0.85 # observed u-ratio of x \nuy  = 0.80 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\nna = 200\nn = 100\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nb1  = r * uy - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / sqrt(abs(1 - uy^2)^3)# First order partial derivitive of ux\nb2  = r * ux - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / sqrt(abs(1 - ux^2)^3)# First order partial derivitive of uy\nb3  = ux*uy# First order partial derivitive of r\n\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.66, SE = 0.08\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction and Measurement Error",
    "text": "Univariate Direct Range Restriction and Measurement Error\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-r_{xx'})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } }\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85# ratio of observed standard deviation of y to reference standard deviation of x (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / ( ux * sqrt(1 - ux^2 * (1 - rxx)) * sqrt( r^2 * (1/ux^2 - 1) + ryy) )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.71, SE = 0.14\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership:\n\\[ \\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) }+d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-\\phi_{gG})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } \\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } \\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10# standard error of observed standardized mean difference\nn = 100 # sample size\nryy = 0.80 # reliability of y\nuy = 0.85 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \n#p_mis = 0.20 # proportion of individuals misclassified (only needed if equal misclassification rate is assumed, and alternative phi calculation)\n\n# Point Estimate\nphi =  sqrt(chi2 / n)  # step 1\n# phi = 1 - 2 * p_mis  # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 2\nrho = r / ( uy * sqrt(1 - uy^2 * (1 - phi)) * sqrt( r^2 * (1/uy^2 - 1) + ryy) )  # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.8, SE = 0.18\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction and Measurement Error",
    "text": "Univariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}}\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.70 # reliability of x\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.75, SE = 0.15\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership: \\[\\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g )} + d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \nn = 100 # sample size\n\n# Point Estimate\nphi =  sqrt(chi2 / n) # step 1\n# phi = 1 - 2 * p_mis # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2)# step 2\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  ) # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) ) # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.85, SE = 0.19\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction and Measurement Error",
    "text": "Bivariate Direct Range Restriction and Measurement Error\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate - Step 1. Define Gamma: \\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n\nStep 2. Correct r for univariate direct range restriction: \\[\\displaystyle{\\hat{\\rho} = \\frac{-\\Gamma u_x u_y + \\mathrm{sign}(r)\\sqrt{\\Gamma^2 + 1} }{\\sqrt{1-u_x^2(1-r_{xx'})}\\sqrt{1-u_y^2(1-r_{yy'})}} }\\]\n\nStandard Error \\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy # step 1\nrho = (-Gamma + sign(r) * sqrt(Gamma^2 + 1)) / (sqrt(1 - ux^2 * (1 - rxx)) * sqrt(1 - uy^2 * (1 - ryy)))# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.74, SE = 0.15\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction and Measurement Error",
    "text": "Bivariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\hat{\\rho} = \\frac{r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }{\\sqrt{1 - u_x^2 (1-r_{xx'})} \\sqrt{1 - u_y^2 (1 - r_{yy'})}}\\]\nStandard Error\n\nStep 1. Calculate the measurement quality index for X in the restricted sample: \\[ q_x = \\sqrt{r_{xx'}}\\]\nStep 2. Calculate the measurement quality index for Y in the restricted sample: \\[ q_y = \\sqrt{r_{yy'}}\\]\nStep 3. Estimated the measurement quality index for X in the unrestricted population:\n\\[q_X = \\sqrt{1 - u_x^2 (1 - r_{xx'})} \\]\nStep 4. Estimate the measurement quality index for Y in the unrestricted population: \\[ q_Y = \\sqrt{1 - u_y^2 (1 - r_{yy'})} \\]\nStep 5. Calculate first order partial derivative of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{xx'}\\) \\[ \\beta_1 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left(1 - u_x^2 \\right)\\left(1 - u_y^2 \\right)} }{q_X^2 q_Y} \\]\nStep 6. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{yy'}\\) \\[ \\beta_2 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left|1 - u_x^2 \\right|\\left|1 - u_y^2 \\right|} }{q_Y^2 q_X} \\]\nStep 7. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[ \\beta_3 = \\frac{u_y r}{q_X q_Y} - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_y^2 \\right|^3}}\\]\nStep 8. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\beta_4 = \\frac{u_x r}{q_X q_Y} - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_x^2 \\right|^3}} \\]\nStep 9. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\beta_5 = \\frac{u_x u_y}{q_X q_Y} \\]\nStep 10. Calculate standard error for unrestricted measure quality index (\\(q_X\\)) \\[se_{q_X} = \\sqrt{\\frac{1}{2} u_x^4 \\left[ \\frac{(1-q_x^2)^2}{1-u_x^2 (1-q_x^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_x^2 q_x^2(1 - q_x^2)^2}{\\left[1-u_x^2 (1-q_x^2)\\right] (n-1)} } \\]\nStep 11. Calculate standard error for unrestricted measure quality index (\\(q_Y\\)) \\[se_{q_Y} = \\sqrt{\\frac{1}{2} u_y^4 \\left[ \\frac{(1-q_y^2)^2}{1-u_y^2 (1-q_y^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_y^2 q_y^2(1 - q_y^2)^2}{\\left[1-u_y^2 (1-q_y^2)\\right] (n-1)} } \\]\nStep 12. Calculate standard error for \\(u_x\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 13. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 14. Calculate standard error for \\(r\\) \\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\]\nStep 15. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{\\beta_1^2 se_{q_X}^2 + \\beta_2^2 se_{q_Y}^2 + \\beta_3^2 se_{u_x}^2 + \\beta_4^2 se_{u_y}^2 + \\beta_5^2 se_{r}^2 } }\\]\n\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # observed correlation between x and y\nrxx = 0.80 # reliability of x within study sample\nryy = 0.70 # reliability of y within study sample\nn = 200 # sample size\nna = 100# sample size of reference sample\nux  = 0.80# observed u-ratio of x \nuy  = 0.85 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nqx = sqrt(rxx)\nqy = sqrt(ryy)\nqX = sqrt(ux^2 * (rxx - 1))\n\nWarning in sqrt(ux^2 * (rxx - 1)): NaNs produced\n\nqY = sqrt(uy^2 * (ryy - 1))\n\nWarning in sqrt(uy^2 * (ryy - 1)): NaNs produced\n\n# First order partial derivitive of qX\nb1  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qX^2 * qY) \n# First order partial derivitive of qY\nb2  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qY^2 * qX) \n# First order partial derivitive of ux\nb3  = (r * uy) / (qX * qY) - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / (qX * qY * sqrt(abs(1 - uy^2)^3))  \n# First order partial derivitive of uy\nb4  = (r * ux) / (qX * qY) - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / (qX * qY * sqrt(abs(1 - ux^2)^3))  \n# First order partial derivitive of r\nb5  = (ux*uy) / (qX * qY)\n\n# Standard error of qX\nSEqX = sqrt( .5 * ux^4 * ( (1-qx^2)^2 / (1-ux^2 * (1-qx^2)) ) * (1/(n-1) + 1/(na-1)) + ( (ux^2 * qx^2 * (1-qx^2)^2) ) / ((1 - ux^2 * (1-qx^2)) * (n-1)) )\n\n# Standard error of qY\nSEqY = sqrt( .5 * uy^4 * ( (1-qy^2)^2 / (1-uy^2 * (1-qy^2)) ) * (1/(n-1) + 1/(na-1)) + ( (uy^2 * qy^2 * (1-qy^2)^2) ) / ((1 - uy^2 * (1-qy^2)) * (n-1)) )\n\n# Standard error of ux\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of uy\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of r\nSEr =  (1 - r^2) / sqrt(n - 1)\n\n# Taylor series approximation\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(delta,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.85, SE = NaN\""
  }
]