[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Email Twitter Github Scholar\n\n\nHello there! I’m Matthew B. Jané - I’m a PhD student in Quantitative Psychology at the University of Connecticut. My research involves developing statistical methods and software to aide in meta-analysis and evidence synthesis. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. My advisor is Dr. Blair T. Johnson and I am a member of the Systematic Health Action Research Program (SHARP). I am also on the editorial board for Psychological Bulletin as a methodological reviewer."
  },
  {
    "objectID": "blog-posts/blog-post-2.html",
    "href": "blog-posts/blog-post-2.html",
    "title": "Approximating Standard Deviation from Inter-Quantile Range",
    "section": "",
    "text": "Studies may report inter-quantile ranges in order to communicate the spread of the data. However, meta-analysts will need standard deviations (SD) to calculate effect size estimates such as standardized mean differences (i.e., Cohen’s \\(d\\)). For this post, we will focus on the inter-quartile range (IQR) although the method described here will apply to any range between a pair of quantiles. The IQR (\\(X_{(75-25)}\\)) is defined as the difference between the 75th and 25th quantile values, that is, the region where 50% of the data lies. We can visualize what this will look like with normally distributed data:\n\n\n\n\n\nNow let’s compare the range of values covered by +/- 1 SDs of a normal distribution:\n\n\n\n\n\nThe range covered by \\(\\pm 1\\) SD covers a considerably larger range of values than the IQR."
  },
  {
    "objectID": "blog-posts/blog-post-2.html#note-the-assumptions",
    "href": "blog-posts/blog-post-2.html#note-the-assumptions",
    "title": "Approximating Standard Deviation from Inter-Quantile Range",
    "section": "Note the Assumptions!",
    "text": "Note the Assumptions!\nSince this method provides an approximation of a normal distribution it is important to point out that the method may be biased under different distributions. For example, if we try the same method on a Student’s t distribution with heavy tails (3.5 degrees of freedom), \\(S_{(75-25)}\\) would be equal to 1.50187 which is slightly larger than 1.34898 that we computed from the normal distribution. If we were to assume a normal distribution when it was truly a student’s t, the method would over-estimate the standard deviation by ~11%.\n\n\n\n\n\n\nApplying it to simulated data\nLets see how it performs in simulated sample of normally distributed data… Close enough!!\n\n# Set seed\nset.seed(343)\n\n# Simulate normal data (Mean = 10, SD = 5)\nX = rnorm(100,10,5)\n\n# Define IQR\nq25 &lt;- as.numeric(quantile(X,.25))\nq75 &lt;- as.numeric(quantile(X,.75))\nX_75_25 &lt;- q75 - q25\n\n# Compute IQR in SD units\nS_75_25 &lt;- qnorm(.75) - qnorm(.25)\n\n# Estimate standard deviation\nS_X &lt;-  X_75_25 / S_75_25\n\n# Print results\nprint(S_X)\n\n[1] 4.706901"
  },
  {
    "objectID": "blog-posts/blog-post-3.html",
    "href": "blog-posts/blog-post-3.html",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "",
    "text": "In order to calculate the pre/post correlation (\\(r\\)), we need the standard deviation (SD) of pre-test scores (\\(SD_{pre}\\)), the SD of post-test scores (\\(SD_{post}\\)), the mean change (\\(M_{change}\\)), the paired t-statistic (\\(t\\)), the sample size (\\(N\\)). In this blog post, we are assuming that the change score standard deviation (\\(SD_{change}\\)) is unavailable to us. If the pre-test SD is available, but the post-test SD is unavailable, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis (\\(\\overline{SD}_{ratio}\\); see the blog post on 9/8/2023), then we can approximate the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nSD_{post}\\approx \\bar{SD}_{ratio}\\times SD_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal. If the study reports an F-statistic from a one-way repeated measures ANOVA, the F-statistic is equal to the square of the t-statistic,\n\\[\nt = \\sqrt{F}\n\\]\nEnsure that you apply the correct sign (negative or positive) to the t-statistic, since the F-statistic is always positive.\n\n# Define standard deviations\nM_change &lt;- 3\nSD_post &lt;- 11\nSD_pre &lt;- 9\nt &lt;- 2.50\nN &lt;- 50"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "href": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "",
    "text": "In order to calculate the pre/post correlation (\\(r\\)), we need the standard deviation (SD) of pre-test scores (\\(SD_{pre}\\)), the SD of post-test scores (\\(SD_{post}\\)), the mean change (\\(M_{change}\\)), the paired t-statistic (\\(t\\)), the sample size (\\(N\\)). In this blog post, we are assuming that the change score standard deviation (\\(SD_{change}\\)) is unavailable to us. If the pre-test SD is available, but the post-test SD is unavailable, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis (\\(\\overline{SD}_{ratio}\\); see the blog post on 9/8/2023), then we can approximate the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nSD_{post}\\approx \\bar{SD}_{ratio}\\times SD_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal. If the study reports an F-statistic from a one-way repeated measures ANOVA, the F-statistic is equal to the square of the t-statistic,\n\\[\nt = \\sqrt{F}\n\\]\nEnsure that you apply the correct sign (negative or positive) to the t-statistic, since the F-statistic is always positive.\n\n# Define standard deviations\nM_change &lt;- 3\nSD_post &lt;- 11\nSD_pre &lt;- 9\nt &lt;- 2.50\nN &lt;- 50"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nLets start by figuring out how to find the change score SD. The paired t-statistic is defined as the mean change score divided by the standard error of change scores, such that, \\[\nt = \\frac{M_{change}}{SE_{change}}\n\\] Since we need the change score SD, we can use the definition of the standard error of the mean to put the t-statistic in terms of \\(SD_{change}\\): \\[\nSE_{change}=\\frac{SD_{change}}{\\sqrt{N}}\n\\] and therefore \\(t\\) can be expressed as,\n\\[\nt=\\frac{M_{change}}{\\left(\\frac{SD_{change}}{\\sqrt{N}}\\right)}\n\\]\nthen we just need to solve for \\(SD_{change}\\):\n\\[\nSD_{change}=\\frac{M_{change}\\times\\sqrt{N}}{t}\n\\]\nOkay so now let us recall the definition of change score SDs from the blog post on 9/8/2023. In that blog we discussed how to obtain the pre/post correlation from the change score SD, now that we have converted \\(t\\) to \\(SD_{change}\\), we can solve for the correlation in a similar way. First things first, the change score SD can be defined as, \\[\nSD_{change} = \\sqrt{SD^2_{pre} + SD^2_{post} - 2\\times r\\times SD_{pre}SD_{post}}\n\\]\nWe can re-arrange this to isolate the pre/post correlation (\\(r\\)),\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - SD^2_{change}}{2 \\times SD_{pre}\\times SD_{post}}\n\\]\nIn our case, the study did not report the change score SD, therefore we can replace it with our derived \\(SD_{change}\\) from a paired t-test:\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - \\left(\\frac{M_{change}\\times\\sqrt{N}}{t}\\right) ^2}{2 \\times SD_{pre}\\times SD_{post}}\n\\] Lets neaten this formulation up a tad:\n\\[\nr = \\frac{t^2\\left(SD^2_{pre} + SD^2_{post}\\right) - N\\times M^2_{change} }{2 \\times t^2 \\times SD_{pre}\\times SD_{post} }\n\\]\nIsn’t that just a beautiful thing?? So there you have it! the full equation for the pre/post correlation from a paired t-test! Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.657\""
  },
  {
    "objectID": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. The calculated correlation is exactly correct!\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nSD_pre &lt;- 9\nSD_post &lt;- 11\nr_true &lt;- .70\nM_pre &lt;- 20\nM_post &lt;- 25\nN &lt;- 100\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=N,\n               mu=c(M_pre,M_post),\n               Sigma = data.frame(x=c(SD_pre^2,r_true*SD_pre*SD_post),\n                                  y=c(r_true*SD_pre*SD_post,SD_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations, t-stats, and mean change\nSD_pre &lt;- sd(x_pre)\nSD_post &lt;- sd(x_post)\nt &lt;- mean(x_change) / (sd(x_change)/sqrt(N))\nM_change &lt;- mean(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "blog-posts/blog-post-1.html",
    "href": "blog-posts/blog-post-1.html",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "",
    "text": "In order to calculate the pre/post correlation (\\(r\\)), we need an estimate of the standard deviation (SD) of pre-test scores (\\(S_{pre}\\)), the SD of post-test scores (\\(S_{post}\\)), and the SD of change scores (\\(S_{change}\\), where \\(x_{change}=x_{post}-x_{pre}\\)). If the post-test SD is unavailable, but the pre-test SD is available, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis,\n\\[\n\\bar{S}_{ratio}=\\frac{1}{k}\\sum_{i=1}^k \\frac{S_{post,i}}{S_{pre,i}}\n\\] Then we can make an approximation of the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nS_{post}\\approx \\bar{S}_{ratio}\\times S_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal.\n\n# Define standard deviations\nS_pre &lt;- 9\nS_post &lt;- 11\nS_change &lt;- 8"
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "href": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "",
    "text": "In order to calculate the pre/post correlation (\\(r\\)), we need an estimate of the standard deviation (SD) of pre-test scores (\\(S_{pre}\\)), the SD of post-test scores (\\(S_{post}\\)), and the SD of change scores (\\(S_{change}\\), where \\(x_{change}=x_{post}-x_{pre}\\)). If the post-test SD is unavailable, but the pre-test SD is available, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis,\n\\[\n\\bar{S}_{ratio}=\\frac{1}{k}\\sum_{i=1}^k \\frac{S_{post,i}}{S_{pre,i}}\n\\] Then we can make an approximation of the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nS_{post}\\approx \\bar{S}_{ratio}\\times S_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal.\n\n# Define standard deviations\nS_pre &lt;- 9\nS_post &lt;- 11\nS_change &lt;- 8"
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nThe correlation between pre-test and post-test scores (\\(r\\)) can be calculated by re-arranging the equation for change score SD:\n\\[\nS_{change} = \\sqrt{S^2_{pre} + S^2_{post} - 2rS_{pre}S_{post}}\n\\] Then we can solve for \\(r\\),\n\\[\nr = \\frac{S^2_{pre} + S^2_{post} - S^2_{change}}{2S_{pre}S_{post}}\n\\] Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.697\""
  },
  {
    "objectID": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. It can be seen that the correlation calculated from the formulas above is perfectly precise.\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nS_pre &lt;- 9\nS_post &lt;- 11\nr_true &lt;- .70\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=200,\n               mu=c(0,0),\n               Sigma = data.frame(x=c(S_pre^2,r_true*S_pre*S_post),\n                                  y=c(r_true*S_pre*S_post,S_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations\nS_pre &lt;- sd(x_pre)\nS_post &lt;- sd(x_post)\nS_change &lt;- sd(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "Software.html",
    "href": "Software.html",
    "title": "Software",
    "section": "",
    "text": "ThemePark\nGenerating popular culture styled ggplot themes. Featured on rweekly.org and flowingdata.com\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\nPOSC\nAn R Package for generating Probability of Outcome Superiority Curves (POSCs).\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\nOpenSynthesis\nWebsite cataloging publicly available meta-analytic databases.\n Github Repository  Webpage\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Simulator\nA Shiny App for Visualizing Statistical Artifacts.\n Shiny App\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Corrections for Effect Sizes\nA webpage documenting equations and code for effect size artifact corrections. MatthewBJane.com/ArtifactCorrections\n Webpage\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-Analysis Data & Code\nRepository of data and code for all meta-analytic projects.\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary Study Data & Code\nRepository of data and code for all primary study projects.\n Github Repository"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Website  Email  Twitter  Github\n\n\nCurrent PhD student interested in developing statistical methods and software for meta-analyses and primary studies. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. I am affiliated with the Systematic Health Action Research Program (SHARP) at the University of Connecticut where I am advised by Dr. Blair T. Johnson. I am also on the editorial board for Psychological Bulletin as a methodological reviewer.\n\n\n\n\n2022–Present | Ph.D. Quantitative Psychology, University of Connecticut, Storrs, Connecticut\n\nResearch involves statistical methods for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut, Storrs, Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut, Storrs, Connecticut\n\n\n\n\n\nIn preparation | Correcting Effect Sizes for Artifacts in R, by Matthew B. Jané and Blair T. Johnson. A guide for correcting bias in effect size estimates induced by artifacts. Includes applications in meta-analysis and implementation in R.\n\n\n\n\n\n R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/theme_park\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. MatthewBJane.github.io/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. MatthewBJane.com/ArtifactCorrections\n Project Data & Code Primary Study Data and Code, Repository of data and code for all primary study projects. https://github.com/MatthewBJane/primary-project-data-code.\n Project Data & Code Meta-Analysis Data and Code, Repository of data and code for all meta-analytic projects. https://github.com/MatthewBJane/meta-analysis-project-data-code.\n\n\n\n\nJané, M. B., Curley, C. Johnson, B. T. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Johnson, B. T. (2023). Temporal and Non-Temporal Sensory Discrimination and Fluid Intelligence: Artifact Correction Meta-Analysis. Manuscript in preparation.\n\nData Code PDF OSF\n\n\nJané, M. B., Johnson, B. T. (2023). Correcting for Measurement Error in Repeated Measures Standardized Mean Differences. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Martinez-Berman, L., Johnson, B. T. (2023). Cognitive Ability Moderates the Accuracy of Self-Reported Perfect Pitch. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Anxiety and Depression in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF"
  },
  {
    "objectID": "CV.html#bio",
    "href": "CV.html#bio",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Current PhD student interested in developing statistical methods and software for meta-analyses and primary studies. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. I am affiliated with the Systematic Health Action Research Program (SHARP) at the University of Connecticut where I am advised by Dr. Blair T. Johnson. I am also on the editorial board for Psychological Bulletin as a methodological reviewer."
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "2022–Present | Ph.D. Quantitative Psychology, University of Connecticut, Storrs, Connecticut\n\nResearch involves statistical methods for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut, Storrs, Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut, Storrs, Connecticut"
  },
  {
    "objectID": "CV.html#textbooks",
    "href": "CV.html#textbooks",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "In preparation | Correcting Effect Sizes for Artifacts in R, by Matthew B. Jané and Blair T. Johnson. A guide for correcting bias in effect size estimates induced by artifacts. Includes applications in meta-analysis and implementation in R."
  },
  {
    "objectID": "CV.html#software",
    "href": "CV.html#software",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/theme_park\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. MatthewBJane.github.io/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. MatthewBJane.com/ArtifactCorrections\n Project Data & Code Primary Study Data and Code, Repository of data and code for all primary study projects. https://github.com/MatthewBJane/primary-project-data-code.\n Project Data & Code Meta-Analysis Data and Code, Repository of data and code for all meta-analytic projects. https://github.com/MatthewBJane/meta-analysis-project-data-code."
  },
  {
    "objectID": "CV.html#publications",
    "href": "CV.html#publications",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Jané, M. B., Curley, C. Johnson, B. T. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Johnson, B. T. (2023). Temporal and Non-Temporal Sensory Discrimination and Fluid Intelligence: Artifact Correction Meta-Analysis. Manuscript in preparation.\n\nData Code PDF OSF\n\n\nJané, M. B., Johnson, B. T. (2023). Correcting for Measurement Error in Repeated Measures Standardized Mean Differences. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Harlow, T. J., Martinez-Berman, L., Johnson, B. T. (2023). Cognitive Ability Moderates the Accuracy of Self-Reported Perfect Pitch. Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Anxiety and Depression in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF"
  },
  {
    "objectID": "ArtifactCorrections.html",
    "href": "ArtifactCorrections.html",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "",
    "text": "Effect sizes are indices that help researchers, stakeholders, and policymakers understand the relationship between variables and draw meaningful conclusions from data. However, the usefulness of an effect size is only as good as its estimation. To ensure the accuracy of effect size estimates, it is important to mitigate the attenuation induced by various statistical artifacts, such as measurement error and range restriction. This page provides documentation on these artifacts and provides corrections that can be applied to attenuated effect sizes to obtain unbiased estimates of the true population effect size. Equations and R code are provided for each correction. For additional information on these corrections and their application in meta-analysis, consult the book by Hunter and Schmidt (1990) and the paper by Wiernik and Dahlke (2020) . The {psychmeta} package (Dahlke and Wiernik 2019) contains many of these corrections with convenient implementation in R."
  },
  {
    "objectID": "ArtifactCorrections.html#small-samples-1",
    "href": "ArtifactCorrections.html#small-samples-1",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Small Samples",
    "text": "Small Samples\nBelow are the correction factors that may be applied to the correlation coefficient (r) and the standardized mean difference (d). Note that the correction for r is not conventionally used as it tends to be a very small adjustment.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = r \\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = se_{r}\\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right)}\\]\n\n# Parameters needed \nr =  0.50 # observed correlation between x and y \nn =  20 # sample size\nSEr = (1 - r^2) / sqrt(n - 1)  # standard error of observed correlation between x and y \n\n#Point Estimate \nrho = r * (1 + (1 - r^2) / (2 * (n - 4))) \n\n#Standard Error\nSErho = SEr *  (1 + (1 - r^2) / (2 * (n - 4)))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.51, SE = 0.18\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate \\[\\displaystyle{ \\hat{\\delta} = d \\left( 1-\\frac{3}{4n-9}\\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = se_{d}\\left( 1-\\frac{3}{4n-9}\\right) }\\]\n\n# Parameters needed \nd = 0.50 # observed standardized mean difference \nn = 20 # total sample size\nSEd = 0.10 # standard error of observed standardized mean difference \n\n# Point Estimate\ndelta = d * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Standard Error\nSEdelta = SEd * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.47, SE = 0.09\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate measurement error in continuous variables",
    "text": "Univariate measurement error in continuous variables\nMeasurement error will likely exist in both variables under invistigation (i.e., \\(x\\) and \\(y\\)), however applying a correction may depend on the research question. For example, if you would like to know how related two psychological constructs are, correcting both variables for measurement error is appropriate, however it may not be appropriate if you would like to use one variable to predict the other (e.g., exam scores to predict college grades). Since observed scores are all that is available to us, therefore correcting the predictor variable for measurement error will not capture its real world predictive utility.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'}}} }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'}}} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x within sample\n\n# Point Estimate\nrho = r / sqrt(rxx)\n\n# Standard Error\nSErho = SEr / sqrt(rxx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{d}{\\sqrt{r_{yy'}}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_{d}}{\\sqrt{r_{yy'}}} }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y within sample\n\n# Point Estimate\ndelta = d / sqrt(ryy)\n\n# Standard Error\nSEdelta = SEd / sqrt(ryy)\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.56, SE = 0.56\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate measurement error for continuous variables",
    "text": "Bivariate measurement error for continuous variables\nUnreliability of \\(x\\) and \\(y\\) will bias a pearson correlation coefficient by adding random, uncorrelated noise into the bivariate relationship. If the goal is to understand the relationship between the true, uncontaminated scores between two things, then measurement error on both variables should be corrected for.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\n\n# Point Estimate\nrho = r / sqrt(rxx * ryy)\n\n# Standard Error\nSErho = SEr / sqrt(rxx * ryy)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.67, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#misclassification",
    "href": "ArtifactCorrections.html#misclassification",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Misclassification",
    "text": "Misclassification\nMisclassification encompasses measurement errors for categorical variables. For example, our ability to identify individuals with major depressive disorder is only as accurate as our measurement of depression, therefore if the measure for depression contains measurement error then so will the assignment of individuals in the major depressive group and the control group (i.e., some people with major depressive disorder will be mis-labeled as a control and vice versa). To correct for group misclassification, you must first have an estimate of the phi coefficient (\\(\\phi_{gG}\\)) from the 2x2 contingency table comparing actual vs observed group membership. Phi can also be approximated directly from the misclassification rate (\\(p_{mis}\\)), however this may cause undercorrections when misclassification rates differ between groups.\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi\\) coefficient from the contingency table between actual group membership (G) and observed group membership (g):\n\\[\\displaystyle{ \\phi_{gG} = \\sqrt{\\frac{\\chi_{gG}^{2}}{n}} }\\] or \\[\\displaystyle{\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} }\\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 3. Dissatenuate correlation for misclassification:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\phi_{gG}} }\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nn = 50 # sample size\nchi2 = 32 # chi squared statistic for actual vs observed group contingency table\n# p_mis = .20 # misclassification rate (only if alternative step 1 is used)\n\n# Point Estimate\nphi = sqrt(chi2 / n)# step 1\n# phi = 1 - 2 * p_mis  # step 1 (assume equal misclassification)\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 2\nrho = r / phi # step 3\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.31, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate dichotomization of naturally continuous variables",
    "text": "Univariate dichotomization of naturally continuous variables\nIn some cases, researchers may categorize naturally continuous variables into two groups in order to facilitate interpretation or to perform specific statistical analyses. For example, congenital amusia, also known as tone deafness, is often diagnosed by scoring below a predetermined cutoff in a pitch or melodic discrimination task. This cutoff is arbitrary (although potentially useful) since pitch discrimination ability typically follows a normal distribution. Before someone can adjust the effect sizes for this artificial dichotomization, it is necessary to determine the proportions of participants above or below the cutoff (\\(p_x\\)) as well as the cutoff value (\\(c_{x}\\)), which can be estimated using the quantile function of a standard normal distribution at \\(p_{x}\\).\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\n\n\n# Point Estimate\ncx = qnorm(px) # Find cut point \nrho = r * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Standard Error\nSErho = SEr * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.63, SE = 0.13\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate - Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)):\n\\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Disattenuate correlation for artificial dichotomization:\n\\[\\displaystyle{ \\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} } }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\npx = 0.50 # proportion of individuals in upper or lower group of X\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\ncx = qnorm(px)# Find cut point\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r * sqrt(px * (1 - px)) / dnorm(cx)# step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 1.39, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate dichotomization of naturally continuous variables",
    "text": "Bivariate dichotomization of naturally continuous variables\nI somewhat rarer circumstances, researchers may dichotomize both x and y variables. This will attenuate correlation coefficients more than usual. If a tetrachoric correlation is available, this is more analogous to a pearson correlation coefficient than the correction below.\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x p_y (1-p_x) (1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x p_y(1-p_x)(1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\npy = 0.50 # proportion of individuals in upper or lower group of Y\n\n# Point Estimate\ncx = qnorm(px)# Find cut point on Y variable \ncy = qnorm(py)# Find cut point on Y variable\nrho = r * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Standard Error\nSErho = SEr * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.79, SE = 0.16\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction",
    "text": "Univariate Direct Range Restriction\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Standard Error\nSErho = SEr / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Correct r for direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed correlation\nSEd = 0.50 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1)) # step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )"
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction",
    "text": "Univariate Indirect Range Restriction\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\nStandard Error \\[\\displaystyle{se_{\\rho} = \\frac{se_{r}}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Standard Error\nSErho = SEr / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} \\]\n-Step 2. Correct r for direct range restriction:\n\\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} \\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 1\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))  # step 2\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.59, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction",
    "text": "Bivariate Direct Range Restriction\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define gamma:\n\\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n-Step 2. Correct r for bivariate direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = -\\Gamma + \\mathrm{sign} (r) \\sqrt{\\Gamma^2 + 1} }\\]\nStandard Error\n\\[\\displaystyle{se_{\\rho} = se_{r} \\left( \\frac{\\hat{\\rho}}{r} \\right) }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy  # step 1\nrho = -Gamma + sign(r) * sqrt(Gamma^2 + 1)# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.61, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction",
    "text": "Bivariate Indirect Range Restriction\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }\\]\nStandard Error\n-Step 1. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[\\displaystyle{ \\beta_1 = u_y r - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{\\sqrt{\\left|1 - u_y^2 \\right|^3}} }\\]\n-Step 2. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\displaystyle{ \\beta_2 = u_x r - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{\\sqrt{\\left|1 - u_x^2 \\right|^3}} }\\]\n-Step 3. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\displaystyle{ \\beta_3 = u_x u_y }\\] -Step 4. Calculate standard error for \\(u_x\\)\n\\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 5. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 6. Calculate standard error for \\(r\\)\n\\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\] -Step 7. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{b_1^2 se_{u_x}^2 + b_2^2 se_{u_y}^2 + b_3^2 se_{r}^2 } }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error for observed correlation between x and y\nrxx = 0.70  # reliability of x within study sample\nryy = 0.80 # reliability of y within study sample\nux  = 0.85 # observed u-ratio of x \nuy  = 0.80 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\nna = 200\nn = 100\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nb1  = r * uy - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / sqrt(abs(1 - uy^2)^3)# First order partial derivitive of ux\nb2  = r * ux - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / sqrt(abs(1 - ux^2)^3)# First order partial derivitive of uy\nb3  = ux*uy# First order partial derivitive of r\n\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.66, SE = 0.08\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction and Measurement Error",
    "text": "Univariate Direct Range Restriction and Measurement Error\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-r_{xx'})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } }\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85# ratio of observed standard deviation of y to reference standard deviation of x (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / ( ux * sqrt(1 - ux^2 * (1 - rxx)) * sqrt( r^2 * (1/ux^2 - 1) + ryy) )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.71, SE = 0.14\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership:\n\\[ \\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) }+d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-\\phi_{gG})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } \\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } \\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10# standard error of observed standardized mean difference\nn = 100 # sample size\nryy = 0.80 # reliability of y\nuy = 0.85 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \n#p_mis = 0.20 # proportion of individuals misclassified (only needed if equal misclassification rate is assumed, and alternative phi calculation)\n\n# Point Estimate\nphi =  sqrt(chi2 / n)  # step 1\n# phi = 1 - 2 * p_mis  # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 2\nrho = r / ( uy * sqrt(1 - uy^2 * (1 - phi)) * sqrt( r^2 * (1/uy^2 - 1) + ryy) )  # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.8, SE = 0.18\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction and Measurement Error",
    "text": "Univariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}}\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.70 # reliability of x\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.75, SE = 0.15\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership: \\[\\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g )} + d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \nn = 100 # sample size\n\n# Point Estimate\nphi =  sqrt(chi2 / n) # step 1\n# phi = 1 - 2 * p_mis # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2)# step 2\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  ) # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) ) # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.85, SE = 0.19\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction and Measurement Error",
    "text": "Bivariate Direct Range Restriction and Measurement Error\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate - Step 1. Define Gamma: \\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n\nStep 2. Correct r for univariate direct range restriction: \\[\\displaystyle{\\hat{\\rho} = \\frac{-\\Gamma u_x u_y + \\mathrm{sign}(r)\\sqrt{\\Gamma^2 + 1} }{\\sqrt{1-u_x^2(1-r_{xx'})}\\sqrt{1-u_y^2(1-r_{yy'})}} }\\]\n\nStandard Error \\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy # step 1\nrho = (-Gamma + sign(r) * sqrt(Gamma^2 + 1)) / (sqrt(1 - ux^2 * (1 - rxx)) * sqrt(1 - uy^2 * (1 - ryy)))# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.74, SE = 0.15\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction and Measurement Error",
    "text": "Bivariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\hat{\\rho} = \\frac{r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }{\\sqrt{1 - u_x^2 (1-r_{xx'})} \\sqrt{1 - u_y^2 (1 - r_{yy'})}}\\]\nStandard Error\n\nStep 1. Calculate the measurement quality index for X in the restricted sample: \\[ q_x = \\sqrt{r_{xx'}}\\]\nStep 2. Calculate the measurement quality index for Y in the restricted sample: \\[ q_y = \\sqrt{r_{yy'}}\\]\nStep 3. Estimated the measurement quality index for X in the unrestricted population:\n\\[q_X = \\sqrt{1 - u_x^2 (1 - r_{xx'})} \\]\nStep 4. Estimate the measurement quality index for Y in the unrestricted population: \\[ q_Y = \\sqrt{1 - u_y^2 (1 - r_{yy'})} \\]\nStep 5. Calculate first order partial derivative of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{xx'}\\) \\[ \\beta_1 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left(1 - u_x^2 \\right)\\left(1 - u_y^2 \\right)} }{q_X^2 q_Y} \\]\nStep 6. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{yy'}\\) \\[ \\beta_2 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left|1 - u_x^2 \\right|\\left|1 - u_y^2 \\right|} }{q_Y^2 q_X} \\]\nStep 7. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[ \\beta_3 = \\frac{u_y r}{q_X q_Y} - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_y^2 \\right|^3}}\\]\nStep 8. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\beta_4 = \\frac{u_x r}{q_X q_Y} - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_x^2 \\right|^3}} \\]\nStep 9. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\beta_5 = \\frac{u_x u_y}{q_X q_Y} \\]\nStep 10. Calculate standard error for unrestricted measure quality index (\\(q_X\\)) \\[se_{q_X} = \\sqrt{\\frac{1}{2} u_x^4 \\left[ \\frac{(1-q_x^2)^2}{1-u_x^2 (1-q_x^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_x^2 q_x^2(1 - q_x^2)^2}{\\left[1-u_x^2 (1-q_x^2)\\right] (n-1)} } \\]\nStep 11. Calculate standard error for unrestricted measure quality index (\\(q_Y\\)) \\[se_{q_Y} = \\sqrt{\\frac{1}{2} u_y^4 \\left[ \\frac{(1-q_y^2)^2}{1-u_y^2 (1-q_y^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_y^2 q_y^2(1 - q_y^2)^2}{\\left[1-u_y^2 (1-q_y^2)\\right] (n-1)} } \\]\nStep 12. Calculate standard error for \\(u_x\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 13. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 14. Calculate standard error for \\(r\\) \\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\]\nStep 15. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{\\beta_1^2 se_{q_X}^2 + \\beta_2^2 se_{q_Y}^2 + \\beta_3^2 se_{u_x}^2 + \\beta_4^2 se_{u_y}^2 + \\beta_5^2 se_{r}^2 } }\\]\n\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # observed correlation between x and y\nrxx = 0.80 # reliability of x within study sample\nryy = 0.70 # reliability of y within study sample\nn = 200 # sample size\nna = 100# sample size of reference sample\nux  = 0.80# observed u-ratio of x \nuy  = 0.85 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nqx = sqrt(rxx)\nqy = sqrt(ryy)\nqX = sqrt(ux^2 * (rxx - 1))\n\nWarning in sqrt(ux^2 * (rxx - 1)): NaNs produced\n\nqY = sqrt(uy^2 * (ryy - 1))\n\nWarning in sqrt(uy^2 * (ryy - 1)): NaNs produced\n\n# First order partial derivitive of qX\nb1  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qX^2 * qY) \n# First order partial derivitive of qY\nb2  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qY^2 * qX) \n# First order partial derivitive of ux\nb3  = (r * uy) / (qX * qY) - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / (qX * qY * sqrt(abs(1 - uy^2)^3))  \n# First order partial derivitive of uy\nb4  = (r * ux) / (qX * qY) - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / (qX * qY * sqrt(abs(1 - ux^2)^3))  \n# First order partial derivitive of r\nb5  = (ux*uy) / (qX * qY)\n\n# Standard error of qX\nSEqX = sqrt( .5 * ux^4 * ( (1-qx^2)^2 / (1-ux^2 * (1-qx^2)) ) * (1/(n-1) + 1/(na-1)) + ( (ux^2 * qx^2 * (1-qx^2)^2) ) / ((1 - ux^2 * (1-qx^2)) * (n-1)) )\n\n# Standard error of qY\nSEqY = sqrt( .5 * uy^4 * ( (1-qy^2)^2 / (1-uy^2 * (1-qy^2)) ) * (1/(n-1) + 1/(na-1)) + ( (uy^2 * qy^2 * (1-qy^2)^2) ) / ((1 - uy^2 * (1-qy^2)) * (n-1)) )\n\n# Standard error of ux\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of uy\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of r\nSEr =  (1 - r^2) / sqrt(n - 1)\n\n# Taylor series approximation\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(delta,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.85, SE = NaN\""
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Meta-Analysis Magic",
    "section": "",
    "text": "Welcome to my blog, Meta-Analysis Magic! Here, you’ll uncover mind-bending tricks that will help you unveil those elusive, unreported statistics tucked away in primary studies. But that’s not all – we’ll also dive into the thrilling realm of lesser-known meta-analysis modeling techniques that tend to slip under the radar.\nTo make your journey even more enchanting, each post will come complete with R code, making it a breeze for you to work your own meta-analysis wizardry. Let’s embark on this adventure together!\nIf you like the blog, consider subscribing below to be notified whenever a new post comes out (it’s free!).\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from a Paired T-Test\n\n\n\nrepeated measures\n\n\ncorrelations\n\n\ntest statistics\n\n\n\nIn this post we will go over how to convert a t-statistic from a paired t-test into a pre/post correlation. This is useful when change score standard deviations are unknown.…\n\n\n\nMatthew B. Jané\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximating Standard Deviation from Inter-Quantile Range\n\n\n\nstandard deviations\n\n\napproximations\n\n\n\nOccasionally researchers report inter-quantile ranges (e.g., inter-quartile range) to measure the spread of the data. Since meta-analysts need standard deviations to…\n\n\n\nMatthew B. Jané\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from the Standard Deviation of Change Scores\n\n\n\nrepeated measures\n\n\ncorrelations\n\n\n\nWelcome to the first blog post of Meta-Analysis Magic! In this post we will go over a couple of ways to directly calculate pre/post correlations from alternative…\n\n\n\nMatthew B. Jané\n\n\nSep 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]