[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Meta-Analysis Magic",
    "section": "",
    "text": "Welcome to my blog, Meta-Analysis Magic! I discuss meta-analysis stuff and sometimes measurement issues.\nIf you like the blog, consider subscribing below to be notified whenever a new post comes out (it’s free!).\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\nIndividual Person Data (IPD) Meta-Analysis with Beta Regression in a Repeated-Measures Cross-Over Design\n\n\n\nIndividual Participant Data\n\n\nRepeated Measures\n\n\nMeta-Analysis\n\n\n\nI worked on a meta-analysis along with my good friend [Tylor J.…\n\n\n\nMatthew B. Jané\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\nUnjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt\n\n\n\nSocial Media\n\n\nMental Health\n\n\nMeta-Analysis\n\n\n\nPart 3 of Haidt and Rausch’s response to the meta-analysis by Ferguson is now out and is written by Haidt’s analyst, David Stein. Stein is still running sub-group analyses…\n\n\n\nMatthew B. Jané\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Additive Models (GAMs) for Meta-Regression using brms\n\n\n\nGeneralized Additive Models\n\n\nMeta-regression\n\n\n\nA GAM is a linear or generalized linear model that allows you to model non-linear relationships using splines, smooth functions that allows to model the “wiggliness” in the…\n\n\n\nMatthew B. Jané\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\n\n\n\nA Response to Jonathan Haidt’s Response\n\n\n\nmeta-analysis\n\n\nsocial media\n\n\n\nI want to thank Jonathan Haidt and Zach Rausch for engaging with my critique of their re-analysis of Ferguson’s…\n\n\n\nMatthew B. Jané\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\nA Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt\n\n\n\nmeta-analysis\n\n\nsocial media\n\n\n\nI don’t know anything about the literature on social media and mental health so my focus on this post is to interrogate the statistical approach taken by the article written…\n\n\n\nMatthew B. Jané\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\n\n\n\nRespectful Operationalism, Realism, and Models of Measurement Error in Psychometrics\n\n\n\npsychological measurement\n\n\nmeasurement error\n\n\n\nFull disclaimer: I am not well-versed in these issues, and I am not a philosopher. The purpose of this post is for me to articulate my own thought process, rather than have…\n\n\n\nMatthew B. Jané\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\nA Response to Jonathan Haidt’s response\n\n\n\nsampling error\n\n\nartifacts\n\n\n\nIn this post we will discuss how squared effect sizes such as \\(\\eta^2\\) and \\(R^2\\) are distributed and the implications of this when conducting a meta-analysis\n\n\n\nMatthew B. Jané\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from a Paired T-Test\n\n\n\nrepeated measures\n\n\npre/post correlations\n\n\ntest statistics\n\n\n\nIn this post we will go over how to convert a t-statistic from a paired t-test into a pre/post correlation. This is useful when change score standard deviations are unknown.…\n\n\n\nMatthew B. Jané\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\nApproximating Standard Deviation from Inter-Quantile Range\n\n\n\nstandard deviations\n\n\napproximations\n\n\n\nOccasionally researchers report inter-quantile ranges (e.g., inter-quartile range) to measure the spread of the data. Since meta-analysts need standard deviations to…\n\n\n\nMatthew B. Jané\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nCalculating Pre/Post Correlation from the Standard Deviation of Change Scores\n\n\n\nrepeated measures\n\n\npre/post correlations\n\n\n\nWelcome to the first blog post of Meta-Analysis Magic! In this post we will go over a couple of ways to directly calculate pre/post correlations from alternative…\n\n\n\nMatthew B. Jané\n\n\nSep 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ArtifactCorrections.html",
    "href": "ArtifactCorrections.html",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "",
    "text": "Effect sizes are indices that help researchers, stakeholders, and policymakers understand the relationship between variables and draw meaningful conclusions from data. However, the usefulness of an effect size is only as good as its estimation. To ensure the accuracy of effect size estimates, it is important to mitigate the attenuation induced by various statistical artifacts, such as measurement error and range restriction. This page provides documentation on these artifacts and provides corrections that can be applied to attenuated effect sizes to obtain unbiased estimates of the true population effect size. Equations and R code are provided for each correction. For additional information on these corrections and their application in meta-analysis, consult the book by Hunter and Schmidt (1990) and the paper by Wiernik and Dahlke (2020) . The {psychmeta} package (Dahlke and Wiernik 2019) contains many of these corrections with convenient implementation in R."
  },
  {
    "objectID": "ArtifactCorrections.html#small-samples-1",
    "href": "ArtifactCorrections.html#small-samples-1",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Small Samples",
    "text": "Small Samples\nBelow are the correction factors that may be applied to the correlation coefficient (r) and the standardized mean difference (d). Note that the correction for r is not conventionally used as it tends to be a very small adjustment.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = r \\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = se_{r}\\cdot \\left( 1 + \\frac{1-r^2}{2(n-4)} \\right)}\\]\n\n# Parameters needed \nr =  0.50 # observed correlation between x and y \nn =  20 # sample size\nSEr = (1 - r^2) / sqrt(n - 1)  # standard error of observed correlation between x and y \n\n#Point Estimate \nrho = r * (1 + (1 - r^2) / (2 * (n - 4))) \n\n#Standard Error\nSErho = SEr *  (1 + (1 - r^2) / (2 * (n - 4)))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.51, SE = 0.18\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate \\[\\displaystyle{ \\hat{\\delta} = d \\left( 1-\\frac{3}{4n-9}\\right) }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = se_{d}\\left( 1-\\frac{3}{4n-9}\\right) }\\]\n\n# Parameters needed \nd = 0.50 # observed standardized mean difference \nn = 20 # total sample size\nSEd = 0.10 # standard error of observed standardized mean difference \n\n# Point Estimate\ndelta = d * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Standard Error\nSEdelta = SEd * ( 1 - 3 / (4 * (n - 9)) ) \n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.47, SE = 0.09\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-measurement-error-in-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate measurement error in continuous variables",
    "text": "Univariate measurement error in continuous variables\nMeasurement error will likely exist in both variables under invistigation (i.e., \\(x\\) and \\(y\\)), however applying a correction may depend on the research question. For example, if you would like to know how related two psychological constructs are, correcting both variables for measurement error is appropriate, however it may not be appropriate if you would like to use one variable to predict the other (e.g., exam scores to predict college grades). Since observed scores are all that is available to us, therefore correcting the predictor variable for measurement error will not capture its real world predictive utility.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'}}} }\\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'}}} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x within sample\n\n# Point Estimate\nrho = r / sqrt(rxx)\n\n# Standard Error\nSErho = SEr / sqrt(rxx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{d}{\\sqrt{r_{yy'}}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_{d}}{\\sqrt{r_{yy'}}} }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y within sample\n\n# Point Estimate\ndelta = d / sqrt(ryy)\n\n# Standard Error\nSEdelta = SEd / sqrt(ryy)\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.56, SE = 0.56\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-measurement-error-for-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate measurement error for continuous variables",
    "text": "Bivariate measurement error for continuous variables\nUnreliability of \\(x\\) and \\(y\\) will bias a pearson correlation coefficient by adding random, uncorrelated noise into the bivariate relationship. If the goal is to understand the relationship between the true, uncontaminated scores between two things, then measurement error on both variables should be corrected for.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}}{\\sqrt{r_{xx'} r_{yy'} }} }\\]\n\n# Parameters needed\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\n\n# Point Estimate\nrho = r / sqrt(rxx * ryy)\n\n# Standard Error\nSErho = SEr / sqrt(rxx * ryy)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.67, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#misclassification",
    "href": "ArtifactCorrections.html#misclassification",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Misclassification",
    "text": "Misclassification\nMisclassification encompasses measurement errors for categorical variables. For example, our ability to identify individuals with major depressive disorder is only as accurate as our measurement of depression, therefore if the measure for depression contains measurement error then so will the assignment of individuals in the major depressive group and the control group (i.e., some people with major depressive disorder will be mis-labeled as a control and vice versa). To correct for group misclassification, you must first have an estimate of the phi coefficient (\\(\\phi_{gG}\\)) from the 2x2 contingency table comparing actual vs observed group membership. Phi can also be approximated directly from the misclassification rate (\\(p_{mis}\\)), however this may cause undercorrections when misclassification rates differ between groups.\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi\\) coefficient from the contingency table between actual group membership (G) and observed group membership (g):\n\\[\\displaystyle{ \\phi_{gG} = \\sqrt{\\frac{\\chi_{gG}^{2}}{n}} }\\] or \\[\\displaystyle{\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} }\\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 3. Dissatenuate correlation for misclassification:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\phi_{gG}} }\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters needed\nd = 0.50 # observed standardized mean difference\nSEd = 0.50 # standard error of observed standardized mean difference\nrxx = 0.80 # reliability of x within sample\nryy = 0.70 # reliability of y within sample\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nn = 50 # sample size\nchi2 = 32 # chi squared statistic for actual vs observed group contingency table\n# p_mis = .20 # misclassification rate (only if alternative step 1 is used)\n\n# Point Estimate\nphi = sqrt(chi2 / n)# step 1\n# phi = 1 - 2 * p_mis  # step 1 (assume equal misclassification)\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 2\nrho = r / phi # step 3\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.31, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#univariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate dichotomization of naturally continuous variables",
    "text": "Univariate dichotomization of naturally continuous variables\nIn some cases, researchers may categorize naturally continuous variables into two groups in order to facilitate interpretation or to perform specific statistical analyses. For example, congenital amusia, also known as tone deafness, is often diagnosed by scoring below a predetermined cutoff in a pitch or melodic discrimination task. This cutoff is arbitrary (although potentially useful) since pitch discrimination ability typically follows a normal distribution. Before someone can adjust the effect sizes for this artificial dichotomization, it is necessary to determine the proportions of participants above or below the cutoff (\\(p_x\\)) as well as the cutoff value (\\(c_{x}\\)), which can be estimated using the quantile function of a standard normal distribution at \\(p_{x}\\).\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\n\n\n# Point Estimate\ncx = qnorm(px) # Find cut point \nrho = r * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Standard Error\nSErho = SEr * sqrt(px * (1 - px)) / dnorm(cx)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.63, SE = 0.13\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate - Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)):\n\\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Disattenuate correlation for artificial dichotomization:\n\\[\\displaystyle{ \\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x(1-p_x)}}{\\Phi(c_x)} } }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\npx = 0.50 # proportion of individuals in upper or lower group of X\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\ncx = qnorm(px)# Find cut point\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r * sqrt(px * (1 - px)) / dnorm(cx)# step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 1.39, SE = 0.13\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "href": "ArtifactCorrections.html#bivariate-dichotomization-of-naturally-continuous-variables",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate dichotomization of naturally continuous variables",
    "text": "Bivariate dichotomization of naturally continuous variables\nI somewhat rarer circumstances, researchers may dichotomize both x and y variables. This will attenuate correlation coefficients more than usual. If a tetrachoric correlation is available, this is more analogous to a pearson correlation coefficient than the correction below.\n\nCorrelation Coefficient (r)\nPoint Estimate Note: \\(\\Phi\\) indicates the normal ordinate of a standard normal distribution\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r\\sqrt{p_x p_y (1-p_x) (1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_{r}\\sqrt{p_x p_y(1-p_x)(1-p_y)}}{\\Phi(c_x)\\Phi(c_y)} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\npx = 0.50 # proportion of individuals in upper or lower group of X\npy = 0.50 # proportion of individuals in upper or lower group of Y\n\n# Point Estimate\ncx = qnorm(px)# Find cut point on Y variable \ncy = qnorm(py)# Find cut point on Y variable\nrho = r * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Standard Error\nSErho = SEr * sqrt(px * py * (1 - px) * (1 - py) ) / (dnorm(cx)*dnorm(cy))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.79, SE = 0.16\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction",
    "text": "Univariate Direct Range Restriction\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\rho}} = \\frac{se_r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Standard Error\nSErho = SEr / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[\\displaystyle{ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} }\\]\n-Step 2. Correct r for direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right) +1}} }\\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed correlation\nSEd = 0.50 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) ) # step 1\nrho = r / (ux * sqrt( r^2 * (1/ux^2 - 1) + 1)) # step 2\ndelta = rho / ( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )"
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction",
    "text": "Univariate Indirect Range Restriction\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate \\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\nStandard Error \\[\\displaystyle{se_{\\rho} = \\frac{se_{r}}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} }\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Standard Error\nSErho = SEr / sqrt( r^2 + ux^2 * (1 - r^2))\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.56, SE = 0.11\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) + d^2}}} \\]\n-Step 2. Correct r for direct range restriction:\n\\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + u_x^2 (1 - r^2)}} \\]\n-Step 3. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups:\n\\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nux = 0.85 # ratio of observed standard deviation to reference standard deviation (ux = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\n\n# Point Estimate\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 1\nrho = r / sqrt( r^2 + ux^2 * (1 - r^2))  # step 2\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 3\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.59, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction",
    "text": "Bivariate Direct Range Restriction\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define gamma:\n\\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n-Step 2. Correct r for bivariate direct range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = -\\Gamma + \\mathrm{sign} (r) \\sqrt{\\Gamma^2 + 1} }\\]\nStandard Error\n\\[\\displaystyle{se_{\\rho} = se_{r} \\left( \\frac{\\hat{\\rho}}{r} \\right) }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy  # step 1\nrho = -Gamma + sign(r) * sqrt(Gamma^2 + 1)# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.61, SE = 0.12\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction",
    "text": "Bivariate Indirect Range Restriction\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\displaystyle{ \\hat{\\rho} = r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }\\]\nStandard Error\n-Step 1. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[\\displaystyle{ \\beta_1 = u_y r - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{\\sqrt{\\left|1 - u_y^2 \\right|^3}} }\\]\n-Step 2. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\displaystyle{ \\beta_2 = u_x r - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{\\sqrt{\\left|1 - u_x^2 \\right|^3}} }\\]\n-Step 3. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\displaystyle{ \\beta_3 = u_x u_y }\\] -Step 4. Calculate standard error for \\(u_x\\)\n\\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 5. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\] -Step 6. Calculate standard error for \\(r\\)\n\\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\] -Step 7. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{b_1^2 se_{u_x}^2 + b_2^2 se_{u_y}^2 + b_3^2 se_{r}^2 } }\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error for observed correlation between x and y\nrxx = 0.70  # reliability of x within study sample\nryy = 0.80 # reliability of y within study sample\nux  = 0.85 # observed u-ratio of x \nuy  = 0.80 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\nna = 200\nn = 100\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nb1  = r * uy - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / sqrt(abs(1 - uy^2)^3)# First order partial derivitive of ux\nb2  = r * ux - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / sqrt(abs(1 - ux^2)^3)# First order partial derivitive of uy\nb3  = ux*uy# First order partial derivitive of r\n\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.66, SE = 0.08\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Direct Range Restriction and Measurement Error",
    "text": "Univariate Direct Range Restriction and Measurement Error\nIn certain situations, the selection of participants can be identical to one of the variables of interest. For instance, if a study is investigating the correlation between school grades and IQ in students with an intellectual disability, and the diagnosis is defined as having an IQ score of less than 70, then the sample would exhibit direct range restriction.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{ \\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-r_{xx'})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } }\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85# ratio of observed standard deviation of y to reference standard deviation of x (ux = SDsample/SDreference)\n\n# Point Estimate\nrho = r / ( ux * sqrt(1 - ux^2 * (1 - rxx)) * sqrt( r^2 * (1/ux^2 - 1) + ryy) )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.71, SE = 0.14\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership:\n\\[ \\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g ) }+d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{u_x \\sqrt{1 - u_x^2 (1-\\phi_{gG})  } \\sqrt{r^2 \\left(\\frac{1}{u_x^2} - 1\\right)+r_{yy'} } } \\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\displaystyle{ \\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} }\\]\nStandard Error\n\\[se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } \\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10# standard error of observed standardized mean difference\nn = 100 # sample size\nryy = 0.80 # reliability of y\nuy = 0.85 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \n#p_mis = 0.20 # proportion of individuals misclassified (only needed if equal misclassification rate is assumed, and alternative phi calculation)\n\n# Point Estimate\nphi =  sqrt(chi2 / n)  # step 1\n# phi = 1 - 2 * p_mis  # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2) # step 2\nrho = r / ( uy * sqrt(1 - uy^2 * (1 - phi)) * sqrt( r^2 * (1/uy^2 - 1) + ryy) )  # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) )  # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.8, SE = 0.18\""
  },
  {
    "objectID": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#univariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Univariate Indirect Range Restriction and Measurement Error",
    "text": "Univariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with one of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. For example, suppose a company is hiring employees based on their performance in a test that is correlated with their IQ. If the company only hires employees who score above a certain threshold on the test, then the range of IQ scores in the selected sample will be indirectly restricted. This is because the IQ scores of the selected employees will be higher than the IQ scores of the general population due to the correlation between the test scores and IQ.\n\nCorrelation Coefficient (r)\nPoint Estimate\n\\[\\displaystyle{\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}}\\]\nStandard Error\n\\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation\nSEr = 0.10 # standard error of observed correlation\nrxx = 0.70 # reliability of x\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80  # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  )\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.75, SE = 0.15\"\n\n\n\n\nStandardized Mean Difference (d)\nPoint Estimate\n-Step 1. Calculate \\(\\phi_{gG}\\) from the contingency table between observed and actual group membership: \\[\\phi_{gG} = \\sqrt{\\frac{\\chi^2}{n}} \\] or \\[\\phi_{gG} \\approx 1 - 2 \\cdot p_{mis} \\]\n-Step 2. Transform d to r using probability of observed group membership (\\(p_g\\)): \\[ r = \\frac{d}{ \\sqrt{\\frac{1}{p_g ( 1-p_g )} + d^2}} \\]\n-Step 3. Correct r for direct range restriction: \\[\\hat{\\rho} = \\frac{r}{\\sqrt{r^2 + \\frac{u_x^2 r_{xx'}(r_{xx'}r_{yy'}-r^2)}{1 - u_x^2 (1-r_{xx'})}}}\\]\n-Step 4. Back-transform \\(\\hat{\\rho}\\) to \\(\\hat{\\delta}\\) using probability of actual group membership (\\(p_G\\)). Observed group membership (\\(p_g\\)) can be used instead assuming equal misclassification rate between groups: \\[\\hat{\\delta} = \\frac{\\hat{\\rho}}{p_G (1-p_G )(1-\\hat{\\rho}^2)} \\]\nStandard Error \\[\\displaystyle{ se_{\\hat{\\delta}} = \\frac{se_d \\left( \\frac{\\hat{\\rho}}{r} \\right) }{\\left( 1+d^2 p_g[1-p_g] \\right)\\sqrt{\\left(d^2 + \\frac{1}{p_g(1-p_g)}\\right) p_G (1-p_G)(1-\\hat{\\rho}^2)^3} } }\\]\n\n# Parameters\nd = 0.50 # observed standardized mean difference\nSEd = 0.10 # standard error of observed standardized mean difference\nryy = 0.80 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\npg = 0.50 # observed proportion of individuals in group 1 or 2\npG = 0.50 # actual proportion of individuals in group 1 or 2 (pG=pg when misclassification rate is equal among groups)\nchi2 = 36 # chi squared statistic for actual vs observed group contingency table \nn = 100 # sample size\n\n# Point Estimate\nphi =  sqrt(chi2 / n) # step 1\n# phi = 1 - 2 * p_mis # step 1 (alternative)\nr = d / sqrt(1 / (pg * (1 - pg)) + d^2)# step 2\nrho = r / sqrt(r^2 +  ux^2 * rxx * (rxx * ryy - r^2) / (1 - ux^2 * (1-rxx))  ) # step 3\ndelta = rho / sqrt( pG * (1 - pG) * (1 - rho^2) ) # step 4\n\n# Standard Error\nSEdelta = SEd * (rho / r) / ( (1 + d^2 * pg * (1 - pg)) * sqrt( (d^2 + 1/(pg*(1-pg))) * ( pG * (1 - pG) * (1 - rho^2)^3) ) )\n\n# Print Results\npaste0('delta = ',round(delta,2),', SE = ',round(SEdelta,2) )\n\n[1] \"delta = 0.85, SE = 0.19\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-direct-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Direct Range Restriction and Measurement Error",
    "text": "Bivariate Direct Range Restriction and Measurement Error\nIn some instances, direct selection can be placed on both x and y variables. This may happen in instances where a researcher requires subjects to be within the “normal” range of x and y, which tends to restrict the range by excluding individuals at the tails of x and y.\n\nCorrelation Coefficient (r)\nPoint Estimate - Step 1. Define Gamma: \\[\\displaystyle{ \\Gamma = u_x u_y \\frac{1 - r^2}{2r}}\\]\n\nStep 2. Correct r for univariate direct range restriction: \\[\\displaystyle{\\hat{\\rho} = \\frac{-\\Gamma u_x u_y + \\mathrm{sign}(r)\\sqrt{\\Gamma^2 + 1} }{\\sqrt{1-u_x^2(1-r_{xx'})}\\sqrt{1-u_y^2(1-r_{yy'})}} }\\]\n\nStandard Error \\[\\displaystyle{se_{\\hat{\\rho}} = se_r \\left( \\frac{\\hat{\\rho}}{r} \\right)}\\]\n\n# Parameters\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # standard error of observed correlation between x and y\nrxx = 0.80 # reliability of x\nryy = 0.70 # reliability of y\nux = 0.85 # ratio of observed standard deviation of x to reference standard deviation of x (ux = SDsample/SDreference)\nuy = 0.80 # ratio of observed standard deviation of y to reference standard deviation of y (uy = SDsample/SDreference)\n\n# Point Estimate\nGamma = (1 - r^2) / (2*r) * ux * uy # step 1\nrho = (-Gamma + sign(r) * sqrt(Gamma^2 + 1)) / (sqrt(1 - ux^2 * (1 - rxx)) * sqrt(1 - uy^2 * (1 - ryy)))# step 2\n\n# Standard Error\nSErho = SEr * (rho / r)\n\n# Print Results\npaste0('rho = ',round(rho,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.74, SE = 0.15\""
  },
  {
    "objectID": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "href": "ArtifactCorrections.html#bivariate-indirect-range-restriction-and-measurement-error",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Bivariate Indirect Range Restriction and Measurement Error",
    "text": "Bivariate Indirect Range Restriction and Measurement Error\nWhen the selection process is correlated with both of the variables of interest then the resulting sample with have a reduced variance due to indirect range restriction. This is often the case in college admissions testing where both the predictor (e.g., SAT scores) and the outcome variable (e.g., first year GPA) are correlated with the college admissions process.\n\nCorrelation Coefficient (r)\nPoint Estimate\n-Step 1. Define lambda:\n\\[\\displaystyle{ \\lambda = \\mathrm{sign} (r_{sx} r_{sy} [1 - u_x] [1 - u_y ])\\frac{\\mathrm{sign} (1-u_x) \\mathrm{min} (u_x, 1/u_x) + \\mathrm{sign} (1-u_y) \\mathrm{min} (u_y, 1/u_y) }{\\mathrm{min} (u_x, 1/u_x) + \\mathrm{min} (u_y, 1/u_y)} }\\]\n-Step 2. Correct r for bivariate indirect range restriction:\n\\[\\hat{\\rho} = \\frac{r u_x u_y + \\lambda \\sqrt{\\left|1-u_x^2 \\right|\\left|1-u_y^2 \\right|} }{\\sqrt{1 - u_x^2 (1-r_{xx'})} \\sqrt{1 - u_y^2 (1 - r_{yy'})}}\\]\nStandard Error\n\nStep 1. Calculate the measurement quality index for X in the restricted sample: \\[ q_x = \\sqrt{r_{xx'}}\\]\nStep 2. Calculate the measurement quality index for Y in the restricted sample: \\[ q_y = \\sqrt{r_{yy'}}\\]\nStep 3. Estimated the measurement quality index for X in the unrestricted population:\n\\[q_X = \\sqrt{1 - u_x^2 (1 - r_{xx'})} \\]\nStep 4. Estimate the measurement quality index for Y in the unrestricted population: \\[ q_Y = \\sqrt{1 - u_y^2 (1 - r_{yy'})} \\]\nStep 5. Calculate first order partial derivative of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{xx'}\\) \\[ \\beta_1 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left(1 - u_x^2 \\right)\\left(1 - u_y^2 \\right)} }{q_X^2 q_Y} \\]\nStep 6. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(\\rho_{yy'}\\) \\[ \\beta_2 = -\\frac{u_x u_y r + \\lambda \\sqrt{\\left|1 - u_x^2 \\right|\\left|1 - u_y^2 \\right|} }{q_Y^2 q_X} \\]\nStep 7. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_x\\) \\[ \\beta_3 = \\frac{u_y r}{q_X q_Y} - \\frac{\\lambda u_x (1 - u_x^2) \\sqrt{\\left|1 - u_x^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_y^2 \\right|^3}}\\]\nStep 8. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(u_y\\) \\[\\beta_4 = \\frac{u_x r}{q_X q_Y} - \\frac{\\lambda u_y (1 - u_y^2) \\sqrt{\\left|1 - u_y^2 \\right|}}{q_X q_Y\\sqrt{\\left|1 - u_x^2 \\right|^3}} \\]\nStep 9. Calculate first order partial derivitive of \\(\\hat{\\rho}\\) with respect to \\(r\\) \\[\\beta_5 = \\frac{u_x u_y}{q_X q_Y} \\]\nStep 10. Calculate standard error for unrestricted measure quality index (\\(q_X\\)) \\[se_{q_X} = \\sqrt{\\frac{1}{2} u_x^4 \\left[ \\frac{(1-q_x^2)^2}{1-u_x^2 (1-q_x^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_x^2 q_x^2(1 - q_x^2)^2}{\\left[1-u_x^2 (1-q_x^2)\\right] (n-1)} } \\]\nStep 11. Calculate standard error for unrestricted measure quality index (\\(q_Y\\)) \\[se_{q_Y} = \\sqrt{\\frac{1}{2} u_y^4 \\left[ \\frac{(1-q_y^2)^2}{1-u_y^2 (1-q_y^2)} \\right] \\left[ \\frac{1}{n-1} + \\frac{1}{n_{\\mathrm{ref}}-1} \\right] + \\frac{u_y^2 q_y^2(1 - q_y^2)^2}{\\left[1-u_y^2 (1-q_y^2)\\right] (n-1)} } \\]\nStep 12. Calculate standard error for \\(u_x\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_x} = u_x \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 13. Calculate standard error for \\(u_y\\) (note: sample size for reference sample is \\(n_\\mathrm{ref}\\)): \\[\\displaystyle{ se_{u_y} = u_y \\sqrt{ \\frac{1}{2(n-1)} + \\frac{1}{2(n_{\\mathrm{ref}} -1)}} }\\]\nStep 14. Calculate standard error for \\(r\\) \\[\\displaystyle{ se_r = \\frac{1-r^2}{\\sqrt{n-1}} }\\]\nStep 15. Calculate standard error of \\(\\hat{\\rho}\\) using a Taylor Series Approximation \\[\\displaystyle{se_{\\hat{\\rho}} \\approx \\sqrt{\\beta_1^2 se_{q_X}^2 + \\beta_2^2 se_{q_Y}^2 + \\beta_3^2 se_{u_x}^2 + \\beta_4^2 se_{u_y}^2 + \\beta_5^2 se_{r}^2 } }\\]\n\n\n# Parameters needed\nr = 0.50 # observed correlation between x and y\nSEr = 0.10 # observed correlation between x and y\nrxx = 0.80 # reliability of x within study sample\nryy = 0.70 # reliability of y within study sample\nn = 200 # sample size\nna = 100# sample size of reference sample\nux  = 0.80# observed u-ratio of x \nuy  = 0.85 # observed u-ratio of y\nrsy = 1 # direction of correlation between selector and y (-1 = negative, 0 = no correlation, 1 = positive)\nrsx = 1 # direction of correlation between selector and x (-1 = negative, 0 = no correlation, 1 = positive)\n\n# Point Estimate\nlambda = sign( rsx * rsy * (1-ux) * (1-uy) ) * ( sign(1 - ux) * min(c(ux,1/ux)) + sign(1 - uy) * min(c(uy,1/uy)) ) / ( min(c(ux,1/ux)) + min(c(uy,1/uy)) )\nrho = r * ux * uy + lambda * sqrt( abs(1 - ux^2) * abs(1 - uy^2) )\n\n# Standard Error (Taylor Series Approximation)\nqx = sqrt(rxx)\nqy = sqrt(ryy)\nqX = sqrt(ux^2 * (rxx - 1))\n\nWarning in sqrt(ux^2 * (rxx - 1)): NaNs produced\n\nqY = sqrt(uy^2 * (ryy - 1))\n\nWarning in sqrt(uy^2 * (ryy - 1)): NaNs produced\n\n# First order partial derivitive of qX\nb1  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qX^2 * qY) \n# First order partial derivitive of qY\nb2  = - ( ux * uy * r + lambda * sqrt(abs(1 - ux^2) * abs(1 - uy^2)) ) / (qY^2 * qX) \n# First order partial derivitive of ux\nb3  = (r * uy) / (qX * qY) - ( lambda * ux *(1 - ux^2) * sqrt( abs(1 - uy^2) ) ) / (qX * qY * sqrt(abs(1 - uy^2)^3))  \n# First order partial derivitive of uy\nb4  = (r * ux) / (qX * qY) - ( lambda * uy *(1 - uy^2) * sqrt( abs(1 - ux^2) ) ) / (qX * qY * sqrt(abs(1 - ux^2)^3))  \n# First order partial derivitive of r\nb5  = (ux*uy) / (qX * qY)\n\n# Standard error of qX\nSEqX = sqrt( .5 * ux^4 * ( (1-qx^2)^2 / (1-ux^2 * (1-qx^2)) ) * (1/(n-1) + 1/(na-1)) + ( (ux^2 * qx^2 * (1-qx^2)^2) ) / ((1 - ux^2 * (1-qx^2)) * (n-1)) )\n\n# Standard error of qY\nSEqY = sqrt( .5 * uy^4 * ( (1-qy^2)^2 / (1-uy^2 * (1-qy^2)) ) * (1/(n-1) + 1/(na-1)) + ( (uy^2 * qy^2 * (1-qy^2)^2) ) / ((1 - uy^2 * (1-qy^2)) * (n-1)) )\n\n# Standard error of ux\nSEux = ux * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of uy\nSEuy = uy * sqrt( 1 / (2*(n-1)) + 1 / (2*(na-1)) )\n\n# Standard error of r\nSEr =  (1 - r^2) / sqrt(n - 1)\n\n# Taylor series approximation\nSErho = sqrt( b1^2 * SEux^2 + b2^2 * SEuy^2 + b3^2 * SEr^2 )\n\n# Print Results\npaste0('rho = ',round(delta,2),', SE = ',round(SErho,2) )\n\n[1] \"rho = 0.85, SE = NaN\""
  },
  {
    "objectID": "Software.html",
    "href": "Software.html",
    "title": "Software",
    "section": "",
    "text": "ThemePark\nGenerating popular culture styled ggplot themes. Featured on rweekly.org and flowingdata.com\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOSC\nAn R Package for generating Probability of Outcome Superiority Curves (POSCs).\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenSynthesis\nWebsite cataloging publicly available meta-analytic databases.\n Github Repository  Webpage\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Simulator\nA Shiny App for Visualizing Statistical Artifacts.\n Shiny App\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtifact Corrections for Effect Sizes\nA webpage documenting equations and code for effect size artifact corrections. MatthewBJane.com/ArtifactCorrections\n Webpage\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-Analysis Data & Code\nRepository of data and code for all meta-analytic projects.\n Github Repository\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary Study Data & Code\nRepository of data and code for all primary study projects.\n Github Repository"
  },
  {
    "objectID": "blog-posts/blog-post-6.html#introduction",
    "href": "blog-posts/blog-post-6.html#introduction",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Introduction",
    "text": "Introduction\nThe article by Rausch and Haidt (R&H) conducts a re-analysis of a meta-analysis by Ferguson (2024) on RCTs assessing the causal effect (for the sake of this post, I am avoiding any causal inference issues) of social media usage on outcomes related to well-being (e.g., life satisfaction, anxiety). However, the re-analysis conducted by R&H has severe flaws. Generally speaking, if you conduct a re-analysis of any study you should be certain that the re-analysis is of higher methodological quality than the original study. That is not the case here. This blog will get a bit technical, but it is important because meta-analysis isn’t just crunching numbers across studies. There is a formal statistical model that we should adhere to to actually produce coherent and interpretable results.\nThe original meta-analysis by Ferguson (2024) is not without a number of flaws, but I think it accomplished what it set out to do. Notably, the meta-analysis was pre-registered, transparent, and had publicly available data. The only problem with the open data is that the statistics used to calculate the effect sizes (reported t-statistics, means, SDs, etc.) are completely missing from the database which makes reproducing it a lot harder.\nFerguson (2024)’s meta-analysis has a couple things that stood out to me as I was skimmed through it. It seems that all measures related to mental health and well-being are combined even if they are conceptually quite different (e.g., self-esteem and anxiety). When a study reported multiple different outcome measures, Ferguson decided to pool them together presumably to handle the dependency in effects. This is not an ideal strategy for two reasons, it assumes there is no heterogeneity between outcomes and it also loses specificity in the outcome. It would have been nice to see narrower bins of constructs meta-analyzed separately such as clinical anxiety/depression measures or life satisfaction/subjective well-being measures.\nAlso, the results convey a common misinterpretation of the estimate in a random effects meta-analysis. The meta-analytic estimate was communicated as follows:\n\nAs can be seen the overall estimate for d across studies was 0.088, which was nonsignificant and well below the SESOI (r = .10, d = 0.21).\n\nWhat does an “overall estimate for d” actually mean (note: d is the effect size, we will talk about this in the next section)? Is there a true d that we are estimating by averaging the d values from all these studies together? No. A random-effects meta-analysis is estimating the mean of true effect sizes. That is, there exists a distribution of true effects and the meta-analytic estimate conveys the central tendency. Therefore, the meta-analytic estimate can not tell you whether the the intervention produces an effect of d = .088 (lets ignore the variability in this estimate for the moment), it only tells you that the effect is d = .088, on average. Heterogeneity in effect sizes describes how wide the distribution of true effects is. You could have genuine true effects that are very positive (i.e., social media use is detrimental) and very negative (i.e., social media use is beneficial) even if the meta-analytic estimate is zero. There also could be study-level characteristics that could account for this heterogeneity, for instance, studies using a particular type of experimental design or a certain outcome shows a positive effect while others don’t.\nA side note: I also came across this sentence from Ferguson (2024)’s meta-analysis on page 3:\n\nGiven the high power of meta-analysis, almost all meta-analyses are “statistically significant.”\n\nI have heard this before but it is just not true, in general. In a random effects meta-analysis where you have both within and between study variance can give pretty wide confidence intervals and large p-values. This would be a reasonable statement if you are referring to a fixed effect meta-analysis with a large total sample size or a random effect meta-analysis with a ton of studies. Neither of which are true in this case."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#lets-get-into-it",
    "href": "blog-posts/blog-post-6.html#lets-get-into-it",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Let’s get into it",
    "text": "Let’s get into it\nThe first part of this blog sets up a statistical meta-analysis model called a random-effects model. It is important to keep in mind that a meta-analysis is not throwing a bunch of standardized effects into a calculator and computing a mean. We need a coherent statistical model that allows us to draw meaningful inferences from the data."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#effect-size-of-interest",
    "href": "blog-posts/blog-post-6.html#effect-size-of-interest",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Effect Size of Interest",
    "text": "Effect Size of Interest\nFirst thing we should do is to define the effect size we are using to quantify the effect of social media on well-being related outcomes. The article by Ferguson (2024) says they used a standardized mean difference known as a Cohen’s d to quantify the difference between the post-test scores in the social media using group and the non-social media using group (or some variation of that). You may think “we should be controlling for baseline differences between treatment and control groups”, well in an RCT subject’s are randomly allocated to each group so their baseline scores should be, on average, the same. We can define a (true) effect size \\(\\delta\\) as,\n\\[\n\\delta = \\frac{\\mu_\\textrm{social} - \\mu_\\textrm{no social}}{\\sigma}\n\\tag{1}\\]\nwhere \\(\\mu_\\textrm{social}\\) and \\(\\mu_\\textrm{no social}\\) is the mean of post-test scores for the social media using group and the non-social media using group (i.e., control group), respectively. The standardizer \\(\\sigma\\) is the within-group standard deviation and it assumes that both the social media and non-social media group have the same standard deviation (probably a fine assumption 🤷). A sample estimator of the effect size will be denoted as \\(d\\) and it represents the effect size we actually observe from the results of a study,\n\\[\nd = \\frac{m_\\textrm{social} - m_\\textrm{no social}}{s_\\mathrm{pooled}},\n\\tag{2}\\]\nwhere English letters denote the sample estimators of the corresponding parameters in Equation 1. The standardizer \\(s_\\mathrm{pooled}\\) here is the pooled standard deviation. Since we only have access to the standard deviation of each group we have to pool them together to get a more precise estimate of \\(\\sigma\\) (again, we are assuming the standard deviations within both groups are equal in the population),\n\\[\ns_\\mathrm{pooled} = \\sqrt{\\frac{(n_\\textrm{social}-1)s^2_\\textrm{social} + (n_\\textrm{no social}-1)s^2_\\textrm{no social}}{n_\\textrm{social} + n_\\textrm{no social} - 2}}\n\\]\nI will note here, that there was no correction for small sample bias in the original meta-analysis as far as I could tell so I will not apply any correction here."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#statistical-model",
    "href": "blog-posts/blog-post-6.html#statistical-model",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Statistical Model",
    "text": "Statistical Model\nUltimately, the observed effect size \\(d\\) is a sample estimate of the true effect size \\(\\delta\\) and therefore will contain error. This error is referred to as sampling error and is defined as the difference between the observed effect size and the true effect size. For a given study \\(i\\), we can relate the observed effect size \\(d_i\\) and the true effect size \\(\\delta_i\\) by including a sampling error term (\\(\\varepsilon_i\\)),\n\\[\nd_i = \\delta_i + \\varepsilon_i\n\\] where \\(\\varepsilon_i\\) is literally defined as \\(\\varepsilon_i = d_i - \\delta_i\\). The extent to which sampling errors are obscuring our effect size can be captured by the sampling variance. We can assume that sampling errors are normally distributed and evenly distributed around zero such that, \\(\\varepsilon_i \\sim \\mathcal{N}(0,v_i)\\) where \\(v_i\\) is the sampling variance for study \\(i\\). The sampling variance for a given study is dependent on the sample size with larger samples having more precision and thus less variance. Assuming normally distributed data, the sampling variance of the standardized mean difference (Equation 2) is calculated with the following formula,\n\\[\nv = \\left(\\frac{n-1}{n-3}\\right) \\left(\\frac{n}{n_\\textrm{social}n_\\textrm{no social}}\\right)\\left( \\frac{d^2}{2n}\\right)\n\\]\nwhere \\(n=n_\\textrm{social}+n_\\textrm{no social}\\). However unfortunately, the available dataset from Ferguson (2024) did not provide the sample sizes within each group so instead we will have to assume they are equal (\\(n_\\textrm{social}=n_\\textrm{no social}\\)) and use the simplified formula,\n\\[\nv = \\left(\\frac{n-1}{n-3}\\right) \\left(\\frac{4}{n}\\right)\\left(1 + \\frac{d^2}{8}\\right)\n\\]\nNow we might expect that any variation in effect sizes across studies is attributable to variance in random sampling errors, however this is unlikely to be the case. In reality, effect sizes tend to vary across studies above and beyond what we would expect from sampling error alone. This extra variation (known as heterogeneity) in effect sizes could be due to sampling from different populations, utilizing different methodologies, using different measures, etc.\nRecall that we modeled an observed effect size as the sum of the true effect size for that study and sampling error \\(d_i = \\delta_i + \\varepsilon_i\\). We defined the distribution of sampling errors so now we can also define the distribution of true effect sizes as \\(\\delta_i \\sim \\mathcal{N}(\\mu_\\delta,\\tau^2)\\) where \\(\\mu_\\delta\\) is the mean of true effect sizes across studies and \\(\\tau^2\\) is the variance of true effect sizes (i.e., heterogeneity). Ultimately our goal in a meta-analysis is to estimate both \\(\\mu_\\delta\\) and \\(\\tau\\). The common misinterpretation in (random-effect) meta-analyses that I described in the introduction is that a meta-analysis is estimating a single true effect size. If there is heterogeneity (\\(\\tau^2 &gt; 0\\)) then there is a distribution of true effects that we must consider."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#parameter-estimation",
    "href": "blog-posts/blog-post-6.html#parameter-estimation",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nIn meta-analysis we are trying to estimate particular parameters (i.e., the mean \\(\\mu_\\delta\\) and variance \\(\\tau\\) of true effect sizes) just like any other statistical model. Simply averaging study effect sizes to obtain some “overall” effect size is not a principled statistical approach. This is why we went through the trouble of describing a random effects model in the previous section. We want to estimate the mean of true effect sizes (not the “overall effect size” or “the true effect size”). To estimate the mean of true effects \\(\\mu_\\delta\\) we can not just calculate the arithmetic mean of observed effect sizes. Instead we must use appropriate weights that take advantage of differential precision across studies. Particularly, we want to weight each study by the inverse variance,\n\\[\nw_i = \\frac{1}{\\tau^2 + v_i}\n\\] This will preferentially weight studies with less sampling variance (i.e., larger sample sizes). Also, since \\(\\tau^2\\) is the same for all studies, the larger \\(\\tau^2\\) is relative to \\(v_i\\) then the weights will become more equal across studies. We can then use these weights to estimate the mean true effect size by taking a weighted average of observed effect sizes across \\(k\\) studies,\n\\[\n\\hat{\\mu}_\\delta = \\frac{\\sum^k_{i=1} w_i d_i}{\\sum^k_{i=1} w_i}\n\\] where the little carrot \\(\\hat{\\cdot}\\) denotes an estimate. We can also compute the 95% confidence interval of \\(\\hat{\\mu}_\\delta\\) (the set of plausible values of the actual mean that are compatible with the data) using the Hartung, Knapp, Sidik and Jonkman method (this adjusts for poor coverage rates in small \\(k\\) meta-analyses, which is the case here),\n\\[\nCI =  \\hat{\\mu}_\\delta \\pm 1.96 \\cdot\\sqrt{\\frac{\\sum^k_{i=1}w_i(d_i-\\mu_\\delta)^2}{(k-1)\\sum^k_{i=1}w_i}}\n\\]\nThe last two things we want to estimate are the variance in true effects \\(\\tau^2\\) and what is known as a prediction interval. The variance in true effects is estimated a variety of different ways and we won’t go through all of them here. Instead we will exclusively use a restricted maximum likelihood estimator (REML) which is an iterative estimation procedure. Ferguson (2024) uses a maximum likelihood estimator (ML), however ML tends to under-estimate the heterogeneity. The 95% prediction interval uses the information from \\(\\hat{\\tau}\\) to construct an interval that can be interpreted as the interval where 95% of true effect sizes fall.\n\\[\nPI = \\hat{\\mu}_\\delta \\pm 1.96 \\cdot\\hat\\tau\n\\]\nSo just keep in mind that the \\(CI\\) describes the variability in the estimate of the mean and \\(PI\\) describes the variability in the true effects."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#summarizing-and-reproducing-ferguson2024",
    "href": "blog-posts/blog-post-6.html#summarizing-and-reproducing-ferguson2024",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Summarizing and reproducing Ferguson (2024)",
    "text": "Summarizing and reproducing Ferguson (2024)\nOkay now that we have set up our model, we can now actually look at the data from the meta-analysis. Let’s load in some R packages and the dataset from the OSF repository referenced in Ferguson (2024).\n\n# load in packages\nlibrary(osfr)\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(readxl)\nlibrary(psychmeta)\n\n# download data from OSF repo\nread_xlsx(\"~/Documents/MatthewBJane.nosync/blog-posts/Best Practices Coding Experiments.xlsx\")\n\n# A tibble: 35 × 16\n   Citation       block     n      d Standardized Outcome…¹ `Validated Outcomes`\n   &lt;chr&gt;          &lt;lgl&gt; &lt;dbl&gt;  &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n 1 Alcott 2020    NA     1661  0.09                       1                    1\n 2 Brailovskaia … NA      286  0.154                      1                    1\n 3 Brailovskaia … NA      322  0                          1                    1\n 4 Collins and E… NA      121 -0.138                      1                    1\n 5 Deters & Mehl… NA       86 -0.207                      1                    1\n 6 Faulhaber et … NA      230  0.484                      1                    1\n 7 Gajdics 2022   NA      235 -0.364                      1                    1\n 8 Hall et al. 2… NA      130 -0.007                      1                    1\n 9 Hunt 2018      NA      143  0.232                      1                    1\n10 Hunt 2021      NA       88  0.374                      1                    1\n# ℹ 25 more rows\n# ℹ abbreviated name: ¹​`Standardized Outcomes`\n# ℹ 10 more variables: `Matched Control Condition` &lt;dbl&gt;,\n#   `Distractor Questionnaires` &lt;dbl&gt;, `Query hypothesis guessing` &lt;dbl&gt;,\n#   Preregistration &lt;dbl&gt;, Age &lt;dbl&gt;, Year &lt;dbl&gt;, `Best Practices Total` &lt;dbl&gt;,\n#   `Verified 1= yes, 2=no` &lt;dbl&gt;, `Ratio at post` &lt;dbl&gt;,\n#   `Citation Bias 1=yes, 2 = no` &lt;dbl&gt;\n\n# load in dataset and filter out studies without data\ndat &lt;- read_excel(\"Best Practices Coding Experiments.xlsx\",\n                  sheet = 1) %&gt;%\n  filter(!is.na(d))\n\n# display effect size data for first 6 studies\nhead(dat[c(\"Citation\",\"n\",\"d\")])\n\n# A tibble: 6 × 3\n  Citation                    n      d\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcott 2020              1661  0.09 \n2 Brailovskaia 2020         286  0.154\n3 Brailovskaia 2022         322  0    \n4 Collins and Edgers 2022   121 -0.138\n5 Deters & Mehl 2013         86 -0.207\n6 Faulhaber et al., 2023    230  0.484\n\n\nThe data only gives us the effect size d and the sample size n for each study so we need to calculate the sampling variance. Using the psychmeta package we can calculate the sampling variance for each study as follows,\n\n# compute sampling variance\ndat &lt;- dat %&gt;% \n  mutate(v = var_error_d(d = d, n1 = n))\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\")])\n\n# A tibble: 6 × 4\n  Citation                    n      d       v\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241\n2 Brailovskaia 2020         286  0.154 0.0141 \n3 Brailovskaia 2022         322  0     0.0125 \n4 Collins and Edgers 2022   121 -0.138 0.0337 \n5 Deters & Mehl 2013         86 -0.207 0.0479 \n6 Faulhaber et al., 2023    230  0.484 0.0181 \n\n\nNow we are set up to fit a random effects model. Using the metafor package we can use the REML method for estimating the heterogeneity and the Hartung-Knapp-Sidik-Jonkman method for obtaining the confidence intervals and p-values.\n\nmdl &lt;- rma(yi = d, \n           vi = v, \n           data = dat,\n           method = \"REML\",\n           test=\"knha\")\nmdl\n\n\nRandom-Effects Model (k = 27; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0564 (SE = 0.0230)\ntau (square root of estimated tau^2 value):      0.2374\nI^2 (total heterogeneity / total variability):   76.62%\nH^2 (total variability / sampling variability):  4.28\n\nTest for Heterogeneity:\nQ(df = 26) = 91.9083, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval    ci.lb   ci.ub    \n  0.0879  0.0576  1.5249  26  0.1394  -0.0306  0.2063    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe are pretty much able to reproduce the results seen in Ferguson (2024) with slight differences probably due to having to assume equal sample sizes between groups, using a different heterogeneity estimator and using the Hartung-Knapp-Sidik-Jonkman method for the confidence intervals. Let’s get the 95% prediction interval as well.\n\npredict.rma(mdl)\n\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.0879 0.0576 -0.0306 0.2063 -0.4143 0.5901 \n\n\nSo, 95% of true effects fall between -.41 and .59 according to our model."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#rhs-analysis",
    "href": "blog-posts/blog-post-6.html#rhs-analysis",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "R&H’s analysis",
    "text": "R&H’s analysis\nR&H conduct a sub-group analysis by chopping up the studies into three bins. First issue here is that this analysis is completely post-hoc and who knows if they created these bins after trying various combinations to reach the conclusion that they desired. R&H separate the 27 studies into three bins (this is verbatim from the article):\n\nMulti-week reduction experiments. These ten studies examined the impact of reducing social media use for at least two weeks, allowing withdrawal symptoms to dissipate.\nShort (one week or less) reduction experiments. These ten studies examined brief periods of abstinence from social media use, which are likely to pick up withdrawal symptoms from heavy users.\nExposure experiments. In these seven studies, the ‘treatment’ is typically brief exposure to some kind of social media, such as requiring high school students to look at their Facebook or Instagram page for 10 minutes.\n\nThe second problem we see is that the first two groups are separated by dichotomizing a naturally continuous variable (length of intervention). Sure it makes the data more digestible, but you simply lose information. This is generally considered bad statistical practice and we will see why a little later.\nTo reproduce R&H’s analysis, we will add a column to the data set that denotes the group membership (1 = Multi-week reduction experiments, 2 = One week reduction experiments, 3 = Less than one week reduction experiments, 4 = Exposure experiments) of each study.\n\ndat &lt;- dat %&gt;%\n  mutate(subgroup = c(1, # Alcott 2020\n                      1, # Brailovskaia 2020\n                      1, # Brailovskaia 2022\n                      1, # Collins and Edgers 2022\n                      4, # Deters & Mehl 2013\n                      1, # Faulhaber et al., 2023\n                      3, # Gajdics 2022\n                      1, # Hall et al. 2021\n                      1, # Hunt 2018\n                      1, # Hunt 2021\n                      2, # Kleefeld dissertation\n                      2, # Lambert 2022\n                      4, # Lepp 2022\n                      2, # Mahalingham 2023\n                      3, # Mitev 2021\n                      4, # Ozimek 2020\n                      3, # Przybylski 2021\n                      4, # Sagioglou 2014 study 2\"\n                      4, # Tartaglia\n                      1, # Thai 2021\n                      1, # Thai 2023\n                      2, # Tromholt 2016\n                      2, # Valley2019\n                      2, # van wezel 2021\n                      3, # Vanman 2018\n                      4, # Ward 2017\n                      4  # Yuen et al., 2019\n                      ))\n\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\",\"subgroup\")])\n\n# A tibble: 6 × 5\n  Citation                    n      d       v subgroup\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241        1\n2 Brailovskaia 2020         286  0.154 0.0141         1\n3 Brailovskaia 2022         322  0     0.0125         1\n4 Collins and Edgers 2022   121 -0.138 0.0337         1\n5 Deters & Mehl 2013         86 -0.207 0.0479         4\n6 Faulhaber et al., 2023    230  0.484 0.0181         1\n\n\nNow that we have the subgroup column, we can conduct the sub-group analysis. Here comes the next problem, R&H decide to simply calculates the arithmetic (unweighted) mean of the observed effect sizes within each group. The problem is that they aren’t capitalizing on the fact that different studies have different levels of precision. The only way it would make sense to compute an unweighted average with respect to the statistical model we are employing is if sampling error didn’t exist (it does) or heterogeneity is infinite (its not), so either way its just the wrong approach.\nLet’s first reproduce R&H’s results by taking the unweighted average of observed effect sizes.\n\nresults &lt;- summarize(.data = dat,\n                     mean_d = mean(d),\n                     k_studies = n(),\n                     .by = subgroup)\n\n# reorder the subgroup display\nresults[c(1,4,3,2),]\n\n# A tibble: 4 × 3\n  subgroup  mean_d k_studies\n     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;\n1        1  0.204         10\n2        2  0.0868         6\n3        3 -0.172          4\n4        4  0.0566         7\n\n\nAll the values are accurate. We can compare it to the table shown in R&H’s article:\n\nOkay we see a positive average effect for the multi-week reduction experiments, but lets now use proper inverse-variance weights to calculate the average. This time we will model the groups as dummy predictors and specify the proper contrasts for each group:\n\n# construct a meta-analytic model with subgroup\n# as a dummy-coded categorical predictor\nmdl &lt;- rma(d ~ factor(subgroup,levels = c(1,2,3,4)), \n           vi = v, \n           data = dat,\n           test=\"knha\")\n\n# acquire subgroup-level mean estimates\nestimates &lt;- as.data.frame(\n  predict(mdl, \n          rbind(g1 = c(0,0,0),\n                g2 = c(1,0,0),\n                g3 = c(0,1,0),\n                g4 = c(0,0,1))))\n\n# print results\nround(data.frame(group = 1:4, mean_delta = estimates$pred),3)\n\n  group mean_delta\n1     1      0.177\n2     2      0.154\n3     3     -0.172\n4     4      0.082\n\n\nAs we can see the estimates are a bit different. Another thing that is completely missing from the R&H analysis is any indication of variability in their mean estimates and their is no indication of heterogeneity. It is conveyed as a fixed quantity, however just because we combine many studies together does not mean that uncertainty ceases to exist.\n\nround(data.frame(group = 1:4, \n                 mean_delta = estimates$pred,\n                 CI_low = estimates$ci.lb,\n                 CI_high = estimates$ci.ub,\n                 PI_low = estimates$pi.lb,\n                 PI_high = estimates$pi.ub),3)\n\n  group mean_delta CI_low CI_high PI_low PI_high\n1     1      0.177 -0.005   0.360 -0.296   0.651\n2     2      0.154 -0.098   0.407 -0.350   0.659\n3     3     -0.172 -0.443   0.099 -0.686   0.342\n4     4      0.082 -0.151   0.315 -0.414   0.577\n\n\nTrue effects for group 1 (multi-week reduction studies, i.e., the focal group) are quite variable and can range from -0.296 to 0.651. An interesting point to bring up is that Ferguson (2024) set the smallest effect size of interest (SESOI) to be d = .21 in his pre-registration (equivalent to a Pearson correlation of r = .10). However, that does not help us much here since the upper bound of the confidence interval is well above d = .21 so you can’t really make the claim that that there is no clinically meaningful effect, on average. And you definitely can’t make the claim that all/most true effects are not clinically meaningful since the upper bound of the PI is 0.651. Truthfully, the SESOI established in Ferguson (2024) does not seem to have any good theoretical or practical justification outside of “seems reasonable” so it is unclear what the utility of this arbitrary boundary is in this case?"
  },
  {
    "objectID": "blog-posts/blog-post-6.html#properly-modeling-the-length-of-intervention-in-reduction-studies",
    "href": "blog-posts/blog-post-6.html#properly-modeling-the-length-of-intervention-in-reduction-studies",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Properly modeling the length of intervention in reduction studies",
    "text": "Properly modeling the length of intervention in reduction studies\nInstead of binning the length of the intervention into groups I will use the weeks reported in the R&H article to create a continuous moderator for reduction studies only.\n\ndat &lt;- dat %&gt;%\n  mutate(length_weeks = c(4, # Alcott 2020\n                          2, # Brailovskaia 2020\n                          2, # Brailovskaia 2022\n                          10, # Collins and Edgers 2022\n                          NA, # Deters & Mehl 2013\n                          2, # Faulhaber et al., 2023\n                          1/7, # Gajdics 2022\n                          2.5, # Hall et al. 2021\n                          1, # Hunt 2018\n                          1, # Hunt 2021\n                          1, # Kleefeld dissertation\n                          1, # Lambert 2022\n                          NA, # Lepp 2022\n                          1, # Mahalingham 2023\n                          1/7, # Mitev 2021\n                          NA, # Ozimek 2020\n                          1/7, # Przybylski 2021\n                          NA, # Sagioglou 2014 study 2\"\n                          NA, # Tartaglia\n                          3, # Thai 2021\n                          3, # Thai 2023\n                          1, # Tromholt 2016\n                          1, # Valley2019\n                          1, # van wezel 2021\n                          5/7, # Vanman 2018\n                          NA, # Ward 2017\n                          NA  # Yuen et al., 2019\n  ))\n\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\",\"length_weeks\")])\n\n# A tibble: 6 × 5\n  Citation                    n      d       v length_weeks\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241            4\n2 Brailovskaia 2020         286  0.154 0.0141             2\n3 Brailovskaia 2022         322  0     0.0125             2\n4 Collins and Edgers 2022   121 -0.138 0.0337            10\n5 Deters & Mehl 2013         86 -0.207 0.0479            NA\n6 Faulhaber et al., 2023    230  0.484 0.0181             2\n\n\nOkay so now we have the moderator we can construct a linear meta-regression model of the form,\n\\[\nd_i = b_0 + b_1 X_\\mathrm{length} + u_i + \\varepsilon_i\n\\] Notice now that true effects how is distributed around the regression line such that the distribution of true effects is now\n\\[\n\\delta_i \\sim \\mathcal{N}(b_0 + b_1 X_\\mathrm{length},\\tau^2)\n\\]\n\nmdl_weeks &lt;- rma(d ~ length_weeks,\n                 vi = v,\n                 data = dat,\n                 method = \"REML\",\n                 test = \"knha\")\n\nWarning: 7 studies with NAs omitted from model fitting.\n\nmdl_weeks\n\n\nMixed-Effects Model (k = 20; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0647 (SE = 0.0299)\ntau (square root of estimated tau^2 value):             0.2544\nI^2 (residual heterogeneity / unaccounted variability): 79.40%\nH^2 (unaccounted variability / sampling variability):   4.85\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 18) = 75.7503, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 18) = 0.0018, p-val = 0.9667\n\nModel Results:\n\n              estimate      se    tval  df    pval    ci.lb   ci.ub    \nintrcpt         0.0884  0.0922  0.9581  18  0.3507  -0.1054  0.2821    \nlength_weeks    0.0014  0.0330  0.0423  18  0.9667  -0.0678  0.0706    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOkay so as it turns out there appears to be little/no effect of the length of the intervention on the effect size. Let us plot out the data to see what the relationship actually looks like:\n\n\n\n\n\n\n\n\n\nThe regression line looks almost parallel to the d = 0 line (the grey line). You may notice the outlier at 10 weeks and you may want to remove it from the analysis. I would ask why? The data point does not appear to be a coding error or a mistake? It is a valid data point and piece of evidence that should not be taken out of the analysis post-hoc. Hell, it could be that the the positive effect of not being on social media only lasts a few weeks and then fades out (not an uncommon trend in psychology interventions). Anyways, any reason to remove that “outlier” would seem to me like a post-hoc rationalization."
  },
  {
    "objectID": "blog-posts/blog-post-6.html#discussion",
    "href": "blog-posts/blog-post-6.html#discussion",
    "title": "A Statistical Interrogation of “The Case for Causality, Part 1” by Rausch and Haidt",
    "section": "Discussion",
    "text": "Discussion\nThe re-analysis of Ferguson (2024)’s meta-analysis by R&H does not have adequate statistical rigor to build a “case for causality”. Post-hoc subgroup analyses conducted by R&H did not use a principled statistical model, they did not report any variability in their estimates, the estimates themselves were sub-optimal (unweighted averaging), and they misinterpreted the point-estimates that they calculated. They did not do a proper comparison of their point-estimates and instead they treated them as fixed quantities and simply claimed that these average effect sizes are different without consideration of variability in their estimation procedure. Their conclusion claimed that:\n\nwe still found that his data supports (rather than undermines) the contention by some scholars that “reductions in social media time would improve adolescent mental health,” at least as long as the reductions continue for two weeks or longer.\n\nBased on my re-analysis of R&H’s re-analysis I find that the intervention length when modeled properly as a continuous variable (since it literally is) does not support this claim.\nIf I did anything incorrectly or if I did not use best practices, I take full responsibility and you can let me know by sending me a DM on twitter or emailing me at matthewbjane@gmail.com."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#some-useful-definitions",
    "href": "blog-posts/blog-post-5.html#some-useful-definitions",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Some useful definitions",
    "text": "Some useful definitions\nLet us first note that when I refer to a measurand I simply mean the (psychological) attribute that we intend to measure.\nA realist defines the measurand outside of the measurement procedure, that is, the measurand is some test-independent attribute that is estimated by scores produced by the test.\nAn operationalist would instead suppose that the measurand is defined by the test itself and can not be defined outside of the test (i.e., the measurand is test-dependent). Note that a strict operationalist in the form that I discuss in this blog post is a sort of extreme operationalist. In the measurement error modeling section, I also describe a laxed operationalist, but it is just a useful descriptor for contrasting two types of operationalist measurement models.\nA respectful operationalist is an operationalist that defines the measurand in terms of the test, but the test is validated for extra-operational meanings and usability (i.e., meanings and usability beyond the test, Vessonen 2021). So the respectful operationalist is kind of like “sure, yeah we can define the measurand in terms of the test, but the test needs to incorporate common connotations of the target concept and produce usable scores”."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#what-might-validity-look-like-in-each-of-these-frameworks",
    "href": "blog-posts/blog-post-5.html#what-might-validity-look-like-in-each-of-these-frameworks",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "What might validity look like in each of these frameworks",
    "text": "What might validity look like in each of these frameworks\nIn realist framework, validity may fall under the principles set by Borsboom, Mellenbergh, and Van Heerden (2004), where the psychological attribute (or construct) has to both 1) exist and 2) cause variation in the measurement procedure. This view of validity is centered around construct validity.\nA (strict) operationalist need not validate a measure, even if the measure is nonsensical, the measurand is still defined by the test.\nA respectful operationalist can validate a measure using a similar approach to Michael T. Kane’s argument-based approach to validity in which he guides researchers to specify and justify the intended use of the measure. Specifically, Vessonen (2021) proposes that a test is valid if the test incorporates common extra-operational connotations of the target concept. This will be more closely related to test-related types of validity such as content and criterion validity."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#operationalist-and-realist-measurement-error-models",
    "href": "blog-posts/blog-post-5.html#operationalist-and-realist-measurement-error-models",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Operationalist and Realist Measurement Error Models",
    "text": "Operationalist and Realist Measurement Error Models\nOkay let’s start at the foundation: we observe a test score \\(X_p\\) for a person, \\(p\\).\nA strict operationalist may simply define a person’s true score \\(T_p\\) as the observed score such that, \\(T_p = X_p\\). Trivially, the observed score can be modeled as the true score,\n\\[\nX_p = T_p\n\\tag{1}\\]\nThere is something attractive and so practical about this model, however there are immediate problems of usability. What if we test that person again and they obtain a different score? What if the scoring is done by a rater and a different rater assigns a different score to that person? To answer this, let us use an example where a person’s true score is defined as the observed score \\(X_{par}\\), for person \\(p\\), test administration \\(a\\), and rater \\(r\\) such that,\n\\[\nT_p = X_{par}\n\\]\nHowever the problem now is that if the observed score varies across test-administrations and/or raters then the true score will not be generalizable across test-administrations or raters,\n\\[\nT_p = X_{par} \\neq X_{pa(r+1)} \\neq X_{p(a+1)r}\n\\]\nThis will force us to define a true score for not only each person, but also each test administration and rater such that,\n\\[\nT_{par} = X_{par}.\n\\]\nHowever differences in observed scores between test-administration and raters may not be of scientific interest if we only really care about person-level differences. This specific operationalist approach is simply unusable in practice. Although, it is interesting to note, that the reliability of observed scores is necessarily perfect and can be shown by the reliability index, \\(\\rho_{XT}=1\\) (i.e., the correlation between true and observed scores).\nA more laxed operationalist definition of true scores would be to define it based on the expectation of observed scores conditional on a person. Therefore a definition of a true score for a given person can be expressed by,\n\\[\nT_p = \\mathbb{E}[X_{par}\\mid p]\n\\]\nConditional expectation (\\(\\mathbb{E}[\\cdot]\\)) is the average observed score over all possible observed scores for a given person. This also provides the beautiful property that \\(X_{p}\\) for a given person, is calibrated to \\(T_p\\), since,\n\\[\nT_p -  \\mathbb{E}[X_p\\mid p] = T_p - T_p =  0.\n\\]\nSo now, even if observed scores vary across raters and/or test administrations, the true score will remain constant. The question at this point is how can we model a single instance of an observed score. Here, we will need to introduce measurement errors. Measurement errors can be defined as differences between observed scores and true scores. Remember that in our strict operationalist model (Equation 1) we did not have any measurement errors since the true score was equivalent to the observed score (i.e., if \\(T_p = X_p\\), then \\(T_p - X_p = 0\\)). Now that the true score is the conditional expectation of the observed score, a single instance of an observed score can differ from the true score. We can thus add a new term to the model described in Equation 1,\n\\[\nX_{p} = T_p + E_{p},\n\\tag{2}\\]\nwhere \\(E_{p}\\) indicates the error in the observed score for a given person and measurement. Since we started with the definition of true scores being \\(T_p = \\mathbb{E}[X \\mid  p]\\), an extremely useful consequence of this is that the conditional expectation of measurement errors within a person is zero. We can demonstrate why this is the case with a short derivation (first re-arranging Equation 2 so that \\(E_{p}\\) is on the left hand side),\n\\[\\begin{align}\n\\mathbb{E}[E_{p}\\mid p] &= \\mathbb{E}[T_p -  X_p\\mid p] \\\\[.3em]\n&= \\mathbb{E}[T_p\\mid p] -  \\mathbb{E}[X_p\\mid p] \\\\[.3em]\n&= T_p -  T_p \\\\[.3em]\n&= 0 \\\\\n\\end{align}\\]\nErrors that balance out over all possible observed scores for a given person is what distinguishes the classical test theory model from other measurement models (Kroc and Zumbo 2020). It is important to note that the algebraic formulation of \\(X=T+E\\) is not what defines the classical test theory model, in fact, many measurement error models come in an analogous form. It is the conditional expectations between the components of the model which distinguish classical errors from other measurement error models (e.g., Berkson errors, see Kroc and Zumbo 2020).\nA realist model may suppose that the true score is a construct score that is an objective value that is defined outside of the measurement operations (the actual value of the construct/attribute being measured; Borsboom and Mellenbergh (2002)). Therefore we can not define true scores in terms of observed scores without knowing how observed scores are calibrated true scores. The realist model for an observed score is superficially identical to Equation 2,\n\\[\nX_{p} = T_p + E_{p},\n\\tag{3}\\]\nwith the major difference being that \\(\\mathbb{E}[X|p] \\neq T_p\\). Note that Equation 3 is not the classical test theory model because it does not, by default, meet the assumptions of classical test theory that the laxed operationalist model does (Equation 2). The big difference between this realist model the laxed operationalist model is that the conditional expectation of errors is no longer zero in the realist approach, and thus systematic errors (i.e., biased estimates of true scores) can exist. In order to recover proper calibration of the observed scores to true scores, we would have to just assume that \\(\\mathbb{E}[E_p\\mid p]=0\\) (see flaws below).\nWhen it comes to psychological constructs the realist approach has two major flaws to me.\n\nNecessitates the existence of a construct score that is independent of the measure. Without sufficient evidence of a construct existing, we are adding additional complexity to our theory. Therefore it adds increased complexity into our theory.\nEven if the construct is real and has a true construct score, we have no idea how the observed scores are calibrated to those true scores. The operationalist model produces a calibrated measure by definition whereas the realist model needs to add an additional assumption (i.e., conditioned on a given person, the expectation of measurement errors is zero) in order for the observed scores to be calibrated.\n\nThe operationalist model may have the biggest flaw so far:\n\nSince the measurand is defined by the measure itself, you can not really draw any meaningful inferences about anything outside of the measure.\n\nThere is a third option though!"
  },
  {
    "objectID": "blog-posts/blog-post-5.html#respectful-operationalist-measurement-error-model",
    "href": "blog-posts/blog-post-5.html#respectful-operationalist-measurement-error-model",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "Respectful Operationalist Measurement Error Model",
    "text": "Respectful Operationalist Measurement Error Model\nAs put by the originator of Respectful Operationalism Elina Vessonen (2021) states,\n\n[Respectful Operationalism] is the view that we may define a target concept in terms of a test, as long as that test is validated to incorporate common connotations of the target concept and the usability of the measure\n\nTherefore we need to identify which observed scores are produced by tests that have been validated for extra-operational connotations for the concept of interest. We generally have inter-subjective agreement about the meanings of concepts like intelligence, anxiety, and depression mean outside of the measurement instrument. We can imagine that items contain content that are more or less related to those inter-subjective meanings. The extent to which the item content is relevant to the concept of interest reflects the item’s content validity. The true score can be defined as the conditional expectation of observed scores obtained from tests containing items that are sufficiently valid (this could be done from inter-rater assessment of the conceptual relatedness of items and concepts) can define the true scores. that contain sufficient validity. Let’s define an observed score produced by a sufficiently valid test as \\(X_p^\\star\\). As an example, a measure of depression that produces observed scores from responses to the question, “what is your age?”, would not encompass common connotations of depression and therefore those observed scores would not in the set of valid observed scores. This new model can be defined as,\n\\[\nX^\\star_p = T_p + E_p\n\\]\nwhere\n\\[\nT_p = \\mathbb{E}[X^\\star_p\\mid p]\n\\]\nIn this way, inferences about concepts can be made from observed scores since they hold inherent relevance by virtue of prior validation of the test’s extra-operational meaning."
  },
  {
    "objectID": "blog-posts/blog-post-5.html#my-current-stance",
    "href": "blog-posts/blog-post-5.html#my-current-stance",
    "title": "Respectful Operationalism, Realism, and Models of Measurement Error in Psychometrics",
    "section": "My current stance",
    "text": "My current stance\nRespectful operationalism appears to offer the practical advantages of operationalism such as calibrated measurement outcomes, without the limitation of being unable to draw meaningful inferences beyond the test. To consider myself a realist for a particular attribute/construct, then I would have to be sufficiently convinced of the tenants of construct validity described by Borsboom, Mellenbergh, and Van Heerden (2004): (1) the attribute exists (2) the construct causes variation in outcomes of the measurement procedure. Currently, I am not sufficiently convinced this is the case this is the case for most (if not all) psychological constructs; therefore, I am not comfortable making the commitments necessary to take on a realist position. Therefore, to me, a know-nothin grad student, Respectful Operationalism seems to best align with my current worldview.\nFurther reading, see these papers by Elina Vessonen (2019, 2021)."
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "href": "blog-posts/blog-post-1.html#step-1-obtain-pre-test-post-test-and-change-score-standard-deviations",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Step 1: Obtain Pre-test, Post-test, and Change score standard deviations",
    "text": "Step 1: Obtain Pre-test, Post-test, and Change score standard deviations\nIn order to calculate the pre/post correlation (\\(r\\)), we need an estimate of the standard deviation (SD) of pre-test scores (\\(S_{pre}\\)), the SD of post-test scores (\\(S_{post}\\)), and the SD of change scores (\\(S_{change}\\), where \\(x_{change}=x_{post}-x_{pre}\\)). If the post-test SD is unavailable, but the pre-test SD is available, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis,\n\\[\n\\bar{S}_{ratio}=\\frac{1}{k}\\sum_{i=1}^k \\frac{S_{post,i}}{S_{pre,i}}\n\\] Then we can make an approximation of the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nS_{post}\\approx \\bar{S}_{ratio}\\times S_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal.\n\n# Define standard deviations\nS_pre &lt;- 9\nS_post &lt;- 11\nS_change &lt;- 8"
  },
  {
    "objectID": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-1.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nThe correlation between pre-test and post-test scores (\\(r\\)) can be calculated by re-arranging the equation for change score SD:\n\\[\nS_{change} = \\sqrt{S^2_{pre} + S^2_{post} - 2rS_{pre}S_{post}}\n\\] Then we can solve for \\(r\\),\n\\[\nr = \\frac{S^2_{pre} + S^2_{post} - S^2_{change}}{2S_{pre}S_{post}}\n\\] Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.697\""
  },
  {
    "objectID": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-1.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from the Standard Deviation of Change Scores",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. It can be seen that the correlation calculated from the formulas above is perfectly precise.\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nS_pre &lt;- 9\nS_post &lt;- 11\nr_true &lt;- .70\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=200,\n               mu=c(0,0),\n               Sigma = data.frame(x=c(S_pre^2,r_true*S_pre*S_post),\n                                  y=c(r_true*S_pre*S_post,S_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations\nS_pre &lt;- sd(x_pre)\nS_post &lt;- sd(x_post)\nS_change &lt;- sd(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (S_pre^2 + S_post^2 - S_change^2) / (2*S_pre*S_post)\n\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "blog-posts/blog-post-2.html#note-the-assumptions",
    "href": "blog-posts/blog-post-2.html#note-the-assumptions",
    "title": "Approximating Standard Deviation from Inter-Quantile Range",
    "section": "Note the Assumptions!",
    "text": "Note the Assumptions!\nSince this method provides an approximation of a normal distribution it is important to point out that the method may be biased under different distributions. For example, if we try the same method on a Student’s t distribution with heavy tails (3.5 degrees of freedom), \\(S_{(75-25)}\\) would be equal to 1.50187 which is slightly larger than 1.34898 that we computed from the normal distribution. If we were to assume a normal distribution when it was truly a student’s t, the method would over-estimate the standard deviation by ~11%.\n\n\n\n\n\n\n\n\n\n\nApplying it to simulated data\nLets see how it performs in simulated sample of normally distributed data… Close enough!!\n\n# Set seed\nset.seed(343)\n\n# Simulate normal data (Mean = 10, SD = 5)\nX = rnorm(100,10,5)\n\n# Define IQR\nq25 &lt;- as.numeric(quantile(X,.25))\nq75 &lt;- as.numeric(quantile(X,.75))\nX_75_25 &lt;- q75 - q25\n\n# Compute IQR in SD units\nS_75_25 &lt;- qnorm(.75) - qnorm(.25)\n\n# Estimate standard deviation\nS_X &lt;-  X_75_25 / S_75_25\n\n# Print results\nprint(S_X)\n\n[1] 4.706901"
  },
  {
    "objectID": "blog-posts/blog-post-8.html#what-is-meta-regression",
    "href": "blog-posts/blog-post-8.html#what-is-meta-regression",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "What is Meta-regression?",
    "text": "What is Meta-regression?\nMeta-regression is a modeling approach used in meta-analysis to see how effect sizes vary across studies. For example, let’s say we want to know the effect of some drug on disease remission, but the effect sizes are highly heterogenous (studies use different methods, sample from different populations, etc. and thus have different estimands). We may want to know what aspects of these studies cause differences in their resulting effects? If different studies use a different doseage amount then we may hypothesize that dose will account for some of that heterogeneity in effects. Doseage is what we would call a moderator of the effect size (moderator is analogous to a predictor in a traditional regression). Meta-regression is like any regression model with the only difference that the data points represent entire studies and that studies are weighted such that more precise studies (i.e., larger sample sizes/smaller standard errors) are preferentially weighted. The regression line will estimate the conditional mean effect size.\nLet’s visualize this example by running a meta-regression and creating a bubble plot (a scatter plot, but the data points are sized relative to the precision of the study). The effect size is the log risk ratio between treatment and control groups,\n\\[\n\\ln RR = \\ln \\frac{p_T}{p_C},\n\\] where \\(p_T\\) and \\(p_C\\) are the proportions of individuals in remission within the treatment and control group, respectively. We will then see how the doseage moderates the \\(\\ln RR\\) across studies. In R, we can use the metafor (Viechtbauer 2010) package to fit a meta-regression.\n\n\nShow the code\nlibrary(metafor)\n\ndat &lt;- escalc(measure = \"RR\",\n              # data set loaded in from metafor package\n              data = dat.viechtbauer2021,\n              # treatment/control number of people in remission\n              ai = xTi, ci = xCi,\n              # treatment/control group sample size in\n              n1 = nTi, n2 = nCi,\n              # label effect size and sampling variance (se^2)\n              var.names = c(\"lnRR\", \"v\"))\n\n# fit model\nmdl &lt;- rma(lnRR ~ dose, \n           vi = v,\n           data = dat)\n\n# plot model\nregplot(mdl,refline = 0)\n\n\n\n\n\n\n\n\n\nWe see a very strong relationship between the effect size and the dosage amount."
  },
  {
    "objectID": "blog-posts/blog-post-8.html#what-would-a-meta-analytic-gam-look-like",
    "href": "blog-posts/blog-post-8.html#what-would-a-meta-analytic-gam-look-like",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "What would a meta-analytic GAM look like?",
    "text": "What would a meta-analytic GAM look like?\nThis brief intro section is a sort of a meta-analytic adaptation of a section of Michael Clark’s online text “Generalized Additive Models”.\nIn our example, we used a linear meta-regression so let us define a simple linear meta-regression model mathematically. An observed (arbitrary) effect size \\(d_i\\) from study \\(i\\) is expressed as,\n\\[\nd_i = \\underset{_\\textrm{intercept}}{b_0} + \\overset{^\\textrm{moderator effect}}{b_1M_i} + \\underset{_\\textrm{true effect variability}}{u_i} + \\overset{^\\textrm{sampling error}}{\\varepsilon_i}.\n\\] The first three terms in the equation is what defines the true effect size \\(\\delta_i\\) for study \\(i\\) and the last term denotes sampling error which is what distinguishes the observed effect size \\(d_i\\) from the true effect size \\(\\delta_i\\). The true effect size is conditionally distributed as,\n\\[\n\\delta_i \\mid M_i \\sim \\mathcal{N}\\left(b_0 + b_1M_i\\, ,\\, \\tau^2\\right)\n\\] Where \\(\\tau^2\\) is the residual variance in true effects (i.e., \\(\\tau^2 = \\mathrm{var}(u_i)\\)). We can see that the conditional expectation of true effects is a linear function of the moderator variable \\(\\mathbb{E}[\\delta_i \\mid M] = b_0 + b_1M_i\\). But we don’t have to model the conditional mean as linear. If the data appears non-linear we can instead model it as some smooth function \\(f\\) such that the conditional mean has some non-linear functional form,\n\\[\n\\delta_i \\mid M_i \\sim \\mathcal{N}\\left(f(M_i),\\, \\tau^2\\right).\n\\]\nFor GAMs we elect to choose a space of functions (e.g., cubic splines) called a basis. A function within the function space can be expressed as a linear combination of basis functions. Essentially, we want to estimate this “big” function with with a series of known “smaller” functions. For our case, the function \\(f(M_i)\\) can be expressed as, \\(f(M_i) = \\sum^m_{q=1} b_q\\xi_q(M_i)\\), where \\(\\xi_q\\) are basis functions. Therefore the conditional distribution of true effects is now given by,\n\\[\n\\delta_i \\mid M_i \\sim \\mathcal{N}\\left(\\sum^m_{q=1} b_q\\xi_q(M_i),\\, \\tau^2\\right).\n\\]\nThe conditional expectation of true effects now can have a lot of flexibility. Note that we do not want to try to interpret the regression coefficients \\(b_q\\) of those smaller basis functions, instead we want to interpret the bigger function which is estimating the conditional mean of true effects. To estimate the conditional expectation, GAMs utilize penalized splines which penalizes the function for being too “wiggly” so as to maintain a parsimonious model. The number of knots (i.e., the joints/abscissa of the “big” function) also does not have to be set manually like they do in other spline models."
  },
  {
    "objectID": "blog-posts/blog-post-8.html#loading-in-packages",
    "href": "blog-posts/blog-post-8.html#loading-in-packages",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Loading in packages",
    "text": "Loading in packages\nWe will need the following packages before we begin.\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(mgcv)\nlibrary(tidybayes)\nlibrary(modelr)\nlibrary(ggdist)"
  },
  {
    "objectID": "blog-posts/blog-post-8.html#non-linear-data",
    "href": "blog-posts/blog-post-8.html#non-linear-data",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Non-linear Data",
    "text": "Non-linear Data\nWe can use some made up non-linear data created by Wolfgang Viechtbauer in his tutorial on spline meta-regression.\n\ndat &lt;- structure(list(\n  # effect size data\n  yi = c(0.99, 0.54, -0.01, 1.29, 0.66, -0.12, 1.18,-0.23, 0.03, 0.73, 1.27, 0.38, -0.19, 0.01, 0.31, 1.41, 1.32, 1.22, 1.24,-0.05, 1.17, -0.17, 1.29, -0.07, 0.04, 1.03, -0.16, 1.25, 0.27, 0.27, 0.07,0.02, 0.7, 1.64, 1.66, 1.4, 0.76, 0.8, 1.91, 1.27, 0.62, -0.29, 0.17, 1.05,-0.34, -0.21, 1.24, 0.2, 0.07, 0.21, 0.95, 1.71, -0.11, 0.17, 0.24, 0.78,1.04, 0.2, 0.93, 1, 0.77, 0.47, 1.04, 0.22, 1.42, 1.24, 0.15, -0.53, 0.73,0.98, 1.43, 0.35, 0.64, -0.09, 1.06, 0.36, 0.65, 1.05, 0.97, 1.28), \n  # standard error\n  sei = sqrt(c(0.018, 0.042, 0.031, 0.022, 0.016, 0.013, 0.066, 0.043, 0.092, 0.009,0.018, 0.034, 0.005, 0.005, 0.015, 0.155, 0.004, 0.124, 0.048, 0.006, 0.134,0.022, 0.004, 0.043, 0.071, 0.01, 0.006, 0.128, 0.1, 0.156, 0.058, 0.044,0.098, 0.154, 0.117, 0.013, 0.055, 0.034, 0.152, 0.022, 0.134, 0.038, 0.119,0.145, 0.037, 0.123, 0.124, 0.081, 0.005, 0.026, 0.018, 0.039, 0.062, 0.012,0.132, 0.02, 0.138, 0.065, 0.005, 0.013, 0.101, 0.051, 0.011, 0.018, 0.012,0.059, 0.111, 0.073, 0.047, 0.01, 0.007, 0.055, 0.019, 0.104, 0.056, 0.006,0.094, 0.009, 0.008, 0.02 )), \n  # moderator values\n  xi = c(9.4, 6.3, 1.9, 14.5, 8.4, 1.8, 11.3,4.8, 0.7, 8.5, 15, 11.5, 4.5, 4.3, 4.3, 14.7, 11.4, 13.4, 11.5, 0.1, 12.3,1.6, 14.6, 5.4, 2.8, 8.5, 2.9, 10.1, 0.2, 6.1, 4, 5.1, 12.4, 10.1, 13.3,12.4, 7.6, 12.6, 12, 15.5, 4.9, 0.2, 6.4, 9.4, 1.7, 0.5, 8.4, 0.3, 4.3, 1.7,15.2, 13.5, 6.4, 3.8, 8.2, 11.3, 11.9, 7.1, 9, 9.9, 7.8, 5.5, 9.9, 2.6,15.5, 15.3, 0.2, 3.2, 10.1, 15, 10.3, 0, 8.8, 3.6, 15, 6.1, 3.4, 10.2, 10.1,13.7)), \n  class = \"data.frame\", row.names = c(NA, -80L))\n\nhead(dat)\n\n     yi       sei   xi\n1  0.99 0.1341641  9.4\n2  0.54 0.2049390  6.3\n3 -0.01 0.1760682  1.9\n4  1.29 0.1483240 14.5\n5  0.66 0.1264911  8.4\n6 -0.12 0.1140175  1.8\n\n\nThe effect size is denoted with yi, the standard errors are denoted with sei, and the study-level moderator values is denoted with xi."
  },
  {
    "objectID": "blog-posts/blog-post-8.html#constructing-the-brms-model",
    "href": "blog-posts/blog-post-8.html#constructing-the-brms-model",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Constructing the brms model",
    "text": "Constructing the brms model\nUsing the brm function in the brms (Bürkner 2021) package we can construct the meta-regression GAM model by utilizing the se() argument to input the standard errors of the effect sizes. Then s() function to tell brms that we are fitting a GAM. The sigma=TRUE function allows us to have heterogeneity, that is, the model includes a \\(\\sigma\\) parameter which represents the standard deviation in true effects which we previously defined as \\(\\tau\\).\n\nmdl_gam &lt;- brm(yi | se(sei, sigma = TRUE) ~ s(xi),\n          data = dat, \n          family = gaussian(),\n          seed = 17,\n          iter = 1000, \n          warmup = 150, \n          chains = 2,\n          file = \"model_gam\",\n          file_refit = \"on_change\")\n\nNow let’s also construct a linear model so that we can compare the two fits.\n\nmdl_lm &lt;- brm(yi | se(sei, sigma = TRUE) ~ xi,\n          data = dat, \n          family = gaussian(),\n          seed = 17,\n          iter = 1000, \n          warmup = 150, \n          chains = 2,\n          file = \"model_lm\",\n          file_refit = \"on_change\")\n\nWe can plot out the models by displaying the estimates of the conditional effect size mean that are compatible with the data (denoted with the dark lines) and the conditional distribution of effect sizes (the density band with the bounds denoting the 95% prediction interval).\n\n\nShow the code\n# Extract conditional mean estimates for GAM\ngam_conditional_means &lt;- dat %&gt;%\n  data_grid(xi = seq_range(xi, n = 101), sei = sei) %&gt;%\n  add_epred_draws(mdl_gam, ndraws = 200)\n\n# Extract true effect size predictions to construct true effect distribution for GAM\ngam_predictions &lt;- dat %&gt;%\n  data_grid(xi = seq_range(xi, n = 71), sei = sei) %&gt;%\n  add_predicted_draws(mdl_gam, ndraws = 400)\n\n\np1 &lt;- ggplot() +\n  stat_lineribbon(data = gam_predictions, \n            aes(x = xi, y = .prediction, fill_ramp = after_stat(.width)),\n            .width = seq(.05,.95,by=.05), fill = \"#ED5C9B\") +\n  scale_fill_ramp_continuous(range = c(1, .1), guide = guide_rampbar(to = \"#ED5C9B\")) +\n  geom_line(data = gam_conditional_means, \n            aes(x = xi, y = .epred, group = .draw),color = \"#861d5e\", alpha = .15) +\n  geom_point(data = dat, aes(x = xi, y = yi, size = 1/sei^2), alpha = .8, color = \"#601543\") +\n  theme_ggdist(base_size = 15) + \n  geom_hline(yintercept = 0, linetype= \"dashed\", color = \"#ED5C9B\",alpha=.4) +\n  theme(legend.position = \"none\",\n        aspect.ratio = .8) +\n  ylab(\"Effect Size\") +\n  xlab(\"Moderator Value\") + \n  ggtitle(\"GAM\")\n\n# Extract conditional mean estimates for GAM\nlm_conditional_means &lt;- dat %&gt;%\n  data_grid(xi = seq_range(xi, n = 101), sei = sei) %&gt;%\n  add_epred_draws(mdl_lm, ndraws = 200)\n\n# Extract true effect size predictions to construct true effect distribution for GAM\nlm_predictions &lt;- dat %&gt;%\n  data_grid(xi = seq_range(xi, n = 51), sei = sei) %&gt;%\n  add_predicted_draws(mdl_lm, ndraws = 200)\n\n\np2 &lt;- ggplot() +\n  stat_ribbon(data = lm_predictions, \n            aes(x = xi, y = .prediction, fill_ramp = after_stat(.width)),\n            .width = seq(.05,.95,by=.05), fill = \"#ED5C9B\") +\n  scale_fill_ramp_continuous(range = c(1, .1), guide = guide_rampbar(to = \"#ED5C9B\")) +\n  geom_line(data = lm_conditional_means, \n            aes(x = xi, y = .epred, group = .draw),color = \"#861d5e\", alpha = .15) +\n  geom_point(data = dat, aes(x = xi, y = yi, size = 1/sei^2), alpha = .8, color = \"#601543\") +\n  theme_ggdist(base_size = 15) + \n  geom_hline(yintercept = 0, linetype= \"dashed\", color = \"#ED5C9B\",alpha=.4) +\n  theme(legend.position = \"none\",\n        aspect.ratio = .8) +\n  ylab(\"Effect Size\") +\n  xlab(\"Moderator Value\") + \n  ggtitle(\"GAM\")\n\n\ncowplot::plot_grid(p1,p2,ncol = 1)"
  },
  {
    "objectID": "blog-posts/blog-post-8.html#predictive-check",
    "href": "blog-posts/blog-post-8.html#predictive-check",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Predictive Check",
    "text": "Predictive Check\nThe predictive check of the GAM model captures the data better than the linear model in this data set.\n\npp1 &lt;- pp_check(mdl_gam, ndraws = 100) +\n  theme(aspect.ratio = 1, legend.position = \"none\") +\n  ggtitle(\"GAM\")\npp2 &lt;- pp_check(mdl_lm, ndraws = 100) +\n  theme(aspect.ratio = 1, legend.position = \"none\") +\n  ggtitle(\"Linear Model\")\n\n\ncowplot::plot_grid(pp1,pp2)"
  },
  {
    "objectID": "blog-posts/blog-post-8.html#session-info",
    "href": "blog-posts/blog-post-8.html#session-info",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ggdist_3.3.2        modelr_0.1.11       tidybayes_3.0.7    \n [4] mgcv_1.9-1          nlme_3.1-164        brms_2.22.0        \n [7] Rcpp_1.0.12         lubridate_1.9.3     forcats_1.0.0      \n[10] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[13] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[16] ggplot2_3.5.1       tidyverse_2.0.0     metafor_4.6-0      \n[19] numDeriv_2016.8-1.1 metadat_1.2-0       Matrix_1.7-0       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     svUnit_1.0.6         viridisLite_0.4.2   \n [4] farver_2.1.2         loo_2.8.0            fastmap_1.2.0       \n [7] tensorA_0.36.2.1     mathjaxr_1.6-0       digest_0.6.36       \n[10] timechange_0.3.0     lifecycle_1.0.4      StanHeaders_2.32.10 \n[13] magrittr_2.0.3       posterior_1.6.0      compiler_4.4.0      \n[16] rlang_1.1.4          tools_4.4.0          utf8_1.2.4          \n[19] yaml_2.3.8           knitr_1.47           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.4      \n[25] plyr_1.8.9           abind_1.4-8          withr_3.0.0         \n[28] grid_4.4.0           stats4_4.4.0         fansi_1.0.6         \n[31] colorspace_2.1-0     inline_0.3.19        scales_1.3.0        \n[34] cli_3.6.3            mvtnorm_1.3-1        rmarkdown_2.27      \n[37] generics_0.1.3       RcppParallel_5.1.9   rstudioapi_0.16.0   \n[40] reshape2_1.4.4       tzdb_0.4.0           rstan_2.32.6        \n[43] splines_4.4.0        bayesplot_1.11.1     parallel_4.4.0      \n[46] matrixStats_1.4.1    vctrs_0.6.5          jsonlite_1.8.8      \n[49] hms_1.1.3            arrayhelpers_1.1-0   glue_1.7.0          \n[52] codetools_0.2-20     cowplot_1.1.3        distributional_0.4.0\n[55] stringi_1.8.4        gtable_0.3.5         QuickJSR_1.3.1      \n[58] munsell_0.5.1        pillar_1.9.0         htmltools_0.5.8.1   \n[61] Brobdingnag_1.2-9    R6_2.5.1             evaluate_0.24.0     \n[64] lattice_0.22-6       backports_1.5.0      broom_1.0.6         \n[67] renv_0.17.3          rstantools_2.4.0     coda_0.19-4.1       \n[70] gridExtra_2.3        checkmate_2.3.1      xfun_0.45           \n[73] pkgconfig_2.0.3"
  },
  {
    "objectID": "blog-posts/blog-post-8.html#citing-r-packages",
    "href": "blog-posts/blog-post-8.html#citing-r-packages",
    "title": "Generalized Additive Models (GAMs) for Meta-Regression using brms",
    "section": "Citing R packages",
    "text": "Citing R packages\nThe following packages were used in this post:\n\nbrms (Bürkner 2021)\nmgcv (R Core Team 2024)\ntidyverse (Wickham et al. 2019)\nggdist (Kay 2023)\nmodelr (Wickham 2023)\ntidybayes (Kay 2024)\n\nThanks to Isabella R. Ghement, Stephen J. Wild, and Solomon Kurz for their advice and feedback on this post."
  },
  {
    "objectID": "Consulting.html",
    "href": "Consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "consulting@matthewbjane.com\nI provide three main types of consulting services: statistical consulting, data visualization, and academic website development.\nIf you’re interested in learning more about how I can assist with your projects or if you have questions or require advice, please don’t hesitate to reach out. I offer a free 20-minute consultation to discuss your specific needs and determine how I can best support you. To schedule this consultation, feel free to contact me at consulting@matthewbjane.com. I will be pleased to support you every step of the way.\n\n\nI offer a wide array of services designed to cater to a broad spectrum of statistical needs, whether you’re an academic or part of the industry. Here’s a detailed list of the services I provide:\n\nMeta-analysis\nData-analysis\nData cleaning and management\nStatistical Modeling\nStatistical Software\nPsychometrics and Structural Equation Modeling\nR Programming\nComputationally Reproducible and Self-Contained Manuscripts\n\nI’ve structured my pricing to accommodate different client types, ensuring a transparent and cost-effective approach. The current hourly rates are as follows:\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/hr\n\n\nAcademic\n$130/hr\n\n\nStudent\n$90/hr\n\n\n\nProjects with a substantial scope (i.e., more than 20 hours worth of consulting work) can be structured so that their pricing will be done on a per-project rather than a per-hour basis.\n\n\n\nI can help you prepare high-quality data visualizations for academic publications, websites, slideshows, etc. There are two types of data visualizations that I provide: stand-alone diagrams and computationally reproducible figures.\nTo ensure that your specific needs are met, you can either send me a detailed description of the desired diagram via email (consulting@matthewbjane.com) with the following information:\n\nWhat kind of figure do you need?\nWhen do you realistically need to have it ready by?\nDo you have all the information needed to produce the figure?\nCould you provide an example of a figure you are trying to emulate?\n\nThen we can set up a free 20-minute virtual meeting to discuss the details further.\n\n\nStand-alone diagrams are used as visual representations of a scientific concept. Each diagram I produce will be made available as a high-resolution PNG, JPEG, and SVG file. For your convenience I included an example of my work below .\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/diagram\n\n\nAcademic\n$120/diagram\n\n\nStudent\n$80/diagram\n\n\n\n\n\n\n\n\nI can also help you generate figure panels that contain real or simulated data. For each panel, my service ensures that you will receive the final visual representation (as a high-resolution PNG or JPG), but also an R script containing the code for reproducing the figure. For your convenience, I’ve included examples of my work, showcasing both a two-panel figure (top) and a single-panel figure (bottom).\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/panel\n\n\nAcademic\n$130/panel\n\n\nStudent\n$100/panel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI can help you create your own academic/personal website powered through Quarto. Websites can be published through Quarto Pub or Github pages. For an example, my website matthewbjane.com is entirely created through Quarto. Below are some base packages that you can start with, however prices may vary if features are added or removed from these base packages:\n\n\n\nFeatures\nRate\n\n\n\n\nHome Page + CV + Blog + Favicon/Banner/Logo\n$1,100\n\n\nHome Page + CV + Blog\n$800\n\n\nHome Page + CV\n$550\n\n\n\n\n\n\nVelu Immonen, Graduate Student at Solent University:\n\nWith Matthew’s expert guidance, I navigated through my first meta-analysis with success. His support in effect size selection, model specification, and data visualisation proved indispensable. It felt like having ChatGPT on your ear, with the distinction that Matthew actually offered up-to-date best practices without making up citations – not to mention the human conversation. This service provided me with clarity and considerable time savings. I wholeheartedly endorse it for anyone delving into the world of meta-analysis.\n\nAlysson Enes, Graduate Student at Federal University of Paraná:\n\nMatthew helped me with a data visualization service. I spent weeks trying to solve a code in a figure, but I couldn’t find the solution. When I contacted Matthew, we had an initial meeting where he was extremely helpful in solving my problem. Matthew quickly sent me a guide with a full explanation of the problem, including how I could use the data visualization on other datasets. I recommend Matthew’s work to anyone who is having problems with data visualization."
  },
  {
    "objectID": "Consulting.html#statistical-consulting-services",
    "href": "Consulting.html#statistical-consulting-services",
    "title": "Consulting",
    "section": "",
    "text": "I offer a wide array of services designed to cater to a broad spectrum of statistical needs, whether you’re an academic or part of the industry. Here’s a detailed list of the services I provide:\n\nMeta-analysis\nData-analysis\nData cleaning and management\nStatistical Modeling\nStatistical Software\nPsychometrics and Structural Equation Modeling\nR Programming\nComputationally Reproducible and Self-Contained Manuscripts\n\nI’ve structured my pricing to accommodate different client types, ensuring a transparent and cost-effective approach. The current hourly rates are as follows:\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/hr\n\n\nAcademic\n$130/hr\n\n\nStudent\n$90/hr\n\n\n\nProjects with a substantial scope (i.e., more than 20 hours worth of consulting work) can be structured so that their pricing will be done on a per-project rather than a per-hour basis."
  },
  {
    "objectID": "Consulting.html#data-visualization-services",
    "href": "Consulting.html#data-visualization-services",
    "title": "Consulting",
    "section": "",
    "text": "I can help you prepare high-quality data visualizations for academic publications, websites, slideshows, etc. There are two types of data visualizations that I provide: stand-alone diagrams and computationally reproducible figures.\nTo ensure that your specific needs are met, you can either send me a detailed description of the desired diagram via email (consulting@matthewbjane.com) with the following information:\n\nWhat kind of figure do you need?\nWhen do you realistically need to have it ready by?\nDo you have all the information needed to produce the figure?\nCould you provide an example of a figure you are trying to emulate?\n\nThen we can set up a free 20-minute virtual meeting to discuss the details further.\n\n\nStand-alone diagrams are used as visual representations of a scientific concept. Each diagram I produce will be made available as a high-resolution PNG, JPEG, and SVG file. For your convenience I included an example of my work below .\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/diagram\n\n\nAcademic\n$120/diagram\n\n\nStudent\n$80/diagram\n\n\n\n\n\n\n\n\nI can also help you generate figure panels that contain real or simulated data. For each panel, my service ensures that you will receive the final visual representation (as a high-resolution PNG or JPG), but also an R script containing the code for reproducing the figure. For your convenience, I’ve included examples of my work, showcasing both a two-panel figure (top) and a single-panel figure (bottom).\n\n\n\nClient Type\nRate\n\n\n\n\nNon-Academic (Industry)\n$150/panel\n\n\nAcademic\n$130/panel\n\n\nStudent\n$100/panel"
  },
  {
    "objectID": "Consulting.html#academic-websites-powered-by-quarto",
    "href": "Consulting.html#academic-websites-powered-by-quarto",
    "title": "Consulting",
    "section": "",
    "text": "I can help you create your own academic/personal website powered through Quarto. Websites can be published through Quarto Pub or Github pages. For an example, my website matthewbjane.com is entirely created through Quarto. Below are some base packages that you can start with, however prices may vary if features are added or removed from these base packages:\n\n\n\nFeatures\nRate\n\n\n\n\nHome Page + CV + Blog + Favicon/Banner/Logo\n$1,100\n\n\nHome Page + CV + Blog\n$800\n\n\nHome Page + CV\n$550"
  },
  {
    "objectID": "Consulting.html#testimonials",
    "href": "Consulting.html#testimonials",
    "title": "Consulting",
    "section": "",
    "text": "Velu Immonen, Graduate Student at Solent University:\n\nWith Matthew’s expert guidance, I navigated through my first meta-analysis with success. His support in effect size selection, model specification, and data visualisation proved indispensable. It felt like having ChatGPT on your ear, with the distinction that Matthew actually offered up-to-date best practices without making up citations – not to mention the human conversation. This service provided me with clarity and considerable time savings. I wholeheartedly endorse it for anyone delving into the world of meta-analysis.\n\nAlysson Enes, Graduate Student at Federal University of Paraná:\n\nMatthew helped me with a data visualization service. I spent weeks trying to solve a code in a figure, but I couldn’t find the solution. When I contacted Matthew, we had an initial meeting where he was extremely helpful in solving my problem. Matthew quickly sent me a guide with a full explanation of the problem, including how I could use the data visualization on other datasets. I recommend Matthew’s work to anyone who is having problems with data visualization."
  },
  {
    "objectID": "Books.html",
    "href": "Books.html",
    "title": "Books",
    "section": "",
    "text": "Guide to Effect Sizes and Confidence Intervals      2024\n    Go to book\n  \n\n\n\n  \n    \n    \n  \n  \n    Artifact Corrections for Effect Sizes\n     Coming 2024\n    Coming Soon!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Email Twitter Github Scholar\n\n\nHello there! I’m Matthew B. Jané - I’m a PhD student in Quantitative Psychology at the University of Connecticut. My research involves developing statistical methods and software to aide in meta-analysis and evidence synthesis. Specifically, my current research projects focus on correcting bias in effect size estimates caused by statistical artifacts. My advisor is Dr. Blair T. Johnson and I am a member of the Systematic Health Action Research Program (SHARP). I am also on the editorial board for Psychological Bulletin as a methodological reviewer."
  },
  {
    "objectID": "blog-posts/blog-post-9.html#introducing-a-living-meta-analysis",
    "href": "blog-posts/blog-post-9.html#introducing-a-living-meta-analysis",
    "title": "Unjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt",
    "section": "Introducing a Living Meta-Analysis",
    "text": "Introducing a Living Meta-Analysis\nI have (along with help from others) created a page on my website that hosts the code and results of a living meta-analysis of RCTs of social media reduction studies for mental health outcomes (depression and anxiety, specifically). The meta-analysis can be found here: matthewbjane.com/social_media_mental_health.html. Since the effect sizes in Ferguson’s database had multiple flaws (miscalculated effect sizes, pooled across conceptually different measures, did not code mid-intervention effects), I decided to fully recalculate all the effect sizes for anxiety and depression separately, obtain as many time-points in each study as I could, and construct a multi-level model to account for effect dependency. I think probably most people will agree that the resulting model, follows much more appropriate statistical practices than Haidt, Rausch, and Stein’s (RH&S) approach.\nThis site will be completely open to changes, critiques, or additions (open an issue on my github repository, DM me on twitter/blue-sky, or email me at matthewbjane@gmail.com). This should be the first site that Haidt, Rausch, and Stein refer to before they make causal claims.\nIf there are any errors at all, let me know and I will make the change ASAP."
  },
  {
    "objectID": "blog-posts/blog-post-9.html#pre-print-by-thrul-et-al.",
    "href": "blog-posts/blog-post-9.html#pre-print-by-thrul-et-al.",
    "title": "Unjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt",
    "section": "Pre-print by Thrul et al.",
    "text": "Pre-print by Thrul et al.\nA re-analysis of Ferguson’s meta-analysis by Thrul et al. makes many of the same mistakes that RH&S did. They dichotomize time again to show that effects after one week are positive and effects less than one week are negative. Aside from the fact that there were errors in the data base, dichotomizing time is virtually never a good modeling approach. The one-week cut-point differed from Haidt, Rausch, and Stein’s cut-point of 2 weeks. In my last post on this debate, I demonstrated how RH&S’s cut-point of 2 weeks is arbitrary and prone to nose mining (if I change the cut-point to greater than two weeks rather than 2 weeks or greater the effect size changes considerably, around a ~50% decrease).\nAs for their non-linear model (which is almost identical to what I did in my second post almost a month ago), it is unclear what their model is? Also, there are no confidence intervals or prediction intervals and the points being the same size makes it seem like they did not weight it?\n\n\n\nTable from Thrul et al.\n\n\nThis is the entire methods section from Thrul et al.:\n\nWe used the information available on the OSF platform related to the original meta-analysis (https://osf.io/jcha2). We then reviewed all included studies (https://osf.io/27dx6). Of the 27 originally included studies, we excluded 7 studies because they were not reduction / abstinence interventions (see online supplement). For example, these studies tested the psychological impact of short sessions of social media use (e.g., active vs. passive use; browsing vs. communicating) (Yuen et al., 2019). We extracted the length of intervention for all studies and categorized them into studies that intervened on social media use for less than 1 week vs. 1 week or longer (see online supplement). Moreover, we also tested intervention length as a continuous variable, using number of weeks as well as number of days as moderator and testing curvilinear relationships by including quadratic terms. All effect sizes were taken from the OSF platform. Random-effects models were estimated using the metafor package in R.\n\nThis paper has been accepted at Psychology of Popular Media. This paper contains results based on incorrect data, unclear methods, and artificial dichotomization. I do not think it should be published. I will be writing a more formal response, soon.\nJonathan Haidt then uses this meta-analysis, ignores the curvilinear relationship and then makes a tweet stating:\n\nA new meta-analysis, in press, from @drjthrul et al, using data from the @CJFerguson1111 analysis, finds that “interventions of less than 1 week resulted in significantly worse mental health outcomes (d=-0.168, SE=0.058, p=.004), while interventions of 1 week or longer resulted in significant improvements (d=0.169, SE=0.065, p=.01). Researchers and journalist should stop saying that the evidence is”just correlational.” There is experimental evidence too. We can certainly debate what this all means, but there is a body of experiments that points, pretty consistently, to benefits from reducing social media use, as long as the reduction goes on long enough to get past withdrawal effects.\n\nOf course, this is using, yet again, an arbitrary cut-point to prove the same point and analyzed on a database with incorrect effect sizes. I have modeled it continuously and now with the new living meta, this should be abundantly clear. I followed up to his tweet with the following reply:\n\n\n;document.getElementById(\"tweet-50777\").innerHTML = tweet[\"html\"];\nAnyways, that is it from me. Here is a much nicer note about causality in these RCTs from Stephen J. Wild."
  },
  {
    "objectID": "blog-posts/blog-post-9.html#a-note-on-causality-in-these-rcts-by-stephen-j.-wild",
    "href": "blog-posts/blog-post-9.html#a-note-on-causality-in-these-rcts-by-stephen-j.-wild",
    "title": "Unjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt",
    "section": "A note on causality in these RCTs (by Stephen J. Wild)",
    "text": "A note on causality in these RCTs (by Stephen J. Wild)\nFiguring out the causal effect social media and mental health is difficult. Social media affects mental health through both direct and indirect paths, some of which we can close in reduction or exposure experiments, and some of which we can’t. On top of everything below, we have the usual internal and external validity threats. Among the two biggest issues that likely affect the Ferguson meta-analysis, we cannot agree on the definition of social media or social media use and we do not have a clear and well-measured outcome. Even when we do a well-designed and executed experiment, a number of other challenges threaten our attempts at causal inference.\nFirst we have interference and spillovers, where the causal effect of our exposure affects units other than the unit under study. For social media, it means that social media use doesn’t just affect the person in the study, but also affects their friends and family. Second is effect heterogeneity and the power of our study. We can and should expect social media to affect different groups differently. Yet many of our studies are not powered to detect small effects, let alone small effects in different subgroups. Third is statistical under or over control, where we include the wrong control variables. We include colliders where we shouldn’t, mediators that block the causal path, and fail to include confounders where we should. Fourth includes failure to account for feedback loops, where our exposure (social media) influences our outcome (mental health), which in turn affects our exposure (we have worse mental health so in turn use more social media, and repeat). And a fifth reason involves substitution effects and the proper attribution of causality. When we reduce our social media use, we increase our time spent on other activities. Is the causal effect due to reduced social media use, or an increase in the other activities?"
  },
  {
    "objectID": "blog-posts/blog-post-9.html#session-info",
    "href": "blog-posts/blog-post-9.html#session-info",
    "title": "Unjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.0    fastmap_1.2.0     cli_3.6.3        \n [5] htmltools_0.5.8.1 tools_4.4.0       rstudioapi_0.16.0 yaml_2.3.8       \n [9] rmarkdown_2.27    knitr_1.47        jsonlite_1.8.8    xfun_0.45        \n[13] digest_0.6.36     rlang_1.1.4       renv_0.17.3       evaluate_0.24.0"
  },
  {
    "objectID": "blog-posts/blog-post-9.html#citing-r-packages",
    "href": "blog-posts/blog-post-9.html#citing-r-packages",
    "title": "Unjustifiable Methods and How I plan to fix this: a Response to Stein, Rausch, and Haidt",
    "section": "Citing R packages",
    "text": "Citing R packages\nThe following packages were used in this post:\n\nbrms (Bürkner 2021)\nmgcv (R Core Team 2024)\ntidyverse (Wickham et al. 2019)\nggdist (Kay 2023)\nmodelr (Wickham 2023)\ntidybayes (Kay 2024)"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "href": "blog-posts/blog-post-3.html#step-1-obtain-the-necessary-statistics",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Step 1: Obtain the necessary statistics",
    "text": "Step 1: Obtain the necessary statistics\nIn order to calculate the pre/post correlation (\\(r\\)), we need the standard deviation (SD) of pre-test scores (\\(SD_{pre}\\)), the SD of post-test scores (\\(SD_{post}\\)), the mean change (\\(M_{change}\\)), the paired t-statistic (\\(t\\)), the sample size (\\(N\\)). In this blog post, we are assuming that the change score standard deviation (\\(SD_{change}\\)) is unavailable to us. If the pre-test SD is available, but the post-test SD is unavailable, you can approximate the post-test SD by, first, taking average ratio of the pre-test SD and post-test SD from \\(k\\) studies in the current meta-analysis (\\(\\overline{SD}_{ratio}\\); see the blog post on 9/8/2023), then we can approximate the post-test SD by multiplying the pre-test SD by the average SD ratio,\n\\[\nSD_{post}\\approx \\bar{SD}_{ratio}\\times SD_{pre}\n\\]\nA rougher approximation would be to simply set the pre-test SD and post-test SD to be equal. If the study reports an F-statistic from a one-way repeated measures ANOVA, the F-statistic is equal to the square of the t-statistic,\n\\[\nt = \\sqrt{F}\n\\]\nEnsure that you apply the correct sign (negative or positive) to the t-statistic, since the F-statistic is always positive.\n\n# Obtain values\nM_change &lt;- 3\nSD_post &lt;- 11\nSD_pre &lt;- 9\nt &lt;- 2.50\nN &lt;- 50"
  },
  {
    "objectID": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "href": "blog-posts/blog-post-3.html#step-2-calculate-the-prepost-correlation",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Step 2: Calculate the Pre/Post Correlation",
    "text": "Step 2: Calculate the Pre/Post Correlation\nLets start by figuring out how to find the change score SD. The paired t-statistic is defined as the mean change score divided by the standard error of change scores, such that, \\[\nt = \\frac{M_{change}}{SE_{change}}\n\\] Since we need the change score SD, we can use the definition of the standard error of the mean to put the t-statistic in terms of \\(SD_{change}\\): \\[\nSE_{change}=\\frac{SD_{change}}{\\sqrt{N}}\n\\] and therefore \\(t\\) can be expressed as,\n\\[\nt=\\frac{M_{change}}{\\left(\\frac{SD_{change}}{\\sqrt{N}}\\right)}\n\\]\nthen we just need to solve for \\(SD_{change}\\):\n\\[\nSD_{change}=\\frac{M_{change}\\times\\sqrt{N}}{t}\n\\]\nOkay so now let us recall the definition of change score SDs from the blog post on 9/8/2023. In that blog we discussed how to obtain the pre/post correlation from the change score SD, now that we have converted \\(t\\) to \\(SD_{change}\\), we can solve for the correlation in a similar way. First things first, the change score SD can be defined as, \\[\nSD_{change} = \\sqrt{SD^2_{pre} + SD^2_{post} - 2\\times r\\times SD_{pre}SD_{post}}\n\\]\nWe can re-arrange this to isolate the pre/post correlation (\\(r\\)),\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - SD^2_{change}}{2 \\times SD_{pre}\\times SD_{post}}\n\\]\nIn our case, the study did not report the change score SD, therefore we can replace it with our derived \\(SD_{change}\\) from a paired t-test:\n\\[\nr = \\frac{SD^2_{pre} + SD^2_{post} - \\left(\\frac{M_{change}\\times\\sqrt{N}}{t}\\right) ^2}{2 \\times SD_{pre}\\times SD_{post}}\n\\] Lets neaten this formulation up a tad:\n\\[\nr = \\frac{t^2\\left(SD^2_{pre} + SD^2_{post}\\right) - N\\times M^2_{change} }{2 \\times t^2 \\times SD_{pre}\\times SD_{post} }\n\\]\nIsn’t that just a beautiful thing?? So there you have it! the full equation for the pre/post correlation from a paired t-test! Note that this is a direct conversion and not merely an approximation.\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\n# Print results\nprint(paste0('r = ',round(r,3)))\n\n[1] \"r = 0.657\""
  },
  {
    "objectID": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "href": "blog-posts/blog-post-3.html#applying-it-to-a-simulated-dataset",
    "title": "Calculating Pre/Post Correlation from a Paired T-Test",
    "section": "Applying it to a simulated dataset",
    "text": "Applying it to a simulated dataset\nWe can simulate correlated pre/post scores from a bivariate Gaussian with known parameters. The calculated correlation is exactly correct!\n\n# install.packages('MASS')\nlibrary(MASS)\n\n# Define parameters\nSD_pre &lt;- 9\nSD_post &lt;- 11\nr_true &lt;- .70\nM_pre &lt;- 20\nM_post &lt;- 25\nN &lt;- 100\n\n# Simulate correlated pre/post scores from bivariate gaussian\ndata &lt;- mvrnorm(n=N,\n               mu=c(M_pre,M_post),\n               Sigma = data.frame(x=c(SD_pre^2,r_true*SD_pre*SD_post),\n                                  y=c(r_true*SD_pre*SD_post,SD_post^2)),\n               empirical = TRUE)\n\n# Obtain simulated scores\nx_pre &lt;- data[,1] # Pre-test scores\nx_post &lt;- data[,2] # Post-test scores\nx_change &lt;- x_post - x_pre # Calculate change scores\n\n# Calculate standard deviations, t-stats, and mean change\nSD_pre &lt;- sd(x_pre)\nSD_post &lt;- sd(x_post)\nt &lt;- mean(x_change) / (sd(x_change)/sqrt(N))\nM_change &lt;- mean(x_change)\n\n# Calculate pre/post correlation\nr &lt;- (t^2*(SD_pre^2 + SD_post^2) - N * M_change^2) / (2*t^2*SD_pre*SD_post)\n\n# print results\nprint(paste0('r = ',r))\n\n[1] \"r = 0.7\""
  },
  {
    "objectID": "blog-posts/blog-post-4.html#defining-a-squared-effect-size",
    "href": "blog-posts/blog-post-4.html#defining-a-squared-effect-size",
    "title": "A Response to Jonathan Haidt’s response",
    "section": "Defining a Squared Effect Size",
    "text": "Defining a Squared Effect Size\nSometimes squared effect sizes are used in order to interpret results (e.g., \\(R^2\\), \\(\\eta^2\\)). However these squared indices are biased in small sample sizes. To see why this is the case we can describe the mathematically. Generally, lets say we have some observed effect size \\(h_i\\) for a sample \\(i\\). This can be modeled as a function of a population correlation \\(\\theta\\) and sampling error \\(e_i\\),\n\\[\nh_i = \\theta + e_i\n\\]\nWhere the expectation of \\(h_i\\) (i.e., the average) upon repeated sampling is equal to \\(\\theta\\). More importantly for us, the expectation of sampling error \\(e_i\\) across repeated samples would be zero. For the expectation of \\(e_i\\) to be zero, \\(e\\) would need to be randomly distributed around zero with both positive and negative values. Let us see what happens to the model once we square both sides to obtain \\(h^2\\):\n\\[\nh^2_i = (\\theta + e_i)^2\n\\]\nand thus,\n\\[\nh^2_i = \\theta^2 + 2\\theta e_i + e_i^2\n\\]\nThe squared effect sizes is now no longer an unbiased estimate of theta. The bias in \\(h_i^2\\) can be seen by taking the expected value of both sides (note that the \\(\\text{Var}(e_i)\\) is the sampling error variance and \\(\\mathbb{E}[\\cdot]\\) is the expected value or the mean across repeated samples):\n\\[\\begin{align}\n\\mathbb{E}[h^2_i] & = \\mathbb{E}[\\theta^2 + 2\\theta e_i + e_i^2] \\\\[.5em]\n                & = \\theta^2 + 2\\theta \\times \\mathbb{E}[e_i] + \\mathbb{E}[e^2_i]\\\\[.5em]\n                & = \\theta^2 + \\mathbb{E}[e_i^2] \\\\[.5em]\n                & = \\theta^2 + \\left(\\text{Var}[e_i] + \\mathbb{E}[e_i]^2\\right) \\\\[.5em]\n                & = \\theta^2 + \\text{Var}[e_i] \\\\\n\\end{align}\\]\nSince the sampling error variance is always positive (unless the sample size was infinite…), there will be a positive bias in expected value of \\(h_i^2\\). It is important to note that the variance in sampling error (i.e., \\(\\text{Var}(e_i)\\)) will be larger with smaller sample sizes."
  },
  {
    "objectID": "blog-posts/blog-post-4.html#simulated-example",
    "href": "blog-posts/blog-post-4.html#simulated-example",
    "title": "A Response to Jonathan Haidt’s response",
    "section": "Simulated Example",
    "text": "Simulated Example\nLets assume that in the population, the squared correlation between predicted and observed outcome (\\(R^2\\)) is zero, that is, the independent variables (\\(X_1\\),\\(X_2\\), and \\(X_3\\)) do not explain any variance in the dependent variable (\\(Y\\)). Lets simulate two studies, where one study has a sample size of 100 and the other has a sample size of 25. Then lets replicate these studies 5,000 times. In the first figure we will see that the mean value of \\(R^2\\) is not zero even though the population value is. And with the smaller sample sizes within each study (\\(N = 25\\)), the mean \\(R^2\\) is larger. The plots below show the distribution of \\(R^2\\) using the ggdist package (kay?).\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(ThemePark)\nlibrary(patchwork)\nlibrary(latex2exp)\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR21 &lt;- R22 &lt;- R23 &lt;- c()\nfor(i in 1:5000){\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N,0,1)\n  x2 &lt;- rnorm(N,0,1)\n  x3 &lt;- rnorm(N,0,1)\n  y &lt;- rnorm(N,0,1)\n  \n  # linear regression model\n  mdl &lt;- lm(y ~ x1 + x2 + x3)\n  R21[i] &lt;- summary(mdl)$r.squared\n\n  # linear regression model\n  mdl &lt;- lm(y ~ x1 + x2 + x3,subset = sample(1:N,25))\n  R22[i] &lt;- summary(mdl)$r.squared\n}\n\n\nh1 &lt;- ggplot(data=NULL, aes(R21)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_slabinterval(color = barbie_theme_colors['dark'], \n                    fill = barbie_theme_colors['light']) +\n  scale_x_continuous(limits = c(0,.5)) +\n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = \"\",x=TeX(\"$R^2$\"), \n       subtitle =  TeX(paste0('N=', 100, ', Ave $R^2$=', round(mean(R21),2))))\n\nh2 &lt;- ggplot(data=NULL, aes(R22)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_slabinterval(color = barbie_theme_colors['dark'], \n                    fill = barbie_theme_colors['light']) +\n  scale_x_continuous(limits = c(0,.5)) +\n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = \"\",x=TeX(\"$R^2$\"), \n       subtitle = TeX(paste0('N=', 25, ', Ave $R^2$=', round(mean(R22),2))))\n  \nh1 + h2 & plot_annotation(theme = theme(plot.background = element_rect(color=\"#f1f9fa\",fill =\"#f1f9fa\")))\n\n\n\n\n\n\n\n\n\nIf we were to run a meta-analysis on all 5,000 studies and found a meta-analytic average \\(R^2\\) of .13, we may conclude that there is a meaningful prediction/effect, however, this could simply be an artifact of sampling error. The plot below shows how the mean \\(R^2\\) changes as you vary the sample size.\n\n\nCode\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR2 &lt;- N &lt;- c()\nfor(i in 1:5000){\n  N[i] &lt;- runif(1,10,70)\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N[i] ,0,1)\n  x2 &lt;- rnorm(N[i] ,0,1)\n  x3 &lt;- rnorm(N[i] ,0,1)\n  y &lt;- rnorm(N[i] ,0,1)\n  # linear regression model\n  mdl &lt;- lm(y ~ x1)\n  R2[i] &lt;- summary(mdl)$r.squared\n}\n\n\nggplot(data=NULL, aes(N,R2)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_smooth( color = barbie_theme_colors['dark']) + \n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = TeX(\"Mean $R^2$\"),x=\"Sample Size\", title = \"\")+ \n  expand_limits(y = 0)\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "blog-posts/blog-post-4.html#conclusion",
    "href": "blog-posts/blog-post-4.html#conclusion",
    "title": "A Response to Jonathan Haidt’s response",
    "section": "Conclusion",
    "text": "Conclusion\nIn a meta-analysis, it is always best to use directional effect sizes that can be both positive and negative such as Cohen’s \\(d\\)’s or Pearson correlations. There are other strategies for meta-analyzing \\(R^2\\) values such as using individual person data meta-analysis (using the raw data for each study) or meta-analyzing the correlation matrix between predictors and outcomes first and then running the regression model on the meta-analytic correlation matrix (this would not allow for non-linear terms though). If it is necessary to meta-analyze \\(R^2\\) you will want to use the adjusted \\(R^2\\) value. See the plot below that shows the adjusted \\(R^2\\) across different sample sizes\n\n\nCode\nset.seed(343)\n\n# sample size\nN &lt;- 100\n\nR2 &lt;- N &lt;- c()\nfor(i in 1:5000){\n  N[i] &lt;- runif(1,10,70)\n  # Obtain simulated scores\n  x1 &lt;- rnorm(N[i] ,0,1)\n  x2 &lt;- rnorm(N[i] ,0,1)\n  x3 &lt;- rnorm(N[i] ,0,1)\n  y &lt;- rnorm(N[i] ,0,1)\n  # linear regression model\n  mdl &lt;- lm(y ~ x1)\n  R2[i] &lt;- summary(mdl)$adj.r.squared\n}\n\n\nggplot(data=NULL, aes(N,R2)) + \n  theme_barbie(barbie_font = FALSE) +\n  stat_smooth( color = barbie_theme_colors['dark']) + \n  theme(aspect.ratio = 1, \n        panel.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        plot.background = element_rect(fill = '#f1f9fa',color = '#f1f9fa'),\n        )+\n  labs(y = TeX(\"Mean adj. $R^2$\"),x=\"Sample Size\", title = \"\")+ \n  ylim(c(-.4,.4)) +\n  expand_limits(y = 0)\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "blog-posts/blog-post-7.html#establishing-my-motivation",
    "href": "blog-posts/blog-post-7.html#establishing-my-motivation",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Establishing my motivation",
    "text": "Establishing my motivation\nAlthough my original post made it seem that I have some prior opinion that social media effects mental health. In reality, I actually think that social media probably does have an average negative effect on mental health. I also probably agree that the RCTs in this meta-analysis probably won’t find any convincing causal estimates. If Rausch and Haidt’s (R&H) made the argument that there is a causal effect, but the RCTs won’t capture then I would have never made my original post. What I know is meta-analysis, not the literature on social media use and mental health. I was motivated to write the original post because of the clear statistical malpractice employed in R&H’s re-analysis. I have no desire to prove or disprove whether social media is actually detrimental to mental health, but I do have a desire to combat the spread of misleading analyses and poor statistical practice.\nAlso, to those absurd comments and emails out there saying that I am somehow working for big Pharma, social media companies, the government, or profiting on some misinformation campaign: I am literally a graduate student who lives with my parents to save money on rent (you can help me out by buying me a coffee). I do this because I truly believe in the fight for a rigorous, open, and transparent science. I have been very critical of how academia operates and I hope to make some positive change to that end. Anyways, on to the post…"
  },
  {
    "objectID": "blog-posts/blog-post-7.html#where-could-i-have-been-better",
    "href": "blog-posts/blog-post-7.html#where-could-i-have-been-better",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Where could I have been better?",
    "text": "Where could I have been better?\nThe best criticisms of my blog outside of R&H’s criticisms seemed to have been that length weeks should have been log scaled (pulling in that 10 week intervention) and also I should have fit some non-linear model. I think these are completely valid points and I will conduct that analysis in this blog post.\nR&H responded to one of my last analyses in the post (regressing the effect size on length of intervention) and said the following:\n\na blog post by Matthew Jané used Ferguson’s numbers and concluded that there is no relationship between the duration of reduction and the effect size.\n\nThis is all true (with the slight quibble that I said “little/no” relationship) and I should have been clearer about the assumptions underlying my claim. In my original post I said this:\n\nOkay so as it turns out there appears to be little/no effect of the length of the intervention on the effect size.\n\nI really should have said this:\n\nGiven the proposed model and the data, there appears to be little/no effect of the length of the intervention on the effect size.\n\nUsually that does not need to be said considering the model was defined, but it is important nonetheless, since if I were to choose a different model it is possible we could have see a relationship between the two variables."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#in-response-to-the-community-note-on-haidts-tweet",
    "href": "blog-posts/blog-post-7.html#in-response-to-the-community-note-on-haidts-tweet",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "In Response to the Community Note on Haidt’s Tweet",
    "text": "In Response to the Community Note on Haidt’s Tweet\nA community note (that I did not write) was attached to Haidt’s tweet containing the R&H re-analysis. It has since disappeared, but here was the content of the note:\n\nAs many statistics experts have pointed out, this analysis is incorrect. If done correctly, the analysis shows that reducing social media has no effect. *then it links to my blog post as a source*\n\nAlthough the first sentence I believe is accurate, the second sentence is not true and I did not state that in the original post. Therefore I am okay that the community note was taken down for whatever reason. Then promptly, a better and even more recent community note was posted that read,\n\nThe reanalysis doesn’t use an appropriate statistical model. It simply calculates an unweighted average across different studies, doesn’t provide confidence intervals, and misinterprets the evidence\n\nAll these points are unequivocally true. However, Haidt then posted the following tweet,\n\nThis tweet seemed to get his followers to rate the community note as unhelpful and thus it was taken down again. Of course, it is difficult to maintain a community note with Haidt’s large following. So I guess we will have to treat these blog posts as a much more detailed community note."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#rhs-analysis-was-unequivocally-a-meta-analysis",
    "href": "blog-posts/blog-post-7.html#rhs-analysis-was-unequivocally-a-meta-analysis",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "R&H’s analysis was unequivocally a meta-analysis",
    "text": "R&H’s analysis was unequivocally a meta-analysis\nIn a tweet by Haidt, he pointed out that R&H’s re-analysis should not be considered a meta-analysis:\n\nI think it was a mistake to treat our essay as a meta-analysis\n\nThe reason I treated R&H’s re-analysis as a meta-analysis is because it is, by definition, a meta-analysis. Let me be clear, R&H’s analysis consisted of 1) averaging results across studies and 2) counting up positive, negative, and null effects, both of which are just different obsolete and unacceptable methods of meta-analysis (the latter is commonly referred to as a vote-counting meta-analysis). If you are unconvinced, here is the definition of a meta-analysis from the Cochrane handbook:\n\nMeta-analysis is the statistical combination of results from two or more separate studies.\n\nIf you simply calculate an average effect size across studies, you have conducted a meta-analysis. If you tally up the number of positive and negative results, you have conducted a meta-analysis. Every table in R&H’s article is literally displaying a meta-analysis, whether that was their intent or not.\nOn top of all of the fundamental flaws in their analysis, R&H also based the following claim on their meta-analytic results:\n\nThese results clearly indicate that multi-week social media reduction experiments improve some aspects of well-being\n\nTheir results absolutely do not clearly indicate this. This is an extremely misleading conclusion that is based on a completely inappropriate analysis.\nI recommend that R&H remove the tables with meta-analytic results and all the conclusions/claims associated with those results. Adding a postscript, a preface, and changing the title does not fix this."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#errors-in-effect-sizes-and-sample-sizes",
    "href": "blog-posts/blog-post-7.html#errors-in-effect-sizes-and-sample-sizes",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Errors in Effect Sizes and Sample Sizes",
    "text": "Errors in Effect Sizes and Sample Sizes\nHaidt also claims that there are errors in the effect sizes and sample sizes in Ferguson (2024)’s meta-analysis:\n\nOur post was about how there are so many errors in the Ferguson meta, including errors in sample sizes and effect sizes, that we can’t yet do weighting or confidence intervals.\n\nYet R&H still conduct their re-analysis on those effect sizes. If the data is untrustworthy why conduct any analysis in the first place? Why is it more justifiable to compute unweighted average effects without any indicator of variability from that data base? If they believe that the data is untrustworthy, why conduct the analysis at all? If we decide to conduct an analysis, we must do it using the appropriate statistical techniques. Like I said previously, if R&H retracted their re-analysis and instead argued that these RCTs do not properly capture the causal effects of social media on mental health, I would probably agree with them.\nI do not have the time to double check all the effect sizes of Ferguson’s meta-analysis. I analyzed them at face value which is exactly what R&H did in their re-analyses."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#haidts-future-meta-analysis",
    "href": "blog-posts/blog-post-7.html#haidts-future-meta-analysis",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Haidt’s future meta-analysis",
    "text": "Haidt’s future meta-analysis\nHaidt has proposed that he is planning to conduct a re-do of the Ferguson (2024)’s meta-analysis\n\nNonetheless, all of this is to show why we are not trying to do our own meta analysis with Ferguson’s data. In a future post we plan to do a review of the evidence from scratch so that we avoid these many problems. (It’s worth noting that we have also found five additional multi-week reduction studies, all of which find benefits to reduction, which are not in Ferguson’s meta).\n\nTo Jonathan Haidt: I would love to offer my help as a collaborator/consultant for this upcoming project so that we can establish a systematic, transparent, reproducible, and rigorous review, and meta-analysis plan. I also know many awesome and knowledgeable meta-analysts that I would be happy to recommend to you. You can email me at matthew.jane@uconn.edu\nConstructing a systematic, rigorous, transparent, and fully reproducible meta-analysis is not only difficult, it is also exceedingly rare."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#controversy-surrounding-the-study-by-collis-and-egger-2022",
    "href": "blog-posts/blog-post-7.html#controversy-surrounding-the-study-by-collis-and-egger-2022",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Controversy surrounding the study by Collis and Egger (2022)",
    "text": "Controversy surrounding the study by Collis and Egger (2022)\nR&H’s primary critique of my analysis was that Collis and Egger’s (2022) study shouldn’t be included because the manipulation check failed. From my reading of the paper, the treatment group did demonstrate lower screen time for facebook, instagram, and snapchat, however higher for instant messaging apps, news/media, gaming, as well as overall screen time. R&H mention WhatsApp as one of the primary apps that people replaced social media with. It does not seem that people really consider instant messaging like WhatsApp a form of social media (see poll results below):\n\nIt does not seem clear to me that this is a failed manipulation check, especially considering total digital screen time is not the target intervention variable. R&H also mention there is no evidence that other social media apps were used:\n\nIn addition, there is no evidence that participants reduced their use of other social media platforms such as TikTok, YouTube, Twitter, and Reddit (all of which were widely used by college students in 2019).\n\nHowever that is true for a lot of the other studies in this meta-analysis where screen time wasn’t even monitored or monitored with self-report rather than more objective measures. I do not think we should punish one study for being transparent and providing a manipulation check.\nAlthough I am against the exclusion of Collis and Eggers (2022) in principle, I will nevertheless fit the regression model with and without it."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#setting-up-the-code",
    "href": "blog-posts/blog-post-7.html#setting-up-the-code",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Setting up the code",
    "text": "Setting up the code\nIn this section we are just going to run the code we went over in the last blog post (see here)\n\n# load in packages\nlibrary(mgcv)\nlibrary(splines)\nlibrary(osfr)\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(readxl)\nlibrary(psychmeta)\n\n# download data from OSF repo\nread_xlsx(\"~/Documents/MatthewBJane.nosync/blog-posts/Best Practices Coding Experiments.xlsx\")\n\n# A tibble: 35 × 16\n   Citation       block     n      d Standardized Outcome…¹ `Validated Outcomes`\n   &lt;chr&gt;          &lt;lgl&gt; &lt;dbl&gt;  &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n 1 Alcott 2020    NA     1661  0.09                       1                    1\n 2 Brailovskaia … NA      286  0.154                      1                    1\n 3 Brailovskaia … NA      322  0                          1                    1\n 4 Collins and E… NA      121 -0.138                      1                    1\n 5 Deters & Mehl… NA       86 -0.207                      1                    1\n 6 Faulhaber et … NA      230  0.484                      1                    1\n 7 Gajdics 2022   NA      235 -0.364                      1                    1\n 8 Hall et al. 2… NA      130 -0.007                      1                    1\n 9 Hunt 2018      NA      143  0.232                      1                    1\n10 Hunt 2021      NA       88  0.374                      1                    1\n# ℹ 25 more rows\n# ℹ abbreviated name: ¹​`Standardized Outcomes`\n# ℹ 10 more variables: `Matched Control Condition` &lt;dbl&gt;,\n#   `Distractor Questionnaires` &lt;dbl&gt;, `Query hypothesis guessing` &lt;dbl&gt;,\n#   Preregistration &lt;dbl&gt;, Age &lt;dbl&gt;, Year &lt;dbl&gt;, `Best Practices Total` &lt;dbl&gt;,\n#   `Verified 1= yes, 2=no` &lt;dbl&gt;, `Ratio at post` &lt;dbl&gt;,\n#   `Citation Bias 1=yes, 2 = no` &lt;dbl&gt;\n\n# load in dataset and filter out studies without data\ndat &lt;- read_excel(\"Best Practices Coding Experiments.xlsx\",\n                  sheet = 1) %&gt;%\n  filter(!is.na(d))\n\n# display effect size data for first 6 studies\nhead(dat[c(\"Citation\",\"n\",\"d\")])\n\n# A tibble: 6 × 3\n  Citation                    n      d\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcott 2020              1661  0.09 \n2 Brailovskaia 2020         286  0.154\n3 Brailovskaia 2022         322  0    \n4 Collins and Edgers 2022   121 -0.138\n5 Deters & Mehl 2013         86 -0.207\n6 Faulhaber et al., 2023    230  0.484\n\n# compute sampling variance\ndat &lt;- dat %&gt;% \n  mutate(v = var_error_d(d = d, n1 = n))\n\n\n# display updated dataset\nhead(dat[c(\"Citation\",\"n\",\"d\",\"v\")])\n\n# A tibble: 6 × 4\n  Citation                    n      d       v\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Alcott 2020              1661  0.09  0.00241\n2 Brailovskaia 2020         286  0.154 0.0141 \n3 Brailovskaia 2022         322  0     0.0125 \n4 Collins and Edgers 2022   121 -0.138 0.0337 \n5 Deters & Mehl 2013         86 -0.207 0.0479 \n6 Faulhaber et al., 2023    230  0.484 0.0181 \n\ndat &lt;- dat %&gt;%\n  mutate(length_weeks = c(4, # Alcott 2020\n                          2, # Brailovskaia 2020\n                          2, # Brailovskaia 2022\n                          10, # Collins and Edgers 2022\n                          NA, # Deters & Mehl 2013\n                          2, # Faulhaber et al., 2023\n                          1/7, # Gajdics 2022\n                          2.5, # Hall et al. 2021\n                          1, # Hunt 2018\n                          1, # Hunt 2021\n                          1, # Kleefeld dissertation\n                          1, # Lambert 2022\n                          NA, # Lepp 2022\n                          1, # Mahalingham 2023\n                          1/7, # Mitev 2021\n                          NA, # Ozimek 2020\n                          1/7, # Przybylski 2021\n                          NA, # Sagioglou 2014 study 2\"\n                          NA, # Tartaglia\n                          3, # Thai 2021\n                          3, # Thai 2023\n                          1, # Tromholt 2016\n                          1, # Valley2019\n                          1, # van wezel 2021\n                          5/7, # Vanman 2018\n                          NA, # Ward 2017\n                          NA  # Yuen et al., 2019\n  ))"
  },
  {
    "objectID": "blog-posts/blog-post-7.html#non-linear-model-with-and-without-collis-eggers-2022",
    "href": "blog-posts/blog-post-7.html#non-linear-model-with-and-without-collis-eggers-2022",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Non-linear model with and without Collis & Eggers (2022)",
    "text": "Non-linear model with and without Collis & Eggers (2022)\nIn my blog post, I regressed the effect size on length of the intervention using a linear meta-regression which estimates the conditional mean effect size with a straight line. I have had a number of people request that weeks should be logged (which will pull in that Collis and Egger study) and fit a non-linear basis spline to the effects. I totally agree with this approach. Essentially we are modeling the conditional mean of the effect size as a non-linear function of the length of weeks. This will help account for the withdrawal effect that R&H had mentioned.\nLet’s first fit the model with the Collis and Egger (2022) included in the data:\n\nnu = 3\n\n# model with basis spline over log(weeks)\nmdl_weeks &lt;- rma(d ~ bs(log(length_weeks), df = nu),\n                 vi = v,\n                 data = dat,\n                 method = \"REML\",\n                 test = \"knha\")\n\nWarning: 7 studies with NAs omitted from model fitting.\n\n\nThe output of the model will be difficult to interpret so we will go ahead and plot it out with the confidence and prediction intervals.\n\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs we kind of would have expected, you see an initial increase in the conditional mean and then a slow fall over time. Still there is a lot of variability both in the true effects (prediction interval) and in the conditional mean effect (confidence interval) so it is difficult to say anything here with much confidence. Now we can construct the same model with Collis and Eggers (2022) removed from the analysis.\n\ndat_no_CollisEggers &lt;- dat %&gt;%\n  filter(Citation != \"Collins and Edgers 2022\")\n\nnu = 3\nmdl_weeks &lt;- rma(d ~ bs(log(length_weeks), df = nu),\n                 vi = v,\n                 data = dat_no_CollisEggers,\n                 method = \"REML\",\n                 test = \"knha\")\n\nWarning: 7 studies with NAs omitted from model fitting.\n\n\nLike we did for the last one, we can plot out the same model without the Collis and Egger (2022) study:\n\n\n\n\n\n\n\n\n\nSurprisingly, even without Collis and Egger’s study there really isn’t a whole lot we can say here with confidence. It does not really even appear is much different than it is with Collis and Eggers (2022) study included. In fact, with the predictions (conditional means) plotted side by side they appear almost identical:\n\n\n\n\n\nModel predictions (conditional mean of true effects) and confidence intervals for the model including Collis & Egger and the model without.\n\n\n\n\n\nA note on effect dependency\nThe confidence and prediction intervals are probably narrower then what it should be since some of the studies clearly come from the same lab and have similar designs. These CIs are likely the best case scenario."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#changing-the-two-week-cut-point-of-rhs",
    "href": "blog-posts/blog-post-7.html#changing-the-two-week-cut-point-of-rhs",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Changing the two week cut-point of R&H’s",
    "text": "Changing the two week cut-point of R&H’s\nBrenton Wiernik suggested that I play around with the two week cut-point that R&H used in their article. I understand R&H had the following justification for their cut-point,\n\nAcute withdrawal symptoms typically peak after a few days, but often last for up to two weeks.\n\nTherefore R&H selected for reduction studies that were two weeks or longer. But if withdrawal symptoms last for two weeks in many cases, then maybe we should play it safe and only select the studies that are longer than two weeks (\\(X_\\mathrm{weeks}&gt;2\\) rather than \\(X_\\mathrm{weeks}\\geq 2\\)). This cut-point could have easily been argued with R&H’s reasoning.\n\nmdl_2weeks &lt;- rma(yi = d,\n                 vi = v,\n                 data = dat,\n                 method = \"REML\",\n                 test = \"knha\",\n                 subset = length_weeks &gt; 2)\n\npredict(mdl_2weeks)\n\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.0980 0.0514 -0.0448 0.2408 -0.0448 0.2408 \n\n\nAfter playing it safe and allowing for withdrawal symptoms to subside we see a mean effect of \\(\\hat{\\mu}_\\delta = .098\\, [-.045,\\, .241]\\). Well seems that the average effect is quite small and about half the size of R&H’s average, but maybe we can try only studies that were more than 3 weeks,\n\nmdl_3weeks &lt;- rma(yi = d,\n                 vi = v,\n                 data = dat,\n                 method = \"REML\",\n                 test = \"knha\",\n                 subset = length_weeks &gt; 3)\n\npredict(mdl_3weeks)\n\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.0446 0.0910 -1.1122 1.2015 -1.5739 1.6631 \n\n\nWe see the mean effect of \\(\\hat{\\mu}_\\delta = .045\\, [-1.112, \\, 1.202]\\) is even smaller with very extremely wide CIs.\nThis goes to show that arbitrary post-hoc analytic decisions can greatly impact the end result. This is just another reason why R&H’s analysis was, at the very least, highly misleading."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#discussion",
    "href": "blog-posts/blog-post-7.html#discussion",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Discussion",
    "text": "Discussion\nUltimately, after re-running the model with a more sophisticated non-linear regression with and without the Collis & Egger study that Haidt suggested, there not great evidence of sustained detrimental effects in these RCTs. This does not mean social media is not, on average, detrimental for mental health, it just means that these these RCTs do not appear be good evidence of that fact.\nIf I did anything incorrectly or if I did not use best practices, I take full responsibility and you can let me know by sending me a DM on twitter or emailing me at matthewbjane@gmail.com."
  },
  {
    "objectID": "blog-posts/blog-post-7.html#citing-r-packages",
    "href": "blog-posts/blog-post-7.html#citing-r-packages",
    "title": "A Response to Jonathan Haidt’s Response",
    "section": "Citing R packages",
    "text": "Citing R packages\nThe following packages were used here:\n\nmgcv (Wood et al. 2016)\nsplines (R Core Team 2024)\nosfr (Wolen et al. 2020)\ntidyverse (Wickham et al. 2019)\nmetafor (Viechtbauer 2010)\nreadxl (Wickham and Bryan 2023)\npsychmeta (Dahlke and Wiernik 2019)\nggdist (Kay 2023)"
  },
  {
    "objectID": "blog-posts/blog-post-10.html#description-of-the-meta-analysis",
    "href": "blog-posts/blog-post-10.html#description-of-the-meta-analysis",
    "title": "Individual Person Data (IPD) Meta-Analysis with Beta Regression in a Repeated-Measures Cross-Over Design",
    "section": "Description of the Meta-analysis",
    "text": "Description of the Meta-analysis\n\nStudy design\nThe studies in this meta-analysis are investigating if auditory stimuli during slow-wave sleep can enhance overnight memory retention (this is based on theoretical reasons we don’t need to get into). In each study the participants undergo two sleep sessions reflecting two conditions. In one of the sessions, auditory stimuli (i.e., pink noise bursts) were presented while the subject’s were in deep slow-wave sleep (“stim” condition). In the other session there was no auditory stimuli (“sham” condition). These sleep sessions were separated by a week (or two) and were counter-balanced such that the ordering of conditions were randomized. Before subject’s go to sleep, subjects were given word-pairs that they are required to memorize. Prior to falling asleep they are assessed on how many word-pairs they can accurately remember (they are given one word and they are told to recall the other word). After the subjects wake up, they are assessed on the same task and the number of word-pairs they can remember correctly. So ultimately we have a repeated-measures (pre-post) cross-over (treatment-control) design.\nWe can load in the necessary packages Kay (2024) and get the data from my github repository.\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\nlibrary(ggdist)\nlibrary(ggtext)\n\nurlfile=\"https://raw.githubusercontent.com/MatthewBJane/meta-analysis-data-code/refs/heads/main/acoustic_stimulation_and_memory/data/individual_person_data.csv\"\n\ndf &lt;- read_csv(url(urlfile))\n\nhead(df)\n\n# A tibble: 6 × 6\n  study preSham postSham preStim postStim total_words\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 W2016      89      103      69       93         120\n2 W2016      59       89      72       92         120\n3 W2016      80       95      48       77         120\n4 W2016      47       75      57       75         120\n5 W2016      40       67      61       91         120\n6 W2016      78      104      71       99         120\n\n\nWe can see that each subject has four values: pre-stim, post-stim, pre-sham, and post-sham. The values denote the number of words recalled correctly and the total words column is the total number of word-pairs the subject was tasked to memorize. In total, there are 7 studies and 110 subjects.\n\n\nOutcome of interest\nAcross studies the total number of word-pairs in the recall task varied (range: 80-120) and, consequently, so does the number of word-pairs correctly recalled. This produced different number of correctly recalled words. For this reason, we will choose to use probability of recalling a word-pair accurately.\nAssume \\(Y_{ij} \\in \\{0,1\\}\\) is a Bernoulli random variable that denotes whether a word-pair has been correctly recalled (1) or not (0) for assessment \\(i\\) and trial \\(j\\). Let each session \\(i\\) have it’s own probability distribution for \\(Y\\) such that \\(Y_{ij} \\sim \\mathrm{Ber}(\\pi_i)\\) where the parameter \\(\\pi_i\\) is the true probability of correctly recalling a word-pair for session \\(i\\) such that, \\(\\pi_i = \\Pr_i(Y_{ij}=1)\\). Our data set will contain observed proportions \\(p_i = \\frac{N_{\\text{correct},i}}{N_{\\text{total word-pairs},i}}\\) which are estimates of \\(\\pi_i\\). Therefore the outcome of interest is a probability-valued random variable \\(\\pi_i\\) which is measured by the observed proportion \\(p_i\\).\n\n\nTime and condition variables\nWe have two other Bernoulli random variables that represents time \\(T_i\\) (pre=0, post=1) and condition \\(C_i\\) (sham=0, stim=1). Of course, condition and time do not vary by trial so the subscript \\(j\\) is removed. We can condition the probability of correctly recalling a word (\\(\\pi_i\\)) on values of \\(T_i\\) and \\(C_i\\) so that we have four possible experimental states,\n\nPre-sleep, sham condition: \\(\\;\\pi^{(0)}_{\\mathrm{pre},i} := \\Pr_i(Y_{ij}\\! =\\!1 \\mid T_i\\! =\\!0, C_i\\! =\\!0)\\)\nPost-sleep, sham condition: \\(\\pi^{(0)}_{\\mathrm{post},i} := \\Pr_i(Y_{ij}\\! =\\!1 \\mid T_i\\! =\\!1, C_i\\! =\\!0)\\)\nPre-sleep, stim condition: \\(\\;\\;\\;\\pi^{(1)}_{\\mathrm{pre},i} := \\Pr_i(Y_{ij}\\! =\\!1 \\mid T_i\\! =\\!0, C_i\\! =\\!1)\\)\nPost-sleep, stim condition: \\(\\;\\;\\pi^{(1)}_{\\mathrm{post},i} := \\Pr_i(Y_{ij}\\! =\\!1 \\mid T_i\\! =\\!1, C_i\\! =\\!1)\\)\n\nEvery participant has an observed proportion for all four experimental states: \\(p^{(0)}_{\\mathrm{pre},i}\\), \\(p^{(0)}_{\\mathrm{post},i}\\), \\(p^{(1)}_{\\mathrm{pre},i}\\), and \\(p^{(1)}_{\\mathrm{post},i}\\). What we want to do is make the data set long where condition is a dummy variable, this will set up our data such that we can fit an ANCOVA model in the next section.\n\ndat &lt;- df %&gt;%\n  # relabel the column headers\n  rename(pre.sham = preSham, pre.stim = preStim, \n         post.sham = postSham, post.stim = postStim) %&gt;%\n  # add row for person id\n  mutate(person = 1:nrow(df), .after = study) %&gt;%\n  # make data set long so that there is one column of scores\n  pivot_longer(cols = c(pre.sham, post.sham, pre.stim, post.stim),\n               names_to = \"session\",\n               values_to = \"num_correct\") %&gt;%\n  # separate the session column into random variables time T and condition C\n  separate(col = session, into = c(\"time\", \"condition\")) %&gt;%\n  # recode variables as factors with levels 0 and 1\n  pivot_wider(names_from = time,\n              values_from = c(num_correct,total_words),\n              id_cols = c(person,condition,study)) %&gt;%\n  # clean up variable values and calculate proportions \n  mutate(condition = factor(condition, levels = c(\"sham\",\"stim\"), labels = c(0,1)),\n         study = factor(study, \n                        levels = rev(c(\"N2013\",\"N2015.1\",\"N2015.2\",\"W2016\",\"H2019.1\",\"H2019.2\",\"S2020\")),\n                        labels = rev(c(\"Ngo (2013)\",\"Ngo (2015a)\",\"Ngo (2015b)\",\"Weigenand (2016)\",\"Henin (2019)\",\"Henin (2019)\",\"Schneider (2020)\"))),\n         p_pre = num_correct_pre / (total_words_pre+(.005*total_words_pre)),\n         p_post = num_correct_post / (total_words_post+(.005*total_words_post))) %&gt;%\n  # clean up columns\n  select(study, person, condition, p_pre, p_post)\n\n# display cleaned data set\nhead(dat,8)\n\n# A tibble: 8 × 5\n  study            person condition p_pre p_post\n  &lt;fct&gt;             &lt;int&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Weigenand (2016)      1 0         0.738  0.854\n2 Weigenand (2016)      1 1         0.572  0.771\n3 Weigenand (2016)      2 0         0.489  0.738\n4 Weigenand (2016)      2 1         0.597  0.763\n5 Weigenand (2016)      3 0         0.663  0.788\n6 Weigenand (2016)      3 1         0.398  0.638\n7 Weigenand (2016)      4 0         0.390  0.622\n8 Weigenand (2016)      4 1         0.473  0.622\n\n\nNow we have columns for the proportions at pre-test and post-test for each condition. Notice that each person has two rows pertaining to the stim and sham condition.\n\n\nEffect Size of Interest\nWe would like to the estimate the effect of auditory stimulation on memory retention, so therefore we would like to compare the post-sleep recall proportion correct between conditions. Our effect size of interest can be the defined as the average difference between post-sleep recall scores in the treatment and control group conditioned on a pre-sleep score of .50,\n\\[\n\\delta = \\mathbb{E}\\left(p^{(1)}_{\\mathrm{post},i}\\,- p^{(0)}_{\\mathrm{post},i}\\, \\middle|\\, p_{\\mathrm{pre},i} = .50\\right).\n\\]\nSince this effect size is essentially an average of risk differences, conditioning on a pre-sleep value of .50 is useful as is put succinctly by Karlson and Quillian (2023) “Risk differences are useful when base rates are in a middle range”. We will see that the IPD model will use the pre-sleep score as a covariate so conditioning on pre-sleep scores is straightforward. The grand average pre-sleep score is ~.47, which makes conditioning on .50 a bit easier to justify. We can visualize the pre-sleep scores for each study below, we can see that they are fairly evenly distributed around .50.\n\n\nCode\nggplot(dat, aes(x = p_pre, y = study)) +\n  stat_slab(point_interval = \"mean_qi\",fill = \"hotpink\",slab_color = \"transparent\", slab_alpha = .15) +\n  stat_dotsinterval(point_interval = \"mean_qi\",fill = \"hotpink2\",slab_color = \"transparent\", color = \"hotpink4\") +\n  lims(x = c(0,1)) +\n  theme_ggdist(base_size = 14) +\n  theme(aspect.ratio = 1.2) +\n  geom_vline(xintercept = c(.5), linetype = \"dashed\", alpha = .1, color = \"black\") +\n  labs(x = \"Pre-sleep scores (proportion correct)\",\n       y = \"Studies\") \n\n\n\n\n\nDistributions of pre-sleep scores by study. The dashed line denotes the 50:50 correct/incorrect value (\\(p_\\mathrm{pre} = .5\\))\n\n\n\n\nWe can interpret the effect size as: on average, a typical person sees a \\([\\delta\\times 100]\\) percentage point increase in accurately recalling a word-pair in the stim condition relative to the sham condition. An effect size value of \\(\\delta &lt; 0\\) would indicate reduced recall in the stim group (i.e., stim was actually detrimental relative to sham), a value between \\(\\delta &gt;0\\) would indicate increased recall in the stim group (i.e., stim was beneficial relative to sham), and a value of \\(\\delta=1\\) would be a null effect. Important to note that \\(\\delta\\) is not standardized in this meta-analysis because scores are comparable across studies."
  },
  {
    "objectID": "blog-posts/blog-post-10.html#ipd-model-pre-sleep-score-as-covariate",
    "href": "blog-posts/blog-post-10.html#ipd-model-pre-sleep-score-as-covariate",
    "title": "Individual Person Data (IPD) Meta-Analysis with Beta Regression in a Repeated-Measures Cross-Over Design",
    "section": "IPD Model (pre-sleep score as covariate)",
    "text": "IPD Model (pre-sleep score as covariate)\nWith our data we can model the observed proportions as a beta distribution such that \\(p_{\\mathrm{post},i} \\sim \\mathrm{Beta}(\\mu_i,\\phi)\\) where \\(\\mu_i\\) is the conditional mean (i.e., \\(\\mu_i:=\\mathbb{E}(p_{\\mathrm{post},i}\\, |\\, p_{\\mathrm{pre},i}, C_i )\\)) and \\(\\phi\\) is the concentration (\\(\\phi\\) is inversely related to the variance and we can obtain the variance of \\(p_i\\) with \\(\\mu(1-\\mu)/(\\phi+1)\\)). We can model the logit transformed \\(\\mu_i\\) with an ANCOVA type model by using the pre-sleep scores as a predictor within a regression model.\n\\[\n\\mathrm{logit}(\\mu_{i})  = \\beta_{0,sp} +  \\beta_{1,s}\\,p_{\\mathrm{pre},i} + \\beta_{2,s}\\, C_i\n\\]\n\\(C_i\\in\\{0,1\\}\\) and represent the condition (0=sham, 1=stim). This means that the model-implied conditional mean is,\n\\[\n\\mu_i = \\frac{\\exp(\\beta_{0,sp} +  \\beta_{1,s}\\,p_{\\mathrm{pre},i} + \\beta_{2,s}\\, C_i)}{1 + \\exp(\\beta_{0,sp} +  \\beta_{1,s}\\,p_{\\mathrm{pre},i} + \\beta_{2,s}\\, C_i)}\n\\]\nTo obtain person and study-level variability, we want to incorporate a random effect for study samples on each of the regression coefficients (there are 7 studies, denoted with subscript \\(s = 1...7\\)). We also allow the intercept to vary between individuals such that each person \\(p\\) has a random intercept (this is reasonable since there are individual differences in recall ability). Note that having condition vary by person (each person has 2 data points pertaining to treatment-control) would produce a model that is not identifiable. We will just use the default brms priors for all parameters.\n\nmdl &lt;- bf(p_post ~ 0 + Intercept + p_pre + condition + (1|person) + (1+p_pre+condition|study), \n          family = Beta(\"logit\"))\n\nprior &lt;- default_prior(mdl, data = dat)\n\n\nfit &lt;- brm(mdl,\n           data = dat,\n           cores = 2, \n           iter = 2000,\n           warmup = 200,\n           chains = 3,\n           prior = prior,\n           file = \"ipd_meta\",\n           file_refit = \"on_change\")\n\nI recommend reading A. Solomon Kurz’s awesome blog post on Beta regression for causal inference, which inspired some of this model (if I made any mistakes that does not reflect on Solomon’s blog, that is just me being dense).\n\nStudy-level estimates of \\(\\delta\\)\nLet’s observe the posterior distribution of effects from each study in the meta-analysis. Visualizations created with the help of the awesome tidybayes and ggdist packages Kay (2023).\n\nnd &lt;- data_grid(condition = 0:1,\n                p_pre = .5,\n                person = NA,\n                study = dat$study,\n                data = dat)\n\nadd_epred_draws(fit, newdata = nd, allow_new_levels = TRUE, re_formula = ~ (1+p_pre+condition|study)) %&gt;%\n  pivot_wider(names_from = condition,\n              values_from = .epred,\n              id_cols = c(study,.draw)) %&gt;%\n  mutate(delta = `1` - `0`) %&gt;%\n  ggplot(aes(x = delta, y = study)) +\n  stat_slabinterval(point_interval = \"mean_qi\",fill = \"hotpink3\", slab_alpha = .80, .width = c(.50,.95)) +\n  theme_ggdist(base_size = 14) +\n  theme(aspect.ratio = 1.1) +\n  coord_cartesian(xlim = c(-.15,.15)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = .1, color = \"black\") +\n  scale_x_continuous(breaks = round(seq(-.15,.15,by=.05),2)) +\n  labs(x = expression(Effect ~ delta),\n       y = \"\") +\n  ggtitle(\"Study-level effects\", subtitle = \"with 50% and 95% intervals\")\n\n\n\n\nStudy-level posterior estimates of the expected value of \\(\\delta\\)\n\n\n\n\nIn our original paper we found a strong moderating effect of publication year on the effect size. Although we were only able to get a subset of the raw data sets, you can still see the shadow of that trend appears here.\n\n\nMean Effect\nNow we want to estimate the average effect across studies, the key here is to set the re_formula argument in the add_epred_draws to NA. The figure below denotes the distribution of plausible values for the average true effect.\n\nnd &lt;- data_grid(condition = 0:1,\n                p_pre = .5,\n                person = NA,\n                study = NA,\n                data = dat)\n\nposterior_mean &lt;- add_epred_draws(fit, newdata = nd, re_formula = NA) %&gt;%\n  pivot_wider(names_from = condition,\n              values_from = .epred,\n              id_cols = .draw) %&gt;%\n  mutate(delta = `1` - `0`)\n\n\nggplot(data = posterior_mean, aes(x = delta, y = 0)) +\n  stat_slabinterval(point_interval = \"mean_qi\",fill = \"hotpink4\", slab_alpha = .80, .width = c(.50,.95)) +\n  theme_ggdist(base_size = 14) +\n  theme(aspect.ratio = .4) +\n  coord_cartesian(xlim = c(-.15,.15)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = .1, color = \"black\") +\n  labs(x = expression(Effect ~ delta),\n       y = \"\") +\n  ggtitle(\"Mean effect\", subtitle = \"with 50% and 95% intervals\") +\n  scale_x_continuous(breaks = round(seq(-.15,.15,by=.05),2)) +\n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nAs we can see there is little/no average effect of the stim condition 0.012 [-0.047, 0.069]. The estimate suggests there is about a 1% in recall probability difference from stim to sham.\nLet’s now obtain the credibility intervals which tells us what in what range of \\(\\delta\\) do X% of study effects fall. The figure below will show 50, 80, and 95% credibility intervals. To do this in brms I am like 99% sure we just need to incorporate all the study-level random effects in the re_formula argument again. Therefore we would want to specify it as re_formula = ~(1+p_pre+condition|study).\n\nnd &lt;- data_grid(condition = 0:1,\n                p_pre = .5,\n                person = NA,\n                study = \"new_study\",\n                data = dat)\n\nadd_predicted_draws(fit, newdata = nd, allow_new_levels = TRUE, re_formula = ~(1+p_pre+condition|study)) %&gt;%\n  pivot_wider(names_from = condition,\n              values_from = .prediction,\n              id_cols = .draw) %&gt;%\n  mutate(delta = `1` - `0`) %&gt;%\n  ggplot(aes(x = delta, y = 0)) +\n  stat_interval(point_interval = \"mean_qi\",color = c(\"hotpink4\",\"hotpink3\",\"hotpink2\"), alpha = c(.7,.5,.3), .width = c(.5,.80,.95)) +\n  coord_cartesian(xlim = c(-.15,.15)) +\n  scale_x_continuous(breaks = round(seq(-.15,.15,by=.05),2)) +\n  theme_ggdist(base_size = 14) +\n  theme(aspect.ratio = .4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = .1, color = \"black\") +\n  labs(x = expression(Effect ~ delta),\n       y = \"\") +\n  ggtitle(\"Credibility intervals\", subtitle = \"Intervals where 50,80,95% of study effects fall\") +\n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\n\n\nDiagnostics\nLet’s observe the posterior predictive check to see if our model performed as we expected.\n\npp_check(fit, ndraws = 200) + \n  xlim(0,1) +\n  theme_ggdist(base_size = 14) +\n  scale_y_continuous(breaks=NULL) +\n  scale_color_manual(values = c(\"hotpink4\", \"hotpink\")) +\n  ggtitle(\"Posterior predictive check\", \n          subtitle = \"&lt;span style = 'color:hotpink4'&gt;**observed distribution**&lt;/span&gt; and &lt;span style = 'color:hotpink'&gt;**predicted distribution**&lt;/span&gt;\") +\n  theme(legend.position = \"none\",\n        title = element_markdown(),\n        axis.text = element_text(family = \"arial\"), \n        aspect.ratio = .7)\n\n\n\n\n\n\n\n\nThe predictive check looks quite good!\nAnyways… thats the post. Let me know if you have any suggestions or critiques."
  },
  {
    "objectID": "blog-posts/blog-post-10.html#session-info",
    "href": "blog-posts/blog-post-10.html#session-info",
    "title": "Individual Person Data (IPD) Meta-Analysis with Beta Regression in a Repeated-Measures Cross-Over Design",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ggtext_0.1.2    ggdist_3.3.2    modelr_0.1.11   tidybayes_3.0.7\n [5] brms_2.22.0     Rcpp_1.0.12     lubridate_1.9.3 forcats_1.0.0  \n [9] stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2     readr_2.1.5    \n[13] tidyr_1.3.1     tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.6         tidyselect_1.2.1     viridisLite_0.4.2   \n [4] farver_2.1.2         loo_2.8.0            fastmap_1.2.0       \n [7] tensorA_0.36.2.1     digest_0.6.36        timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  magrittr_2.0.3      \n[13] posterior_1.6.0      compiler_4.4.0       rlang_1.1.4         \n[16] tools_4.4.0          utf8_1.2.4           yaml_2.3.8          \n[19] knitr_1.47           labeling_0.4.3       bridgesampling_1.1-2\n[22] htmlwidgets_1.6.4    bit_4.0.5            pkgbuild_1.4.4      \n[25] plyr_1.8.9           xml2_1.3.6           abind_1.4-8         \n[28] withr_3.0.0          grid_4.4.0           stats4_4.4.0        \n[31] fansi_1.0.6          colorspace_2.1-0     inline_0.3.19       \n[34] scales_1.3.0         cli_3.6.3            mvtnorm_1.3-1       \n[37] rmarkdown_2.27       crayon_1.5.3         generics_0.1.3      \n[40] RcppParallel_5.1.9   rstudioapi_0.16.0    reshape2_1.4.4      \n[43] tzdb_0.4.0           commonmark_1.9.2     rstan_2.32.6        \n[46] bayesplot_1.11.1     parallel_4.4.0       matrixStats_1.4.1   \n[49] vctrs_0.6.5          Matrix_1.7-0         jsonlite_1.8.8      \n[52] hms_1.1.3            arrayhelpers_1.1-0   bit64_4.0.5         \n[55] glue_1.7.0           codetools_0.2-20     distributional_0.4.0\n[58] stringi_1.8.4        gtable_0.3.5         QuickJSR_1.3.1      \n[61] quadprog_1.5-8       munsell_0.5.1        pillar_1.9.0        \n[64] htmltools_0.5.8.1    Brobdingnag_1.2-9    R6_2.5.1            \n[67] vroom_1.6.5          evaluate_0.24.0      lattice_0.22-6      \n[70] markdown_1.13        backports_1.5.0      gridtext_0.1.5      \n[73] broom_1.0.6          renv_0.17.3          rstantools_2.4.0    \n[76] gridExtra_2.3        coda_0.19-4.1        nlme_3.1-164        \n[79] checkmate_2.3.1      xfun_0.45            pkgconfig_2.0.3"
  },
  {
    "objectID": "blog-posts/blog-post-10.html#citing-r-packages",
    "href": "blog-posts/blog-post-10.html#citing-r-packages",
    "title": "Individual Person Data (IPD) Meta-Analysis with Beta Regression in a Repeated-Measures Cross-Over Design",
    "section": "Citing R packages",
    "text": "Citing R packages\nThe following packages were used in this post:\n\nbrms (Bürkner 2021)\ntidyverse (Wickham et al. 2019)\nggdist (Kay 2023)\nmodelr (Wickham 2023)\ntidybayes (Kay 2024)"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Website  Email  Twitter  Github\n\n\n\n2022–present | Ph.D. Quantitative Psychology, University of Connecticut\n\nResearch involves developing statistical methods and software for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut\n\n\n\n\n\nJané, M. B., Xiao, Q., Yeung, S. K., Caldwell, A. R., Dunleavy, D. J., … (2023) Guide to Effect Sizes and Confidence Intervals. A collaborative guide to calculating, interpreting, and reporting effect sizes and confidence intervals. In Progress. See preliminary version here: https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/\nJané, M. B., Kroc, E., Wiernik, B. M., Oswald, F. L., Johnson, B. T. (2023) Artifact Corrections for Effect Sizes. An open-source book for correcting bias in effect size estimates contaminated by statistical artifacts. In Progress. See preliminary version here: https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/\n\n\n\n\n\n R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/ThemePark\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. matthewbjane.com/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. matthewbjane.com/ArtifactCorrections\n\n\n\n\n\nMatthew B. Jané Consulting statistical consulting services for clients in academia and industry research. matthewbjane.github.io/Consulting.html\nPhysalia Courses “Meta-Analysis in R” Workshop hosted a four day workshop in meta-analysis with application to R programming language. www.physalia-courses.org/courses-workshops/metain-r/\nR workshops for Ukraine “Meta-Analysis in R” Workshop hosted a 2-hour workshop in meta-analysis with application to R programming language for charity. sites.google.com/view/dariia-mykhailyshyna/main/r-workshops-for-ukraine#h.qwn56mbd02im.\nUSF “Meta-Analysis in R” Workshop hosted a 1-hour workshop in meta-analysis with application to R programming language for the University of South Florida.\n\n\n\n\n\nMeta-Analysis Magic Blog blog discussing statistical modeling and other quantitative topics in meta-analysis and measurement.\nX/Twitter and Blue Sky using twitter and bluesky as a means of scientific communication.\n\n\n\n\nJané, M. B., Harlow, T. J., Khu, E. C., and colleagues (2024). Extracting Pre-Post Correlations for Meta-Analyses of Repeated Measures Designs https://matthewbjane.quarto.pub/pre-post-correlations/ Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nJohnson, B. T., Jané, M. B., Curley, C. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Stress Reduction in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Uanhoro, J., Boika, N. (2024). Predictive Validity of the Unified Tertiary Matriculation Examination on University Grades: A Living Meta-Analysis. https://matthewbjane.com/utme-validity-meta-analysis/ Manuscript in Preparation.\n\nData Code PDF OSF"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "2022–present | Ph.D. Quantitative Psychology, University of Connecticut\n\nResearch involves developing statistical methods and software for meta-analysis and evidence synthesis. Primarily focusing on correcting bias in effect size estimates induced by statistical artifacts.\n\n2021–2022 | M.S. Behavioral Neuroscience, University of Connecticut\n2018–2020 | B.S. Computational Neuroscience, University of Connecticut"
  },
  {
    "objectID": "CV.html#textbooks",
    "href": "CV.html#textbooks",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Jané, M. B., Xiao, Q., Yeung, S. K., Caldwell, A. R., Dunleavy, D. J., … (2023) Guide to Effect Sizes and Confidence Intervals. A collaborative guide to calculating, interpreting, and reporting effect sizes and confidence intervals. In Progress. See preliminary version here: https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/\nJané, M. B., Kroc, E., Wiernik, B. M., Oswald, F. L., Johnson, B. T. (2023) Artifact Corrections for Effect Sizes. An open-source book for correcting bias in effect size estimates contaminated by statistical artifacts. In Progress. See preliminary version here: https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/"
  },
  {
    "objectID": "CV.html#software",
    "href": "CV.html#software",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "R Package ThemePark, Generating popular culture styled ggplot themes. github.com/MatthewBJane/theme_park. Featured on rweekly.org and flowingdata.com\n R Package POSC An R Package for generating Probability of Outcome Superiority Curves (POSCs). github.com/MatthewBJane/ThemePark\n Database OpenSynthesis, website cataloging publicly available meta-analytic databases. matthewbjane.com/opensynthesis\n Shiny App Artifact Simulator, A Shiny App for Visualizing Statistical Artifacts. matthewbjane.shinyapps.io/effect_size_artifact_bias\n Webpage Artifact Corrections for Effect Sizes, A webpage documenting equations and code for effect size artifact corrections. matthewbjane.com/ArtifactCorrections"
  },
  {
    "objectID": "CV.html#consulting-and-workshops",
    "href": "CV.html#consulting-and-workshops",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Matthew B. Jané Consulting statistical consulting services for clients in academia and industry research. matthewbjane.github.io/Consulting.html\nPhysalia Courses “Meta-Analysis in R” Workshop hosted a four day workshop in meta-analysis with application to R programming language. www.physalia-courses.org/courses-workshops/metain-r/\nR workshops for Ukraine “Meta-Analysis in R” Workshop hosted a 2-hour workshop in meta-analysis with application to R programming language for charity. sites.google.com/view/dariia-mykhailyshyna/main/r-workshops-for-ukraine#h.qwn56mbd02im.\nUSF “Meta-Analysis in R” Workshop hosted a 1-hour workshop in meta-analysis with application to R programming language for the University of South Florida."
  },
  {
    "objectID": "CV.html#scientific-communication",
    "href": "CV.html#scientific-communication",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Meta-Analysis Magic Blog blog discussing statistical modeling and other quantitative topics in meta-analysis and measurement.\nX/Twitter and Blue Sky using twitter and bluesky as a means of scientific communication."
  },
  {
    "objectID": "CV.html#publications",
    "href": "CV.html#publications",
    "title": "Matthew B. Jané",
    "section": "",
    "text": "Jané, M. B., Harlow, T. J., Khu, E. C., and colleagues (2024). Extracting Pre-Post Correlations for Meta-Analyses of Repeated Measures Designs https://matthewbjane.quarto.pub/pre-post-correlations/ Manuscript in Preparation.\n\nData Code PDF OSF\n\n\nJohnson, B. T., Jané, M. B., Curley, C. (2023). Methodological Quality in Research in Health Psychology. Under review at Sage Journal of Health Psychology.\n\nData Code PDF OSF\n\n\nChampion, G., Jané, M. B., Johnson, B. T., and colleagues (2023). Efficacy of Mindfulness Based Stress Reduction Interventions on Stress Reduction in the United States: A Meta-Analysis. Data being collected.\n\nData Code PDF OSF\n\n\nHarlow*, T. J., Jané*, M. B., Read, H. L., & Chrobak, J. J. (2023). Memory retention following acoustic stimulation in slow-wave sleep: a meta-analytic review of replicability and measurement quality. Frontiers in Sleep.\n\nData Code PDF OSF\n\n\nJané, M. B., Pisupati, S., Smith, K. E., Castro-Tonelli, L., Melo-Thomas, L., Schwarting, R. K., … & Read, H. L. (2022). Correlations across timing cues in natural vocalizations predict biases in judging synthetic sound burst durations. bioRxiv.\n\nData Code PDF OSF\n\n\nJané, M. B., Uanhoro, J., Boika, N. (2024). Predictive Validity of the Unified Tertiary Matriculation Examination on University Grades: A Living Meta-Analysis. https://matthewbjane.com/utme-validity-meta-analysis/ Manuscript in Preparation.\n\nData Code PDF OSF"
  },
  {
    "objectID": "social_media_mental_health.html",
    "href": "social_media_mental_health.html",
    "title": "The Effect of Social Media Reduction on Depression and Anxiety: A Meta-Analysis of RCTs",
    "section": "",
    "text": "This site has been relocated to: matthewbjane.quarto.pub/social-media-and-mental-health/"
  }
]